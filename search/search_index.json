{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"developers/plugin_development_guide/","title":"Plugin Development Guide","text":"<p>This guide provides step-by-step instructions for developers who want to extend Compileo's functionality by creating plugins.</p>"},{"location":"developers/plugin_development_guide/#overview","title":"Overview","text":"<p>Compileo's plugin system allows you to extend the platform's capabilities without modifying the core codebase. Plugins are self-contained <code>.zip</code> packages that include a manifest file and Python source code.</p> <p>The system is designed around Extension Points. Core modules define these points (standard interfaces), and plugins implement them. This allows plugins to provide new functionality\u2014such as custom dataset formats, new data sources, or additional processing steps\u2014depending on which extension points they target.</p>"},{"location":"developers/plugin_development_guide/#1-plugin-structure","title":"1. Plugin Structure","text":"<p>A valid plugin package is a <code>.zip</code> file with the following structure:</p> <pre><code>my-plugin.zip\n\u251c\u2500\u2500 plugin.yaml        # Manifest file (Required)\n\u251c\u2500\u2500 README.md          # Documentation (Recommended)\n\u2514\u2500\u2500 src/               # Source code directory\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 my_module.py   # Implementation code\n</code></pre>"},{"location":"developers/plugin_development_guide/#2-the-manifest-file-pluginyaml","title":"2. The Manifest File (<code>plugin.yaml</code>)","text":"<p>The <code>plugin.yaml</code> file is the heart of your plugin. It defines metadata and tells Compileo how to load your code.</p>"},{"location":"developers/plugin_development_guide/#fields","title":"Fields","text":"<ul> <li><code>id</code>: Unique identifier for your plugin (e.g., <code>my-custom-plugin</code>). Use lowercase, hyphens allowed.</li> <li><code>name</code>: Human-readable name (e.g., \"My Custom Plugin\").</li> <li><code>version</code>: Semantic version string (e.g., \"1.0.0\").</li> <li><code>author</code>: Name of the creator or team.</li> <li><code>description</code>: Brief description of what the plugin does.</li> <li><code>entry_point</code>: The Python module path to your code (e.g., <code>src.my_module</code>).</li> <li><code>extensions</code>: A dictionary where keys are Extension Point IDs (defined by core modules) and values are mappings of names to your implementation classes.</li> <li><code>format_metadata</code>: (Optional) Metadata specific to certain extension types (like formatters).</li> <li><code>install_script</code>: (Optional) A shell command string to run after the plugin is installed (e.g., to download external binaries). Runs in the project's virtual environment.</li> <li><code>uninstall_script</code>: (Optional) A shell command string to run before the plugin is uninstalled.</li> </ul>"},{"location":"developers/plugin_development_guide/#example-pluginyaml","title":"Example <code>plugin.yaml</code>","text":"<pre><code>id: \"compileo-anki-plugin\"\nname: \"Anki Dataset Exporter\"\nversion: \"1.0.0\"\nauthor: \"Compileo Team\"\ndescription: \"Exports datasets in Anki-compatible text format.\"\nentry_point: \"src.anki_formatter\"\nextensions:\n  # Hooking into the dataset generation formatter extension point\n  compileo.datasetgen.formatter:\n    anki: \"AnkiOutputFormatter\"\nformat_metadata:\n  anki:\n    file_extension: \"txt\"\n    description: \"Anki-compatible semicolon-separated text format\"\n# Example of an install script (e.g., for a scraper plugin)\n# install_script: \"python -m playwright install\"\n</code></pre>"},{"location":"developers/plugin_development_guide/#3-implementing-extensions","title":"3. Implementing Extensions","text":"<p>To extend Compileo, you need to implement the interface defined by a specific Extension Point.</p>"},{"location":"developers/plugin_development_guide/#available-extension-points","title":"Available Extension Points","text":""},{"location":"developers/plugin_development_guide/#1-dataset-formatter","title":"1. Dataset Formatter","text":"<ul> <li>ID: <code>compileo.datasetgen.formatter</code></li> <li>Purpose: Define custom output formats for generated datasets.</li> <li>Interface: <code>format(self, dataset_content: Union[str, List[Dict]]) -&gt; str</code></li> <li>Generation Mode Awareness: Formatters should check the <code>generation_mode</code> field in dataset items to adapt output format accordingly (e.g., instruction-response pairs vs Q&amp;A vs summarization).</li> <li>Data Structure: Dataset items include fields like <code>instruction</code>, <code>input</code>, <code>output</code>, <code>question</code>, <code>answer</code>, <code>summary</code>, <code>key_points</code>, and <code>generation_mode</code>.</li> </ul>"},{"location":"developers/plugin_development_guide/#2-ingestion-handler","title":"2. Ingestion Handler","text":"<ul> <li>ID: <code>compileo.ingestion.handler</code></li> <li>Purpose: Handle custom input sources (e.g., URLs) for ingestion.</li> <li>Interface:<ul> <li><code>can_handle(self, input_path: str) -&gt; bool</code>: Return True if plugin can handle this input.</li> <li><code>process(self, path: str, **kwargs) -&gt; Optional[str]</code>: Process the input and return extracted text.</li> </ul> </li> </ul>"},{"location":"developers/plugin_development_guide/#3-api-router","title":"3. API Router","text":"<ul> <li>ID: <code>compileo.api.router</code></li> <li>Purpose: Expose new API endpoints.</li> <li>Interface: The registered object should be a FastAPI <code>APIRouter</code> instance or a class with a <code>router</code> attribute.</li> </ul>"},{"location":"developers/plugin_development_guide/#4-cli-command","title":"4. CLI Command","text":"<ul> <li>ID: <code>compileo.cli.command</code></li> <li>Purpose: Add new commands to the Compileo CLI.</li> <li>Interface: The registered object should be a <code>click.Command</code> or <code>click.Group</code>.</li> </ul>"},{"location":"developers/plugin_development_guide/#example-dataset-formatter-extension","title":"Example: Dataset Formatter Extension","text":"<pre><code>from typing import List, Dict, Any, Union\nimport json\n\nclass MyCustomFormatter:\n    \"\"\"\n    Example implementation for the dataset formatter extension point.\n    Supports generation mode awareness for proper field mapping.\n    \"\"\"\n\n    def format(self, dataset_content: Union[str, List[Dict[str, Any]]]) -&gt; str:\n        # Handle JSON string input (standard Compileo output)\n        if isinstance(dataset_content, str):\n            dataset_content = json.loads(dataset_content)\n\n        # Ensure we have a list of dictionaries\n        if not isinstance(dataset_content, list):\n            raise ValueError(\"Formatter requires a list of items\")\n\n        lines = []\n\n        for item in dataset_content:\n            if isinstance(item, dict):\n                # Determine generation mode and extract appropriate fields\n                generation_mode = item.get(\"generation_mode\", \"question_answer\")\n\n                if generation_mode == \"instruction_following\" or item.get(\"instruction\"):\n                    # Instruction following mode\n                    instruction = item.get(\"instruction\", \"\")\n                    input_text = item.get(\"input\", \"\")\n                    output_text = item.get(\"output\", \"\")\n\n                    # Format for your specific output requirements\n                    front = instruction\n                    if input_text:\n                        front = f\"{instruction}\\n\\nInput: {input_text}\"\n                    back = output_text\n\n                elif generation_mode == \"summarization\" or item.get(\"summary\"):\n                    # Summarization mode\n                    front = \"Summary:\"\n                    back = item.get(\"summary\", \"\")\n\n                else:\n                    # Default Q&amp;A mode\n                    front = item.get(\"question\", \"\")\n                    back = item.get(\"answer\", \"\")\n\n                # Format fields for your output format\n                # ... processing logic ...\n                lines.append(f\"{front}|{back}\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"developers/plugin_development_guide/#4-packaging-your-plugin","title":"4. Packaging Your Plugin","text":"<ol> <li>Organize your files according to the structure defined in Section 1.</li> <li>Dependencies:<ul> <li>Option A (Preferred): Include a <code>requirements.txt</code> file at the root of your plugin zip. The system will attempt to install these dependencies when the plugin is installed.</li> <li>Option B (Legacy): Vendor dependencies by including their source code inside your <code>src/</code> directory.</li> </ul> </li> <li>Post-Install Scripts:<ul> <li>If your plugin requires external tools (like browsers for Playwright), define an <code>install_script</code> in <code>plugin.yaml</code>.</li> <li>The command will be executed in the project's virtual environment. Use <code>python -m module ...</code> to ensure the correct Python interpreter is used.</li> </ul> </li> <li>Create a zip archive of your plugin directory. Ensure <code>plugin.yaml</code> is at the root of the archive.</li> </ol> <p>Important: Do not include a top-level folder in the zip file. When unzipped, <code>plugin.yaml</code> should be immediate, not inside <code>my-plugin-folder/plugin.yaml</code>.</p>"},{"location":"developers/plugin_development_guide/#5-documentation","title":"5. Documentation","text":"<p>It is highly recommended to include a <code>README.md</code> file in your plugin package.</p> <ul> <li>Description: Detailed explanation of features.</li> <li>Usage: Instructions on how to use the plugin.</li> <li>Configuration: If your plugin accepts specific parameters (via naming conventions or other means), document them clearly.</li> <li>Endpoints: If your plugin introduces new behaviors that affect API endpoints, describe them here.</li> </ul>"},{"location":"developers/plugin_development_guide/#6-installation","title":"6. Installation","text":"<ol> <li>Navigate to the Plugins section in the Compileo GUI.</li> <li>Click Upload Plugin.</li> <li>Select your <code>.zip</code> file.</li> <li>The system will validate and install the plugin.</li> </ol> <p>Alternatively, you can use the API: <code>POST /api/v1/plugins/install</code> with the file upload.</p>"},{"location":"developers/plugin_development_guide/#7-best-practices","title":"7. Best Practices","text":"<ul> <li>Security: Plugins run with \"Trusted Execution\" privileges. Validate all inputs carefully. Do not use <code>eval()</code> or execute arbitrary system commands.</li> <li>Robustness: Your plugin logic might run in a separate process (e.g., RQ worker). Avoid relying on global state that isn't pickle-able.</li> <li>Subprocesses: If your plugin spawns subprocesses (e.g., to run a scraper script), ensure you set up the environment (<code>PYTHONPATH</code>) correctly so it can import project modules. Be mindful of relative path calculations (e.g., use <code>../../../</code> to reach project root from <code>plugins/my-plugin/src/</code>).</li> <li>Metadata: Use <code>format_metadata</code> or similar fields in <code>plugin.yaml</code> to provide UI hints if supported by the extension point.</li> <li>Error Handling: Raise clear <code>ValueError</code> or <code>TypeError</code> exceptions if input data is invalid. These will be caught and logged by the core system.</li> </ul>"},{"location":"userGuide/api_authentication/","title":"API Authentication Guide","text":"<p>Compileo implements mandatory API key authentication for all protected endpoints to ensure secure access to document processing and dataset engineering features.</p>"},{"location":"userGuide/api_authentication/#authentication-mechanism","title":"Authentication Mechanism","text":"<p>Compileo uses a custom header-based authentication mechanism. All requests to protected endpoints must include a valid API key in the request headers.</p> <p>Header Name: <code>X-API-Key</code></p>"},{"location":"userGuide/api_authentication/#public-endpoints-no-auth-required","title":"Public Endpoints (No Auth Required)","text":"<p>The following endpoints are accessible without authentication: *   <code>/</code> (Root API information) *   <code>/health</code> (Service health check) *   <code>/docs</code> (Swagger UI documentation) *   <code>/redoc</code> (ReDoc documentation) *   <code>/openapi.json</code> (OpenAPI specification)</p>"},{"location":"userGuide/api_authentication/#setting-up-api-keys","title":"Setting Up API Keys","text":"<p>API keys can be defined via the GUI, CLI arguments, or environment variables.</p>"},{"location":"userGuide/api_authentication/#auto-lock-model","title":"\ud83d\udd10 Auto-Lock Model","text":"<p>Compileo uses an \"Auto-Lock\" security model for maximum user-friendliness: *   Unsecured Mode: If no API keys are defined anywhere, the API allows all requests. This is ideal for initial setup. *   Secured Mode: As soon as at least one API key is defined (via GUI, CLI, or Env), the API \"locks\" and requires a valid key for all protected endpoints.</p>"},{"location":"userGuide/api_authentication/#1-via-gui-recommended","title":"1. Via GUI (Recommended)","text":"<ol> <li>Open the Compileo Web GUI.</li> <li>Go to Settings &gt; \ud83d\udd17 API Configuration.</li> <li>Enter your desired key in the API Key field and save.</li> <li>The API will immediately lock down using this key.</li> </ol>"},{"location":"userGuide/api_authentication/#2-via-cli","title":"2. Via CLI","text":"<p>You can pass a static API key when starting the backend server: <pre><code>python -m src.compileo.api.main --api-key your_secret_key\n</code></pre></p>"},{"location":"userGuide/api_authentication/#3-via-environment-variables","title":"3. Via Environment Variables","text":"<p>Set these in your shell, <code>.env</code> file, or Docker configuration: *   <code>COMPILEO_API_KEY</code>: A single master key. *   <code>COMPILEO_API_KEYS</code>: A comma-separated list of multiple authorized keys.</p> <p>Example (.env): <pre><code>COMPILEO_API_KEY=master_key\nCOMPILEO_API_KEYS=app1_key,app2_key\n</code></pre></p>"},{"location":"userGuide/api_authentication/#configuration-priorities","title":"Configuration Priorities","text":"<p>The API consolidates keys from all sources. If multiple sources define keys, they are all added to the list of authorized keys.</p> <p>If no API keys are defined anywhere, the API operates in Unsecured Mode.</p>"},{"location":"userGuide/api_authentication/#usage-examples","title":"Usage Examples","text":""},{"location":"userGuide/api_authentication/#using-curl","title":"Using <code>curl</code>","text":"<pre><code>curl -X GET \"http://localhost:8000/api/v1/projects\" \\\n  -H \"X-API-Key: your_secret_key\"\n</code></pre>"},{"location":"userGuide/api_authentication/#using-python-requests","title":"Using Python <code>requests</code>","text":"<pre><code>import requests\n\nAPI_URL = \"http://localhost:8000/api/v1\"\nAPI_KEY = \"your_secret_key\"\n\nheaders = {\n    \"X-API-Key\": API_KEY\n}\n\nresponse = requests.get(f\"{API_URL}/projects\", headers=headers)\n\nif response.status_code == 200:\n    print(\"Successfully authenticated!\")\n    print(response.json())\nelif response.status_code == 401:\n    print(\"Authentication failed: Invalid or missing API key\")\n</code></pre>"},{"location":"userGuide/api_authentication/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Use Strong Keys: Generate random, high-entropy strings for your API keys.</li> <li>Environment Isolation: Use different keys for development, staging, and production environments.</li> <li>Secure Storage: Never commit your <code>.env</code> file containing real API keys to version control. Add <code>.env</code> to your <code>.gitignore</code>.</li> <li>HTTPS: In production environments, always serve the API over HTTPS to protect the API key in transit.</li> </ol>"},{"location":"userGuide/benchmarkingapi/","title":"Benchmarking API in Compileo","text":""},{"location":"userGuide/benchmarkingapi/#overview","title":"Overview","text":"<p>The Compileo Benchmarking API provides comprehensive evaluation capabilities for AI models across multiple benchmark suites. It supports automated testing, performance tracking, and comparative analysis with full integration into Compileo's asynchronous job processing system.</p>"},{"location":"userGuide/benchmarkingapi/#base-url-apiv1benchmarking","title":"Base URL: <code>/api/v1/benchmarking</code>","text":""},{"location":"userGuide/benchmarkingapi/#1-run-benchmarks","title":"1. Run Benchmarks","text":"<p>Endpoint: <code>POST /run</code></p> <p>Description: Initiates a new benchmarking job for an AI model against specified evaluation suites using Compileo's job queue system.</p> <p>Request Body: <pre><code>{\n  \"project_id\": 1,\n  \"suite\": \"glue\",\n  \"config\": {\n    \"provider\": \"ollama\",\n    \"model\": \"mistral:latest\",\n    \"ollama_params\": {\n      \"temperature\": 0.1,\n      \"top_p\": 0.9\n    }\n  }\n}\n</code></pre></p> <p>Parameters: - <code>project_id</code> (integer, required): Project ID for the benchmark job - <code>suite</code> (string): Benchmark suite (<code>glue</code>, <code>superglue</code>, <code>mmlu</code>, <code>medical</code>) - <code>config</code> (object, required): AI model configuration   - <code>provider</code> (string): AI provider (<code>ollama</code>, <code>gemini</code>, <code>grok</code>)   - <code>model</code> (string): Model identifier   - <code>ollama_params</code> (object, optional): Ollama-specific parameters     - <code>temperature</code> (float): Sampling temperature     - <code>top_p</code> (float): Top-p sampling     - <code>top_k</code> (integer): Top-k sampling     - <code>num_predict</code> (integer): Maximum tokens to generate     - <code>num_ctx</code> (integer): Context window size     - <code>seed</code> (integer): Random seed</p> <p>Success Response (200 OK): <pre><code>{\n  \"job_id\": \"a967f363-ee96-4bac-9f52-4d169bbc4851\",\n  \"message\": \"Benchmarking started for suite: glue\",\n  \"estimated_duration\": \"10-30 minutes\"\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#2-get-benchmark-results","title":"2. Get Benchmark Results","text":"<p>Endpoint: <code>GET /results/{job_id}</code></p> <p>Description: Retrieves the current status and results of a benchmarking job.</p> <p>Path Parameters: - <code>job_id</code> (string, required): Benchmarking job identifier</p> <p>Success Response (200 OK): <pre><code>{\n  \"job_id\": \"a967f363-ee96-4bac-9f52-4d169bbc4851\",\n  \"status\": \"completed\",\n  \"summary\": {\n    \"total_evaluations\": 8,\n    \"benchmarks_run\": [\"glue\"],\n    \"models_evaluated\": 1,\n    \"total_time_seconds\": 847.23\n  },\n  \"performance_data\": {\n    \"glue\": {\n      \"cola\": {\"accuracy\": {\"mean\": 0.823, \"std\": 0.012}},\n      \"sst2\": {\"accuracy\": {\"mean\": 0.945, \"std\": 0.008}},\n      \"mrpc\": {\"f1\": {\"mean\": 0.876, \"std\": 0.015}},\n      \"qqp\": {\"f1\": {\"mean\": 0.892, \"std\": 0.011}},\n      \"mnli\": {\"accuracy\": {\"mean\": 0.834, \"std\": 0.009}},\n      \"qnli\": {\"accuracy\": {\"mean\": 0.901, \"std\": 0.007}},\n      \"rte\": {\"accuracy\": {\"mean\": 0.678, \"std\": 0.023}},\n      \"wnli\": {\"accuracy\": {\"mean\": 0.512, \"std\": 0.031}}\n    }\n  },\n  \"completed_at\": \"2025-12-07T20:55:17Z\"\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#3-cancel-benchmark-job","title":"3. Cancel Benchmark Job","text":"<p>Endpoint: <code>POST /cancel/{job_id}</code></p> <p>Description: Cancels a running or pending benchmark job.</p> <p>Path Parameters: - <code>job_id</code> (string, required): Benchmarking job identifier</p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"Job a967f363-ee96-4bac-9f52-4d169bbc4851 cancelled successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#4-list-benchmark-results","title":"4. List Benchmark Results","text":"<p>Endpoint: <code>GET /results</code></p> <p>Description: Retrieves a list of benchmark jobs with optional filtering.</p> <p>Query Parameters: - <code>model_name</code> (string, optional): Filter by model name - <code>suite</code> (string, optional): Filter by benchmark suite - <code>status</code> (string, optional): Filter by job status (<code>pending</code>, <code>running</code>, <code>completed</code>, <code>failed</code>) - <code>limit</code> (integer, optional, default: 20): Maximum number of results</p> <p>Success Response (200 OK): <pre><code>{\n  \"results\": [\n    {\n      \"job_id\": \"a967f363-ee96-4bac-9f52-4d169bbc4851\",\n      \"status\": \"completed\",\n      \"model_name\": \"mistral:latest\",\n      \"benchmark_suite\": \"glue\",\n      \"created_at\": \"2025-12-07T20:55:17Z\",\n      \"completed_at\": \"2025-12-07T20:56:44Z\"\n    }\n  ],\n  \"total\": 5\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#5-compare-models","title":"5. Compare Models","text":"<p>Endpoint: <code>POST /compare</code></p> <p>Description: Compares multiple models across specified metrics and benchmark suites.</p> <p>Request Body: <pre><code>{\n  \"model_ids\": [\"gpt-4\", \"claude-3\", \"gemini-pro\"],\n  \"benchmark_suite\": \"glue\",\n  \"metrics\": [\"accuracy\", \"f1\"]\n}\n</code></pre></p> <p>Success Response (200 OK): <pre><code>{\n  \"comparison\": {\n    \"models_compared\": [\"gpt-4\", \"claude-3\", \"gemini-pro\"],\n    \"best_performing\": \"gpt-4\",\n    \"performance_gap\": 0.023,\n    \"statistical_significance\": \"p &lt; 0.05\",\n    \"recommendations\": [\n      \"GPT-4 shows superior performance across all metrics\",\n      \"Consider GPT-4 for production use\"\n    ]\n  }\n}\n</code></pre></p> <p>Note: This endpoint provides mock comparison data. Full implementation planned for future release.</p>"},{"location":"userGuide/benchmarkingapi/#6-get-benchmark-history","title":"6. Get Benchmark History","text":"<p>Endpoint: <code>GET /history</code></p> <p>Description: Retrieves historical benchmarking data with optional filtering.</p> <p>Query Parameters: - <code>model_name</code> (string, optional): Filter by model name - <code>days</code> (integer, optional, default: 30): Number of days to look back</p> <p>Success Response (200 OK): <pre><code>{\n  \"history\": [\n    {\n      \"job_id\": \"a967f363-ee96-4bac-9f52-4d169bbc4851\",\n      \"status\": \"completed\",\n      \"model_name\": \"mistral:latest\",\n      \"benchmark_suite\": \"glue\",\n      \"created_at\": \"2025-12-07T20:55:17Z\",\n      \"completed_at\": \"2025-12-07T20:56:44Z\"\n    }\n  ],\n  \"total_runs\": 3,\n  \"date_range\": \"Last 30 days\"\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#7-get-leaderboard","title":"7. Get Leaderboard","text":"<p>Endpoint: <code>GET /leaderboard</code></p> <p>Description: Retrieves a ranked leaderboard of models for specified criteria.</p> <p>Query Parameters: - <code>suite</code> (string, default: \"glue\"): Benchmark suite - <code>metric</code> (string, default: \"accuracy\"): Ranking metric - <code>limit</code> (integer, optional, default: 10): Number of top models to return</p> <p>Success Response (200 OK): <pre><code>{\n  \"leaderboard\": [\n    {\n      \"rank\": 1,\n      \"model\": \"gpt-4\",\n      \"score\": 0.892,\n      \"provider\": \"OpenAI\",\n      \"benchmark_count\": 5\n    }\n  ],\n  \"total_models\": 3,\n  \"last_updated\": \"2025-12-07T20:56:44Z\"\n}\n</code></pre></p> <p>Note: This endpoint provides mock leaderboard data. Full implementation planned for future release.</p>"},{"location":"userGuide/benchmarkingapi/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/benchmarkingapi/#common-error-responses","title":"Common Error Responses","text":"<p>400 Bad Request: <pre><code>{\n  \"detail\": \"Invalid benchmark suite. Supported: glue, superglue, mmlu, medical\"\n}\n</code></pre></p> <p>404 Not Found: <pre><code>{\n  \"detail\": \"Benchmark job a967f363-ee96-4bac-9f52-4d169bbc4851 not found\"\n}\n</code></pre></p> <p>429 Too Many Requests: <pre><code>{\n  \"detail\": \"Queue is full. Please try again later.\",\n  \"retry_after\": 300\n}\n</code></pre></p> <p>500 Internal Server Error: <pre><code>{\n  \"detail\": \"Benchmark execution failed: job_id and project_id are required\"\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#job-specific-errors","title":"Job-Specific Errors","text":"<p>Dataset Loading Errors: <pre><code>{\n  \"detail\": \"Failed to load GLUE dataset: Invalid pattern: '**' can only be an entire path component\"\n}\n</code></pre></p> <p>API Provider Errors: <pre><code>{\n  \"detail\": \"AI provider error: API key not configured for Gemini\"\n}\n</code></pre></p> <p>Resource Limit Errors: <pre><code>{\n  \"detail\": \"Job execution failed: Resource limits exceeded\"\n}\n</code></pre></p>"},{"location":"userGuide/benchmarkingapi/#rate-limiting-queue-management","title":"Rate Limiting &amp; Queue Management","text":"<ul> <li>Concurrent Jobs: Maximum 3 concurrent benchmarking jobs system-wide</li> <li>Queue Size: Maximum 10 queued jobs per user</li> <li>Job Timeout: 3 hours maximum execution time</li> <li>API Rate Limits: 100 requests per minute per user</li> </ul>"},{"location":"userGuide/benchmarkingapi/#queue-priorities","title":"Queue Priorities","text":"<ul> <li>High Priority: Interactive jobs (GUI/API initiated)</li> <li>Normal Priority: Background jobs</li> <li>Low Priority: Scheduled maintenance jobs</li> </ul>"},{"location":"userGuide/benchmarkingapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/benchmarkingapi/#job-management","title":"Job Management","text":"<ul> <li>Monitor job progress using real-time status updates</li> <li>Use appropriate AI models for your use case (Ollama for local, Gemini/Grok for API)</li> <li>Cancel unnecessary jobs to free up queue resources</li> <li>Check job status before starting new evaluations</li> </ul>"},{"location":"userGuide/benchmarkingapi/#model-selection","title":"Model Selection","text":"<ul> <li>Ollama: Best for local, private model evaluation</li> <li>Gemini: Good for Google's latest models with custom configuration</li> <li>Grok: Ideal for xAI models with advanced reasoning</li> </ul>"},{"location":"userGuide/benchmarkingapi/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>GLUE benchmarks typically take 10-30 minutes per model</li> <li>Schedule large benchmarking runs during off-peak hours</li> <li>Monitor system resources (CPU/memory) during execution</li> <li>Use appropriate Ollama parameters for your model size</li> </ul>"},{"location":"userGuide/benchmarkingapi/#result-analysis","title":"Result Analysis","text":"<ul> <li>Focus on accuracy as the primary metric for classification tasks</li> <li>Compare models using the same benchmark suite for fair evaluation</li> <li>Consider both mean performance and standard deviation</li> <li>Use historical data to track model performance trends</li> </ul>"},{"location":"userGuide/benchmarkingapi/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Check RQ worker logs for detailed error information</li> <li>Verify API keys are properly configured in environment variables</li> <li>Ensure sufficient system resources for benchmark execution</li> <li>Use smaller test runs before full benchmark suites</li> </ul>"},{"location":"userGuide/benchmarkinggui/","title":"Benchmarking in Compileo GUI","text":""},{"location":"userGuide/benchmarkinggui/#overview","title":"Overview","text":"<p>The Compileo GUI provides a comprehensive web interface for AI model benchmarking with full integration into the asynchronous job processing system. Users can run performance evaluations on Ollama, Gemini, and Grok models, compare results, track historical performance, and generate detailed reports.</p>"},{"location":"userGuide/benchmarkinggui/#accessing-benchmarking","title":"Accessing Benchmarking","text":"<ol> <li>Navigate to the Application: Open Compileo in your web browser</li> <li>Select Benchmarking: Click on \"\ud83d\udcca AI Model Benchmarking Dashboard\" in the sidebar</li> <li>Choose Operation: Select from Overview, Run Benchmarks, Model Comparison, History, or Leaderboard tabs</li> </ol>"},{"location":"userGuide/benchmarkinggui/#interface-components","title":"Interface Components","text":""},{"location":"userGuide/benchmarkinggui/#overview-tab","title":"Overview Tab","text":"<p>The overview dashboard provides key metrics and visualizations of recent benchmarking activity.</p>"},{"location":"userGuide/benchmarkinggui/#key-metrics-cards","title":"Key Metrics Cards","text":"<ul> <li>Average Accuracy: Shows mean accuracy scores across completed benchmarks</li> <li>Total Benchmark Runs: Count of completed evaluations</li> <li>Models Evaluated: Number of unique models tested</li> <li>Latest Run: Timestamp of most recent benchmark completion</li> </ul>"},{"location":"userGuide/benchmarkinggui/#filters","title":"Filters","text":"<ul> <li>Benchmark Suite: Select from GLUE (fully supported) or other suites (framework ready)</li> <li>Primary Metric: Choose accuracy or f1 score</li> <li>Days Back: Set time range (1-90 days, default: 30)</li> </ul>"},{"location":"userGuide/benchmarkinggui/#performance-trends-chart","title":"Performance Trends Chart","text":"<p>Visualizes performance over time with: - Line chart showing accuracy scores by completion date - Color-coded lines for different models - Interactive tooltips with detailed information - Date range filtering</p>"},{"location":"userGuide/benchmarkinggui/#recent-results-table","title":"Recent Results Table","text":"<p>Displays latest benchmark runs with: - Job ID and model name - Benchmark suite and completion status - Accuracy and F1 scores - Completion timestamps</p>"},{"location":"userGuide/benchmarkinggui/#run-benchmarks-tab","title":"Run Benchmarks Tab","text":"<p>Execute new benchmarking evaluations for AI models using Compileo's job queue system.</p>"},{"location":"userGuide/benchmarkinggui/#ai-provider-selection","title":"AI Provider Selection","text":"<ul> <li>Provider: Select from Ollama, Gemini, or Grok</li> <li>Dynamic Model Loading: Available models fetched automatically based on provider</li> <li>Provider-Specific Settings: Different configuration options per provider</li> </ul>"},{"location":"userGuide/benchmarkinggui/#ollama-configuration-local-models","title":"Ollama Configuration (Local Models)","text":"<ul> <li>Model Selection: Dropdown populated from available Ollama models</li> <li>Temperature: Sampling temperature (0.0-2.0)</li> <li>Top P: Nucleus sampling (0.0-1.0)</li> <li>Top K: Top-k sampling (0-100)</li> <li>Num Predict: Maximum tokens to generate</li> <li>Num Context: Context window size</li> <li>Seed: Random seed (optional)</li> </ul>"},{"location":"userGuide/benchmarkinggui/#geminigrok-configuration-api-models","title":"Gemini/Grok Configuration (API Models)","text":"<ul> <li>Model Selection: Available models for the provider</li> <li>Custom Configuration: JSON configuration for advanced options</li> <li>API Key: Automatically loaded from settings (environment variables)</li> </ul>"},{"location":"userGuide/benchmarkinggui/#benchmark-configuration","title":"Benchmark Configuration","text":"<ul> <li>Benchmark Suite: Currently supports GLUE (fully implemented)</li> <li>Project ID: Associated project for the benchmark job</li> </ul>"},{"location":"userGuide/benchmarkinggui/#benchmark-execution","title":"Benchmark Execution","text":"<ol> <li>Select Provider: Choose Ollama, Gemini, or Grok</li> <li>Choose Model: Select from available models for the provider</li> <li>Configure Parameters: Set provider-specific options</li> <li>Start Benchmarking: Click \"\ud83d\ude80 Start Benchmarking\"</li> <li>Monitor Progress: Real-time progress tracking with:</li> <li>Job ID display</li> <li>Status updates (pending \u2192 running \u2192 completed/failed)</li> <li>Auto-refresh options (1, 5, 30, or 60 second intervals)</li> <li>Progress percentage and current step information</li> <li>Stop Job: Button to cancel running benchmarks</li> </ol>"},{"location":"userGuide/benchmarkinggui/#progress-monitoring","title":"Progress Monitoring","text":"<ul> <li>Real-time Updates: Automatic status polling</li> <li>Job Persistence: Jobs continue running even if browser is closed</li> <li>Job Cancellation: Stop long-running jobs directly from the interface</li> <li>Result Access: Direct links to results when complete</li> <li>Error Handling: Clear error messages and recovery options</li> </ul>"},{"location":"userGuide/benchmarkinggui/#model-comparison-tab","title":"Model Comparison Tab","text":"<p>Compare performance across multiple AI models (framework ready - implementation pending).</p>"},{"location":"userGuide/benchmarkinggui/#model-selection","title":"Model Selection","text":"<ul> <li>Available Models: List of models with completed benchmarks</li> <li>Multi-select: Choose multiple models for comparison</li> <li>Suite Selection: Filter by benchmark suite</li> </ul>"},{"location":"userGuide/benchmarkinggui/#comparison-features-planned","title":"Comparison Features (Planned)","text":"<ul> <li>Performance Gap Analysis: Difference between best and worst models</li> <li>Statistical Significance: p-value calculations</li> <li>Visual Comparisons: Charts and graphs for easy interpretation</li> </ul>"},{"location":"userGuide/benchmarkinggui/#history-tab","title":"History Tab","text":"<p>Review past benchmarking runs with comprehensive filtering and search.</p>"},{"location":"userGuide/benchmarkinggui/#filters_1","title":"Filters","text":"<ul> <li>Model Name: Text search for specific models</li> <li>Suite Filter: Dropdown for benchmark suite selection</li> <li>Status Filter: Filter by job status (All, completed, running, failed)</li> </ul>"},{"location":"userGuide/benchmarkinggui/#history-table","title":"History Table","text":"<p>Displays filtered results with: - Job ID and model name - Benchmark suite and completion status - Created and completed timestamps - Export options (CSV, JSON)</p>"},{"location":"userGuide/benchmarkinggui/#summary-statistics","title":"Summary Statistics","text":"<ul> <li>Total Runs: Count of matching benchmark jobs</li> <li>Completed Runs: Successful evaluations</li> <li>Success Rate: Percentage of successful runs</li> </ul>"},{"location":"userGuide/benchmarkinggui/#leaderboard-tab","title":"Leaderboard Tab","text":"<p>Ranked performance comparison across all evaluated models (framework ready - implementation pending).</p>"},{"location":"userGuide/benchmarkinggui/#leaderboard-settings","title":"Leaderboard Settings","text":"<ul> <li>Benchmark Suite: Select evaluation framework (default: GLUE)</li> <li>Ranking Metric: Choose performance metric (default: accuracy)</li> <li>Show Top N: Display configurable number of top models</li> </ul>"},{"location":"userGuide/benchmarkinggui/#leaderboard-features-planned","title":"Leaderboard Features (Planned)","text":"<ul> <li>Ranked List: Ordered by performance metrics</li> <li>Model Details: Name, provider, and score information</li> <li>Benchmark Count: Number of evaluations per model</li> <li>Trend Indicators: Performance change indicators</li> </ul>"},{"location":"userGuide/benchmarkinggui/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/benchmarkinggui/#common-error-types","title":"Common Error Types","text":"<ul> <li>\ud83d\udd0c Connection Error: Network issues preventing API communication</li> <li>\u23f1\ufe0f Timeout Error: Benchmark execution exceeding 3-hour limit</li> <li>\ud83e\udd16 Model API Error: Issues with AI provider APIs (Gemini, Grok)</li> <li>\ud83d\udcca Invalid Configuration: Incorrect benchmark parameters</li> <li>\ud83d\udc0c Rate Limit Exceeded: Too many concurrent benchmarking jobs</li> <li>\ud83d\udcbe Database Error: Issues with result storage or retrieval</li> </ul>"},{"location":"userGuide/benchmarkinggui/#error-recovery","title":"Error Recovery","text":"<ul> <li>Automatic Retry: RQ worker handles transient failures</li> <li>Job Persistence: Failed jobs can be restarted</li> <li>Configuration Validation: Frontend validates parameters before submission</li> <li>Resource Monitoring: System prevents resource exhaustion</li> </ul>"},{"location":"userGuide/benchmarkinggui/#export-capabilities","title":"Export Capabilities","text":""},{"location":"userGuide/benchmarkinggui/#data-export-formats","title":"Data Export Formats","text":"<ul> <li>CSV Export: Tabular data for spreadsheet analysis</li> <li>JSON Export: Structured data for programmatic processing</li> </ul>"},{"location":"userGuide/benchmarkinggui/#export-locations","title":"Export Locations","text":"<ul> <li>Overview Tab: Export recent results and performance data</li> <li>History Tab: Export filtered historical benchmark data</li> </ul>"},{"location":"userGuide/benchmarkinggui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/benchmarkinggui/#benchmark-planning","title":"Benchmark Planning","text":"<ul> <li>Provider Selection: Choose Ollama for local/private, Gemini/Grok for API models</li> <li>Model Selection: Use appropriate model sizes for your hardware capabilities</li> <li>Resource Planning: GLUE benchmarks take 10-30 minutes per model</li> </ul>"},{"location":"userGuide/benchmarkinggui/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Progress Tracking: Use real-time monitoring for job status</li> <li>Resource Awareness: Monitor system impact during benchmarking</li> <li>Result Validation: Verify benchmark results are reasonable</li> </ul>"},{"location":"userGuide/benchmarkinggui/#data-management","title":"Data Management","text":"<ul> <li>Regular Exports: Save important results for future reference</li> <li>Historical Analysis: Use History tab for performance trend analysis</li> <li>Job Management: Cancel unnecessary jobs to free queue resources</li> </ul>"},{"location":"userGuide/benchmarkinggui/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"userGuide/benchmarkinggui/#dataset-generation-workflow","title":"Dataset Generation Workflow","text":"<ol> <li>Run Benchmarks: Evaluate model performance on GLUE tasks</li> <li>Analyze Results: Review accuracy and F1 scores</li> <li>Model Selection: Choose best-performing models for dataset generation</li> <li>Quality Validation: Use benchmark results to inform quality thresholds</li> </ol>"},{"location":"userGuide/benchmarkinggui/#job-queue-integration","title":"Job Queue Integration","text":"<ul> <li>Asynchronous Processing: All benchmarks run via RQ job queue</li> <li>Resource Management: Automatic resource allocation and monitoring</li> <li>Progress Tracking: Real-time status updates across all interfaces</li> <li>Error Recovery: Automatic retry and failure handling</li> </ul>"},{"location":"userGuide/benchmarkinggui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/benchmarkinggui/#jobs-not-starting","title":"Jobs Not Starting","text":"<ul> <li>Queue Full: Wait for current jobs to complete (max 3 concurrent)</li> <li>Invalid Parameters: Check model selection and configuration</li> <li>API Keys: Verify environment variables for Gemini/Grok</li> <li>Worker Status: Ensure RQ worker is running</li> </ul>"},{"location":"userGuide/benchmarkinggui/#progress-not-updating","title":"Progress Not Updating","text":"<ul> <li>Browser Cache: Hard refresh to clear cached data</li> <li>Connection Issues: Check network connectivity to Compileo</li> <li>Job Completion: Very long-running jobs may show delayed updates</li> </ul>"},{"location":"userGuide/benchmarkinggui/#results-not-appearing","title":"Results Not Appearing","text":"<ul> <li>Job Status: Wait for job to reach \"completed\" status</li> <li>Error Checking: Review job details for failure reasons</li> <li>Database Issues: Check system logs for storage problems</li> </ul>"},{"location":"userGuide/benchmarkinggui/#provider-specific-issues","title":"Provider-Specific Issues","text":""},{"location":"userGuide/benchmarkinggui/#ollama-problems","title":"Ollama Problems","text":"<ul> <li>Model Not Available: Check <code>ollama list</code> for installed models</li> <li>Connection Failed: Verify Ollama server is running on port 11434</li> <li>Parameter Errors: Validate temperature/top_p ranges</li> </ul>"},{"location":"userGuide/benchmarkinggui/#geminigrok-problems","title":"Gemini/Grok Problems","text":"<ul> <li>API Key Missing: Check environment variables <code>GOOGLE_API_KEY</code>/<code>GROK_API_KEY</code></li> <li>Rate Limits: Wait before retrying (provider-specific limits)</li> <li>Model Access: Verify API key has access to requested models</li> </ul>"},{"location":"userGuide/benchmarkinggui/#advanced-features","title":"Advanced Features","text":""},{"location":"userGuide/benchmarkinggui/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"userGuide/benchmarkinggui/#ollama-advanced-settings","title":"Ollama Advanced Settings","text":"<ul> <li>Temperature Control: Fine-tune randomness (0.0-2.0)</li> <li>Context Window: Adjust token limits for large models</li> <li>Sampling Parameters: Top-k, top-p, and repetition penalty</li> <li>Seed Control: Reproducible results with fixed seeds</li> </ul>"},{"location":"userGuide/benchmarkinggui/#api-provider-options","title":"API Provider Options","text":"<ul> <li>Custom Configuration: JSON settings for advanced parameters</li> <li>Model Selection: Latest available models from providers</li> <li>Error Handling: Automatic retry with exponential backoff</li> </ul>"},{"location":"userGuide/benchmarkinggui/#job-management","title":"Job Management","text":"<ul> <li>Queue Monitoring: Real-time queue status and worker health</li> <li>Job Cancellation: Stop running jobs via GUI button or CLI</li> <li>Result Persistence: All results stored in SQLite database</li> <li>Historical Tracking: Complete audit trail of all benchmarks</li> </ul>"},{"location":"userGuide/benchmarkinggui/#system-integration","title":"System Integration","text":"<ul> <li>Environment Variables: Secure API key management</li> <li>Database Transactions: Atomic operations for data consistency</li> <li>Logging: Comprehensive debug logging for troubleshooting</li> <li>Health Checks: System monitoring and automatic recovery</li> </ul> <p>This benchmarking interface provides production-ready AI model evaluation capabilities with seamless integration into Compileo's asynchronous processing system.</p>"},{"location":"userGuide/chunkapi/","title":"Chunk Module API Usage Guide","text":"<p>The Compileo Chunk Module provides a flexible API for document chunking with multiple strategies. This guide covers how to use the chunking API endpoints programmatically.</p>"},{"location":"userGuide/chunkapi/#chunking-strategy-options","title":"Chunking Strategy Options","text":"<pre><code>graph TD\n    A[Choose Chunking Strategy] --&gt; B{Strategy Type}\n    B --&gt; C[Character]\n    B --&gt; D[Semantic]\n    B --&gt; E[Schema]\n    B --&gt; F[Delimiter]\n    B --&gt; G[Token]\n\n    C --&gt; C1[chunk_size: int]\n    C --&gt; C2[overlap: int]\n\n    D --&gt; D1[semantic_prompt: string]\n\n    E --&gt; E1[schema_definition: string]\n\n    F --&gt; F1[delimiter: string]\n\n    G --&gt; G1[chunk_size: int]\n    G --&gt; G2[overlap: int]\n</code></pre>"},{"location":"userGuide/chunkapi/#api-endpoints","title":"API Endpoints","text":""},{"location":"userGuide/chunkapi/#chunk-retrieval","title":"Chunk Retrieval","text":"<p>GET <code>/api/v1/chunks/document/{document_id}</code></p> <p>Retrieve all chunks for a specific document.</p> <p>Response: <pre><code>{\n  \"document_id\": 1,\n  \"chunks\": [\n    {\n      \"id\": 1,\n      \"chunk_index\": 1,\n      \"token_count\": 278,\n      \"file_path\": \"storage/chunks/1/1/chunk_1.md\",\n      \"content_preview\": \"# PROGNOSIS\\nMost patients respond well...\",\n      \"chunk_strategy\": \"schema\"\n    }\n  ],\n  \"total\": 5\n}\n</code></pre></p> <p>GET <code>/api/v1/chunks/project/{project_id}</code></p> <p>Retrieve chunks for all documents associated with a specific project. Useful for validating project-wide processing status.</p> <p>Query Parameters: - <code>limit</code>: Maximum number of chunks to return (default: 100)</p> <p>Response: <pre><code>{\n  \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n  \"chunks\": [\n    {\n      \"id\": \"uuid-1\",\n      \"document_id\": \"uuid-doc-1\",\n      \"chunk_index\": 1,\n      \"chunk_strategy\": \"semantic\",\n      \"file_path\": \"storage/chunks/...\"\n    }\n  ],\n  \"total\": 1\n}\n</code></pre></p>"},{"location":"userGuide/chunkapi/#chunk-deletion","title":"Chunk Deletion","text":"<p>DELETE <code>/api/v1/chunks/{chunk_id}</code></p> <p>Delete a specific chunk.</p> <p>Response: <pre><code>{\n  \"message\": \"Chunk 1 deleted successfully\"\n}\n</code></pre></p> <p>DELETE <code>/api/v1/chunks/batch</code></p> <p>Delete multiple chunks by their IDs. Supports flexible input formats for bulk operations.</p> <p>Request Body: <pre><code>{\n  \"chunk_ids\": [1, 2, 3, 5, 7, 10]\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Successfully deleted 6 chunks\",\n  \"deleted_count\": 6\n}\n</code></pre></p> <p>DELETE <code>/api/v1/chunks/document/{document_id}</code></p> <p>Delete all chunks for a document.</p> <p>Response: <pre><code>{\n  \"message\": \"Deleted 5 chunks for document 1\",\n  \"deleted_count\": 5\n}\n</code></pre></p>"},{"location":"userGuide/chunkapi/#document-processing","title":"Document Processing","text":"<p>POST <code>/api/v1/documents/process</code></p> <p>Process documents with specified chunking strategy and parameters.</p>"},{"location":"userGuide/chunkapi/#ai-assisted-analysis","title":"AI-Assisted Analysis","text":"<p>POST <code>/api/v1/documents/analyze-chunking</code></p> <p>Get AI recommendations for optimal chunking strategy based on document analysis.</p>"},{"location":"userGuide/chunkapi/#ai-analysis-request-parameters","title":"AI Analysis Request Parameters","text":"Parameter Type Required Description <code>document_id</code> integer Yes ID of document to analyze <code>goal</code> string Yes Description of chunking objective <code>examples</code> array No List of example strings from document <code>model</code> string No AI model for analysis (<code>gemini</code>, <code>grok</code>, <code>ollama</code>)"},{"location":"userGuide/chunkapi/#ai-analysis-example-request","title":"AI Analysis Example Request","text":"<pre><code>{\n  \"document_id\": 101,\n  \"goal\": \"Split the document at every chapter, but each chapter has a different name and format\",\n  \"examples\": [\n    \"Page 1: Headers: # Chapter 1: Introduction\",\n    \"Page 3: Section: This chapter provides an overview...\",\n    \"Page 5: Selected: Each new chapter starts with a level 1 header\"\n  ],\n  \"model\": \"gemini\"\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#ai-analysis-response","title":"AI Analysis Response","text":"<pre><code>{\n  \"recommended_strategy\": \"schema\",\n  \"parameters\": {\n    \"json_schema\": \"{\\\"rules\\\": [{\\\"type\\\": \\\"pattern\\\", \\\"value\\\": \\\"^# \\\"}, {\\\"type\\\": \\\"delimiter\\\", \\\"value\\\": \\\"\\\\n\\\\n\\\"}], \\\"combine\\\": \\\"any\\\"}\",\n    \"explanation\": \"Schema-based chunking recommended for consistent chapter header patterns\"\n  },\n  \"confidence\": 0.85,\n  \"alternative_strategies\": [\n    {\n      \"strategy\": \"semantic\",\n      \"parameters\": {\"custom_prompt\": \"Split at chapter boundaries...\"},\n      \"confidence\": 0.72\n    }\n  ]\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#request-parameters","title":"Request Parameters","text":"Parameter Type Required Description <code>project_id</code> integer Yes ID of the project containing documents <code>document_ids</code> array Yes List of document IDs to process <code>parser</code> string No Document parser (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>) <code>chunk_strategy</code> string No Chunking strategy (<code>token</code>, <code>character</code>, <code>semantic</code>, <code>delimiter</code>, <code>schema</code>) <code>chunk_size</code> integer No Chunk size (tokens for token strategy, characters for character strategy) <code>overlap</code> integer No Overlap between chunks <code>num_ctx</code> integer No Context window size for Ollama models (overrides default setting) <code>semantic_prompt</code> string No Custom prompt for semantic chunking <code>schema_definition</code> string No JSON schema for schema-based chunking <code>character_chunk_size</code> integer No Character chunk size (overrides chunk_size) <code>character_overlap</code> integer No Character overlap (overrides overlap) <code>sliding_window</code> boolean No Enable sliding window chunking for multi-file documents (auto-enabled for multi-file docs) <code>system_instruction</code> string No System-level instructions to guide the model's behavior, especially for Gemini."},{"location":"userGuide/chunkapi/#character-based-chunking","title":"Character-Based Chunking","text":"<p>Split documents by character count with configurable overlap. Fast and deterministic.</p>"},{"location":"userGuide/chunkapi/#example-request","title":"Example Request","text":"<pre><code>{\n  \"project_id\": 1,\n  \"document_ids\": [101, 102],\n  \"parser\": \"pypdf\",\n  \"chunk_strategy\": \"character\",\n  \"character_chunk_size\": 500,\n  \"character_overlap\": 50\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#use-cases","title":"Use Cases","text":"<ul> <li>Fixed-size text processing</li> <li>Memory-constrained environments</li> <li>Deterministic chunking results</li> <li>Simple document structures</li> </ul>"},{"location":"userGuide/chunkapi/#semantic-chunking","title":"Semantic Chunking","text":"<p>Use AI to intelligently split documents based on meaning and context. Supports multi-file documents with dynamic cross-file chunking for semantic coherence.</p>"},{"location":"userGuide/chunkapi/#simplified-universal-cross-file-document-support","title":"Simplified Universal Cross-File Document Support","text":"<p>The API automatically handles multi-file documents using universal forwarding logic that ensures semantic coherence across file boundaries:</p> <ul> <li>Universal Forwarding Rules: All chunking strategies use the same simple rule - if content remains at the end, forward it to the next file</li> <li>Strategy-Agnostic Detection: Removed complex per-strategy incomplete chunk detection code</li> <li>Automatic Content Spacing: Intelligent space insertion between forwarded content and main content prevents word concatenation</li> <li>Memory-Based State Management: Simplified ChunkState object maintains forwarded content between file processing</li> </ul> <p>Automatic Processing: Cross-file chunking is automatically applied to multi-file documents. The system dynamically forwards incomplete chunks as overlap content to subsequent files.</p> <p>Benefits: - Improved semantic chunking quality at file boundaries - Better search results with reduced duplication - More coherent chunks for AI processing - Simplified architecture with universal forwarding rules - All 5 chunking strategies (character, token, semantic, schema, delimiter) use identical logic</p>"},{"location":"userGuide/chunkapi/#example-request_1","title":"Example Request","text":"<p><pre><code>{\n  \"project_id\": 1,\n  \"document_ids\": [101],\n  \"parser\": \"ollama\",\n  \"chunk_strategy\": \"semantic\",\n  \"semantic_prompt\": \"This is a medical textbook that is structured as follows: disease / condition and discussion about it, then another disease / condition and discussion about it. Split should occur at the end of each discussion and before next disease / condition title.\",\n  \"num_ctx\": 4096\n}```\n\n### Prompt Examples\n\n**General Purpose (Recommended for all models including Gemini):**\n</code></pre> You are an expert document analysis tool. Your task is to split a document into logical chunks based on the user's instruction. You will be given an instruction and the document text. You must identify the exact headings or titles that mark the beginning of a new chunk according to the instruction.</p> <p>User Instruction:</p> <p>Output Requirements: - Return ONLY a comma-separated list of the exact heading strings that should start a new chunk. - Do not include any other text, explanations, or formatting. - Each heading should be exactly as it appears in the document.</p> <p>Example: If the instruction is \"Split by chapter\" and the text contains \"# Chapter 1\" and \"# Chapter 2\", your output should be:</p>"},{"location":"userGuide/chunkapi/#chapter-1-chapter-2","title":"Chapter 1,# Chapter 2","text":"<p>Document to analyze: <pre><code>**Medical Textbooks (Example of specific user_instruction):**\n</code></pre> This is a medical textbook that is structured as follows: disease / condition and discussion about it, then another disease / condition and discussion about it. Split should occur at the end of each discussion and before next disease / condition title. <pre><code>**General Medical Documents (Example of specific user_instruction):**\n</code></pre> Split this medical document at natural section boundaries, ensuring each chunk contains complete clinical information about a single condition, symptom, or treatment. <pre><code>**Legal Documents (Example of specific user_instruction):**\n</code></pre> Divide this legal document at section boundaries, keeping each complete legal clause, definition, or contractual obligation in a single chunk. <pre><code>**Technical Documentation (Example of specific user_instruction):**\n</code></pre> Split this technical document at logical boundaries, ensuring each chunk contains complete explanations of single concepts, algorithms, or procedures. <pre><code>### Use Cases\n- Complex document structures\n- Meaning preservation\n- Context-aware splitting\n- Domain-specific requirements\n- Medical textbooks and clinical documents\n- Multi-file document processing\n\n### Ollama Context Window Configuration\n\nWhen using Ollama models for semantic chunking, you can control the context window size:\n\n- **`num_ctx`**: Specifies the maximum context length in tokens for Ollama models\n- **Default**: Uses the value configured in GUI settings (typically 60000 tokens)\n- **Override**: API parameter takes precedence over settings default\n- **Performance**: Smaller values reduce memory usage but may limit complex analysis\n- **Compatibility**: Only applies to Ollama models; ignored for Gemini/Grok\n\n### Recent Improvements\n\n**Simplified Universal Cross-File Chunking Architecture:**\n- **Universal Forwarding Rules**: All chunking strategies use the same simple rule - if content remains at end, forward it to next file\n- **Strategy-Agnostic Detection**: Removed 50+ lines of complex per-strategy incomplete chunk detection code\n- **Automatic Content Spacing**: Intelligent space insertion between forwarded content and main content prevents word concatenation\n- **Memory-Based State Management**: Simplified ChunkState object maintains forwarded content between file processing\n- **All Strategies Unified**: Character, token, semantic, schema, and delimiter strategies use identical cross-file logic\n\n**Enhanced Content Processing:**\n- **Intelligent Spacing**: Automatic space insertion prevents issues like \"glutendamages\" \u2192 \"gluten damages\"\n- **Simplified Architecture**: Single forwarding mechanism instead of strategy-specific code\n- **Memory Efficient**: No duplicate content storage or complex overlap calculations\n- **Universal Compatibility**: Works identically across all 5 chunking strategies\n\n**Streamlined Implementation:**\n- **Removed Strategy-Specific Code**: Eliminated complex per-strategy incomplete chunk detection logic\n- **Dynamic Overlap Generation**: Overlap created naturally during chunking, not pre-computed\n- **Simplified Data Structures**: Clean content processing with automatic forwarding\n- **Improved Performance**: Reduced complexity and memory usage in cross-file processing\n\n**Enhanced Quality Assurance:**\n- **Comprehensive Testing**: Verified all chunking strategies work correctly with cross-file processing\n- **Real-World Validation**: Tested on medical documents with proper semantic coherence\n- **Spacing Integrity**: Automatic prevention of word concatenation across file boundaries\n- **Universal Logic**: Same forwarding rules apply to all strategies regardless of complexity\n\n## Dynamic Cross-File Chunking\n\nAdvanced chunking method that dynamically generates overlap content during processing, ensuring semantic coherence across file boundaries. Automatically applied to multi-file documents with guaranteed boundary integrity.\n\n### How It Works\n\n1. **Sequential File Processing**: Files are processed one by one in order\n2. **Universal Forwarding Logic**: All chunking strategies use the same simple rule - if content remains at the end, forward it to the next file\n3. **Automatic Content Spacing**: Intelligent space insertion between forwarded content and main content prevents word concatenation\n4. **Memory-Based State Management**: Simplified ChunkState object maintains forwarded content between file processing\n5. **Strategy Transparency**: Chunking engines unaware of cross-file logic - they process complete content\n\n### Processing Flow\n</code></pre> File 1 Processing: \u251c\u2500\u2500 Apply chunking strategy to main_content \u251c\u2500\u2500 Create complete chunks + leftover content \u251c\u2500\u2500 Forward leftover \u2192 File 2's overlap_content</p> <p>File 2 Processing: \u251c\u2500\u2500 Combine: overlap_from_file1 + separator + main_content \u251c\u2500\u2500 Apply chunking strategy to combined content \u251c\u2500\u2500 Create complete chunks + new leftover content \u251c\u2500\u2500 Forward new leftover \u2192 File 3's overlap_content</p> <p>File N Processing: \u251c\u2500\u2500 Combine: overlap_from_prev + separator + main_content \u251c\u2500\u2500 Apply chunking strategy \u251c\u2500\u2500 Create final complete chunks <pre><code>### Universal Forwarding Mechanism\n\nThe architecture ensures **100% boundary integrity** with simplified logic:\n\n- **Parsing**: Creates clean `main_content` without overlap assumptions\n- **First File**: Processes from natural document start\n- **Subsequent Files**: Receive overlap as guaranteed start boundary\n- **All Strategies**: Use identical forwarding rules regardless of chunking method\n- **Automatic Spacing**: Prevents word concatenation (e.g., \"glutendamages\" \u2192 \"gluten damages\")\n\n### Example Request\n\n```json\n{\n  \"project_id\": 1,\n  \"document_ids\": [101, 102, 103],\n  \"parser\": \"gemini\",\n  \"chunk_strategy\": \"semantic\",\n  \"sliding_window\": true\n}\n</code></pre></p>"},{"location":"userGuide/chunkapi/#window-processing-structure","title":"Window Processing Structure","text":"<p>Each window contains structured content for AI processing:</p> <pre><code>{\n  \"content_type\": \"sliding_window_chunk\",\n  \"main_content\": \"# Chapter 3: Advanced Topics\\n\\nThis chapter covers...\",\n  \"overlap_content\": \"# Chapter 2 Conclusion\\n\\nIn summary, the basic concepts...\",\n  \"metadata\": {\n    \"window_size\": 2500,\n    \"overlap_tokens\": 400,\n    \"total_tokens\": 2900\n  }\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#use-cases_1","title":"Use Cases","text":"<ul> <li>Large Multi-File Documents: PDFs split into multiple parts</li> <li>Cross-File Continuity: Topics spanning artificial file boundaries</li> <li>Semantic Coherence: Maintaining context across pagination breaks</li> <li>Quality Improvement: Better search results and AI processing</li> </ul>"},{"location":"userGuide/chunkapi/#configuration-options","title":"Configuration Options","text":"<ul> <li>Automatic Detection: Applied automatically to documents with multiple parsed files</li> </ul>"},{"location":"userGuide/chunkapi/#schema-based-chunking","title":"Schema-Based Chunking","text":"<p>Apply custom rules combining patterns and delimiters for precise control.</p>"},{"location":"userGuide/chunkapi/#schema-format","title":"Schema Format","text":"<p>The API automatically attempts to fix common JSON syntax errors in regex patterns (e.g., unescaped <code>\\s</code>, <code>\\n</code>) and literal control characters, but it is best practice to provide a fully escaped JSON string.</p> <p>Regex Support: Schema strategies now support <code>re.MULTILINE</code>, allowing the use of <code>^</code> anchors to match the start of lines within a document.</p> <pre><code>{\n  \"rules\": [\n    {\n      \"type\": \"pattern\",\n      \"value\": \"# [A-Z\\\\\\\\s]+\"\n    },\n    {\n      \"type\": \"delimiter\",\n      \"value\": \"\\\\\\\\n\\\\\\\\n\"\n    }\n  ],\n  \"combine\": \"any\"\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#combine-options","title":"Combine Options","text":"<ul> <li><code>\"any\"</code>: Split when any rule matches</li> <li><code>\"all\"</code>: Split only when all rules match at the same position</li> </ul>"},{"location":"userGuide/chunkapi/#example-request_2","title":"Example Request","text":"<pre><code>{\n  \"project_id\": 1,\n  \"document_ids\": [101],\n  \"parser\": \"unstructured\",\n  \"chunk_strategy\": \"schema\",\n  \"schema_definition\": \"{\\\"rules\\\": [{\\\"type\\\": \\\"pattern\\\", \\\"value\\\": \\\"^## \\\"}, {\\\"type\\\": \\\"delimiter\\\", \\\"value\\\": \\\"\\\\n\\\\n\\\"}], \\\"combine\\\": \\\"any\\\"}\"\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#rule-types","title":"Rule Types","text":"<p>Pattern Rules: - Use regex patterns to match specific text structures - Supports <code>re.MULTILINE</code> mode (use <code>^</code> to match start of line) - Examples: <code>\"^# \"</code>, <code>\"^[0-9]+\\.\"</code>, <code>\"&lt;chapter&gt;\"</code></p> <p>Delimiter Rules: - Split on exact string matches - Examples: <code>\"\\n\\n\"</code>, <code>\"&lt;hr&gt;\"</code>, <code>\"---\"</code></p>"},{"location":"userGuide/chunkapi/#use-cases_2","title":"Use Cases","text":"<ul> <li>Structured documents with known patterns</li> <li>Custom document formats</li> <li>Precise control requirements</li> <li>Multi-criteria splitting</li> </ul>"},{"location":"userGuide/chunkapi/#delimiter-based-chunking","title":"Delimiter-Based Chunking","text":"<p>Simple splitting on specified delimiter strings with enhanced flexibility for multiple delimiters.</p>"},{"location":"userGuide/chunkapi/#parameters","title":"Parameters","text":"Parameter Type Required Description <code>delimiters</code> array No List of delimiter strings to split on (default: <code>[\"\\n\\n\", \"\\n\"]</code>) <code>chunk_size</code> integer No Maximum chunk size in characters <code>overlap</code> integer No Overlap between chunks in characters"},{"location":"userGuide/chunkapi/#example-request_3","title":"Example Request","text":"<pre><code>{\n  \"project_id\": 1,\n  \"document_ids\": [101],\n  \"parser\": \"pypdf\",\n  \"chunk_strategy\": \"delimiter\",\n  \"delimiters\": [\"#\", \"\\n\\n\", \"---\"],\n  \"chunk_size\": 1000,\n  \"overlap\": 100\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#delimiter-examples","title":"Delimiter Examples","text":"<p>Markdown Headers: <pre><code>{\n  \"delimiters\": [\"#\", \"##\", \"###\"]\n}\n</code></pre></p> <p>Mixed Delimiters: <pre><code>{\n  \"delimiters\": [\"\\n\\n\", \"---\", \"&lt;hr&gt;\"]\n}\n</code></pre></p> <p>Custom Patterns: <pre><code>{\n  \"delimiters\": [\"SECTION:\", \"CHAPTER\", \"&lt;div class=\\\"chapter\\\"&gt;\"]\n}\n</code></pre></p>"},{"location":"userGuide/chunkapi/#use-cases_3","title":"Use Cases","text":"<ul> <li>Simple document structures</li> <li>Known separator patterns</li> <li>Quick processing needs</li> <li>Markdown document chunking</li> <li>Custom delimiter patterns</li> </ul>"},{"location":"userGuide/chunkapi/#token-based-chunking","title":"Token-Based Chunking","text":"<p>Precise token counting using tiktoken library with overlap. Requires tiktoken package to be installed.</p>"},{"location":"userGuide/chunkapi/#example-request_4","title":"Example Request","text":"<pre><code>{\n  \"project_id\": 1,\n  \"document_ids\": [101],\n  \"parser\": \"grok\",\n  \"chunk_strategy\": \"token\",\n  \"chunk_size\": 512,\n  \"overlap\": 50\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#error-handling","title":"Error Handling","text":"<p>Token chunking will fail explicitly if: - <code>tiktoken</code> library is not installed - Invalid tokenizer model specified - Strategy creation fails for any reason</p> <p>Error Response: <pre><code>{\n  \"detail\": \"Failed to create token-based chunking strategy: No module named 'tiktoken'. Token-based chunking requires tiktoken library.\"\n}\n</code></pre></p>"},{"location":"userGuide/chunkapi/#use-cases_4","title":"Use Cases","text":"<ul> <li>LLM input preparation with exact token limits</li> <li>Token-aware processing for API constraints</li> <li>Precise semantic chunking based on token boundaries</li> </ul>"},{"location":"userGuide/chunkapi/#response-format","title":"Response Format","text":""},{"location":"userGuide/chunkapi/#success-response","title":"Success Response","text":"<pre><code>{\n  \"job_id\": \"chunk_job_12345\",\n  \"message\": \"Successfully processed 1 documents, created 5 chunks\",\n  \"processed_documents\": 1,\n  \"total_chunks\": 5,\n  \"estimated_duration\": \"Completed\",\n  \"debug_info\": {\n    \"total_requested\": 1,\n    \"project_id\": 1,\n    \"parser\": \"pypdf\",\n    \"chunk_strategy\": \"character\",\n    \"character_chunk_size\": 500,\n    \"character_overlap\": 50\n  }\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#job-status-checking","title":"Job Status Checking","text":"<p>GET <code>/api/v1/documents/process/{job_id}/status</code></p> <pre><code>{\n  \"status\": \"completed\",\n  \"result\": {\n    \"processed_documents\": 1,\n    \"total_chunks\": 5\n  }\n}\n</code></pre>"},{"location":"userGuide/chunkapi/#error-handling_1","title":"Error Handling","text":""},{"location":"userGuide/chunkapi/#common-errors","title":"Common Errors","text":"<p>400 Bad Request: <pre><code>{\n  \"detail\": \"Invalid chunk_strategy. Must be one of: token, character, semantic, delimiter, schema\"\n}\n</code></pre></p> <p>422 Validation Error: <pre><code>{\n  \"detail\": [\n    {\n      \"loc\": [\"body\", \"character_chunk_size\"],\n      \"msg\": \"ensure this value is greater than 0\",\n      \"type\": \"value_error.const\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"userGuide/chunkapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/chunkapi/#strategy-selection","title":"Strategy Selection","text":"<ul> <li>Character: For simple, fast processing</li> <li>Semantic: For complex documents requiring AI understanding</li> <li>Schema: For structured documents with known patterns</li> <li>Delimiter: For simple separator-based splitting</li> <li>Token: For LLM-specific token limits</li> </ul>"},{"location":"userGuide/chunkapi/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Use character strategy for large volumes</li> <li>Batch multiple documents together</li> <li>Choose appropriate parsers for document types</li> <li>Monitor job status for long-running processes</li> </ul>"},{"location":"userGuide/chunkapi/#error-prevention","title":"Error Prevention","text":"<ul> <li>Validate schema JSON before submission</li> <li>Test prompts with sample documents</li> <li>Use appropriate chunk sizes for your use case</li> <li>Monitor API rate limits for AI strategies</li> </ul>"},{"location":"userGuide/chunkapi/#integration-examples","title":"Integration Examples","text":""},{"location":"userGuide/chunkapi/#python-client","title":"Python Client","text":"<pre><code>import requests\n\n# Character chunking\nresponse = requests.post(\"http://localhost:8000/api/v1/documents/process\", json={\n    \"project_id\": 1,\n    \"document_ids\": [101],\n    \"chunk_strategy\": \"character\",\n    \"character_chunk_size\": 500,\n    \"character_overlap\": 50\n})\n\n# Semantic chunking\nresponse = requests.post(\"http://localhost:8000/api/v1/documents/process\", json={\n    \"project_id\": 1,\n    \"document_ids\": [101],\n    \"chunk_strategy\": \"semantic\",\n    \"semantic_prompt\": \"Split at natural topic boundaries...\"\n})\n</code></pre>"},{"location":"userGuide/chunkapi/#javascript-client","title":"JavaScript Client","text":"<pre><code>// Character chunking\nconst response = await fetch('/api/v1/documents/process', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    project_id: 1,\n    document_ids: [101],\n    chunk_strategy: 'character',\n    character_chunk_size: 500,\n    character_overlap: 50\n  })\n});\n</code></pre> <p>This API provides comprehensive document chunking capabilities with multiple strategies to handle diverse document processing needs.</p>"},{"location":"userGuide/chunkcli/","title":"Chunk Module CLI Usage Guide","text":"<p>The Compileo Chunk Module provides comprehensive command-line tools for document chunking with multiple strategies. This guide covers how to use the chunking CLI commands for batch processing and automation.</p> <p>Note: The CLI has been recently refactored into modular components for better maintainability. The commands remain the same but are now powered by a more robust architecture.</p>"},{"location":"userGuide/chunkcli/#chunking-strategy-options","title":"Chunking Strategy Options","text":"<pre><code>graph TD\n    A[Choose Chunking Strategy] --&gt; B{Strategy Type}\n    B --&gt; C[Character]\n    B --&gt; D[Semantic]\n    B --&gt; E[Schema]\n    B --&gt; F[Delimiter]\n    B --&gt; G[Token]\n\n    C --&gt; C1[--character-chunk-size INT]\n    C --&gt; C2[--character-overlap INT]\n\n    D --&gt; D1[--semantic-prompt TEXT]\n\n    E --&gt; E1[--schema-definition TEXT]\n\n    F --&gt; F1[--delimiters TEXT]\n    F --&gt; F2[--chunk-size INT]\n    F --&gt; F3[--overlap INT]\n\n    G --&gt; G1[--chunk-size INT]\n    G --&gt; G2[--overlap INT]\n</code></pre>"},{"location":"userGuide/chunkcli/#cli-commands","title":"CLI Commands","text":""},{"location":"userGuide/chunkcli/#document-upload","title":"Document Upload","text":"<p>Upload documents to a project before processing:</p> <pre><code>compileo documents upload --project-id 1 document1.pdf document2.docx\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID to upload documents to (required) - <code>file_paths</code>: One or more document file paths (required)</p> <p>Example Output: <pre><code>\ud83d\udce4 Uploading 2 documents to project 1...\n\u2705 Documents uploaded successfully. Job ID: doc_upload_12345\n\ud83d\udcca Files uploaded: 2\n\u2705 Upload completed. 2 documents processed.\n</code></pre></p>"},{"location":"userGuide/chunkcli/#document-parsing","title":"Document Parsing","text":"<p>Parse documents to markdown format without chunking:</p> <pre><code>compileo documents parse --project-id 1 --document-ids 101,102 --parser gemini\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID containing documents (required) - <code>--document-ids</code>: Comma-separated list of document IDs (required) - <code>--parser</code>: Document parser (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>) (default: <code>gemini</code>)</p> <p>Example Output: <pre><code>\ud83d\udcc4 Parsing 2 documents in project 1 with gemini\n\u2705 Parsing started. Job ID: parse_job_12345\n\u23f3 Waiting for parsing completion...\n\u2705 Parsing completed successfully!\n\ud83d\udcca Results: 2 documents parsed to markdown\n</code></pre></p>"},{"location":"userGuide/chunkcli/#ai-assisted-chunking-analysis","title":"AI-Assisted Chunking Analysis","text":"<p>Get AI recommendations for optimal chunking strategy:</p> <pre><code>compileo documents analyze --document-id 101 --goal \"Split the document at every chapter, but each chapter has a different name and format\" --examples \"Page 1: Headers: # Chapter 1: Introduction\" \"Page 3: Section: This chapter provides an overview...\"\n</code></pre> <p>Parameters: - <code>--document-id</code>: Document ID to analyze (required) - <code>--goal</code>: Description of chunking objective (required) - <code>--examples</code>: Example strings from document (optional, multiple allowed) - <code>--model</code>: AI model for analysis (<code>gemini</code>, <code>grok</code>, <code>ollama</code>) (default: <code>gemini</code>)</p> <p>Example Output: <pre><code>\ud83e\udd16 Analyzing document 101 for chunking recommendations...\n\ud83d\udcca Document: research_paper.pdf (14,258 chars, 2,350 words)\n\ud83c\udfaf Goal: Split the document at every chapter, but each chapter has a different name and format\n\ud83d\udcdd Examples: 3 provided\n\u2705 Analysis complete!\n\n\ud83d\udccb AI Recommendations:\nStrategy: schema\nParameters:\n  json_schema: {\"rules\": [{\"type\": \"pattern\", \"value\": \"^# \"}, {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}], \"combine\": \"any\"}\nConfidence: 85%\nExplanation: Schema-based chunking recommended for consistent chapter header patterns\n\n\ud83d\udca1 Alternative: semantic (72% confidence)```\n\n### Document Chunking\n\nChunk already parsed documents using specified chunking strategy. For multi-file documents, dynamic cross-file chunking is automatically applied with guaranteed boundary integrity.\n\n```bash\ncompileo documents chunk --project-id 1 --document-ids 101,102 --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n</code></pre></p> <p>Parameters: - <code>--project-id</code>: Project ID containing documents (required) - <code>--document-ids</code>: Comma-separated list of document IDs (required) - <code>--chunk-strategy</code>: Chunking strategy (<code>token</code>, <code>character</code>, <code>semantic</code>, <code>delimiter</code>, <code>schema</code>) (default: <code>token</code>) - <code>--chunk-size</code>: Chunk size (tokens for token strategy, characters for character strategy) (default: 512) - <code>--overlap</code>: Overlap between chunks (default: 50) - <code>--num-ctx</code>: Context window size for Ollama models (overrides default setting) - <code>--chunker</code>: AI model for intelligent chunking (<code>gemini</code>, <code>grok</code>, <code>ollama</code>) (default: <code>gemini</code>) - <code>--semantic-prompt</code>: Custom prompt for semantic chunking - <code>--schema-definition</code>: JSON schema for schema-based chunking - <code>--delimiters</code>: Comma-separated list of delimiter strings for delimiter-based chunking (default: <code>[\"\\n\\n\", \"\\n\"]</code>) - <code>--character-chunk-size</code>: Character chunk size (overrides --chunk-size) - <code>--character-overlap</code>: Character overlap (overrides --overlap) - <code>--sliding-window</code>: Force sliding window chunking (default: auto-detected for multi-file documents) - <code>--window-size</code>: Sliding window size in tokens (default: 3000). Note: This parameter is currently ignored; system settings are used instead. - <code>--window-overlap</code>: Sliding window overlap in tokens (default: 500). Note: This parameter is currently ignored; system settings are used instead. - <code>--system-instruction</code>: System-level instructions to guide the model's behavior, especially for Gemini.</p> <p>Example Output: <pre><code>\u2702\ufe0f Chunking 2 documents in project 1\nStrategy: character, Model: gemini\n\u2705 Chunking started. Job ID: chunk_job_67890\n\u23f3 Waiting for chunking completion...\n\u2705 Chunking completed successfully!\n\ud83d\udcca Results: 2 documents processed, 15 chunks created\n</code></pre></p>"},{"location":"userGuide/chunkcli/#document-processing-combined-parse-chunk","title":"Document Processing (Combined Parse + Chunk)","text":"<p>Process documents with both parsing and chunking in a single step:</p> <pre><code>compileo documents process --project-id 1 --document-ids 101,102 --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID containing documents (required) - <code>--document-ids</code>: Comma-separated list of document IDs (required) - <code>--parser</code>: Document parser (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>) (default: <code>gemini</code>) - <code>--chunk-strategy</code>: Chunking strategy (<code>token</code>, <code>character</code>, <code>semantic</code>, <code>delimiter</code>, <code>schema</code>) (default: <code>token</code>) - <code>--chunk-size</code>: Chunk size (tokens for token strategy, characters for character strategy) (default: 512) - <code>--overlap</code>: Overlap between chunks (default: 50) - <code>--semantic-prompt</code>: Custom prompt for semantic chunking - <code>--schema-definition</code>: JSON schema for schema-based chunking - <code>--character-chunk-size</code>: Character chunk size (overrides --chunk-size) - <code>--character-overlap</code>: Character overlap (overrides --overlap) - <code>--system-instruction</code>: System-level instructions to guide the model's behavior, especially for Gemini.</p>"},{"location":"userGuide/chunkcli/#view-document-content","title":"View Document Content","text":"<p>View parsed content of a document with pagination support:</p> <pre><code>compileo documents content 101 --page 1 --page-size 3000\n</code></pre> <p>Parameters: - <code>document_id</code>: Document ID to view (required) - <code>--page</code>: Page number to view (default: 1) - <code>--page-size</code>: Characters per page (default: 3000) - <code>--output</code>: Save content to file instead of displaying</p> <p>Example Output: <pre><code>Document 101 - Page 1 of 5\nTotal: 14,258 characters, 2,350 words, 280 lines\n[Content displayed here...]\n\nPage 1 of 5\nNext: compileo documents content 101 --page 2\n</code></pre></p>"},{"location":"userGuide/chunkcli/#character-based-chunking","title":"Character-Based Chunking","text":"<p>Split documents by character count with configurable overlap. Fast and deterministic for batch processing.</p>"},{"location":"userGuide/chunkcli/#basic-usage","title":"Basic Usage","text":"<pre><code># Simple character chunking\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy character --character-chunk-size 1000 --character-overlap 100\n\n# With custom parser\ncompileo documents process --project-id 1 --document-ids 101 --parser pypdf --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n</code></pre>"},{"location":"userGuide/chunkcli/#use-cases","title":"Use Cases","text":"<ul> <li>Batch Processing: Process large volumes of documents quickly</li> <li>Memory Management: Predictable memory usage for large document sets</li> <li>Automation: Integrate into CI/CD pipelines and scripts</li> <li>Testing: Deterministic results for testing and validation</li> </ul>"},{"location":"userGuide/chunkcli/#example-output","title":"Example Output","text":"<pre><code>\u2699\ufe0f Processing 1 documents in project 1\n\ud83d\udd0d Parser: pypdf\n\u2702\ufe0f Chunk Strategy: character\n\u2705 Processing started. Job ID: chunk_job_67890\n\u23f3 Waiting for processing completion...\n\u2705 Processing completed successfully!\n\ud83d\udcca Results: 1 documents processed, 15 chunks created\n</code></pre>"},{"location":"userGuide/chunkcli/#semantic-chunking","title":"Semantic Chunking","text":"<p>Use AI to intelligently split documents based on meaning and context with custom prompts. Supports multi-file documents with dynamic cross-file chunking for guaranteed boundary integrity.</p>"},{"location":"userGuide/chunkcli/#simplified-universal-multi-file-document-support","title":"Simplified Universal Multi-File Document Support","text":"<p>The CLI automatically handles multi-file documents using universal forwarding logic that ensures semantic coherence across file boundaries:</p> <ul> <li>Universal Forwarding Rules: All chunking strategies use the same simple rule - if content remains at the end, forward it to the next file</li> <li>Strategy-Agnostic Detection: Removed complex per-strategy incomplete chunk detection code</li> <li>Automatic Content Spacing: Intelligent space insertion between forwarded content and main content prevents word concatenation</li> <li>Memory-Based State Management: Simplified ChunkState object maintains forwarded content between file processing</li> </ul> <p>Automatic Processing: Cross-file chunking is automatically applied to multi-file documents. The system dynamically forwards incomplete chunks as overlap content to subsequent files.</p> <p>Benefits: - Improved semantic chunking quality at file boundaries - Better search results with reduced duplication - More coherent chunks for AI processing - Simplified architecture with universal forwarding rules - All 5 chunking strategies (character, token, semantic, schema, delimiter) use identical logic</p>"},{"location":"userGuide/chunkcli/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Medical textbook chunking with custom context window (recommended)\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy semantic --chunker ollama --num-ctx 4096 --semantic-prompt \"This is a medical textbook that is structured as follows: disease / condition and discussion about it, then another disease / condition and discussion about it. Split should occur at the end of each discussion and before next disease / condition title.\"\n\n# General medical document chunking\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy semantic --semantic-prompt \"Split this medical document at natural section boundaries, ensuring each chunk contains complete clinical information about a single condition or procedure.\"\n\n# Legal document chunking\ncompileo documents process --project-id 1 --document-ids 102 --chunk-strategy semantic --semantic-prompt \"Divide this legal document at section boundaries, keeping each complete legal clause, definition, or contractual obligation in a single chunk.\"\n</code></pre>"},{"location":"userGuide/chunkcli/#advanced-prompt-examples","title":"Advanced Prompt Examples","text":"<p>General Purpose (Recommended for all models including Gemini): <pre><code>compileo documents process --project-id 1 --document-ids 101 --chunk-strategy semantic --semantic-prompt \"You are an expert document analysis tool. Your task is to split a document into logical chunks based on the user's instruction. You will be given an instruction and the document text. You must identify the exact headings or titles that mark the beginning of a new chunk according to the instruction.\n\n**User Instruction:**\n{user_instruction}\n\n**Output Requirements:**\n- Return ONLY a comma-separated list of the exact heading strings that should start a new chunk.\n- Do not include any other text, explanations, or formatting.\n- Each heading should be exactly as it appears in the document.\n\n**Example:**\nIf the instruction is 'Split by chapter' and the text contains '# Chapter 1' and '# Chapter 2', your output should be:\n# Chapter 1,# Chapter 2\n\n**Document to analyze:**\n\" --system-instruction \"You are an expert document analysis AI. Your role is to act as a text-splitting engine. You will be given a user instruction and a document. Your sole purpose is to identify the exact split points in the document based on the user's semantic instructions and split the text into chunks.\n\nHere is your chain of thought:\n1.  **Analyze the user's instruction**: Understand the semantic meaning of how the user wants to split the document.\n2.  **Scan the document**: Read through the document to identify the natural structure and content blocks.\n3.  **Identify split points**: Based on the user's instruction, pinpoint the exact locations in the text where a split should occur.\n4.  **Create chunks**: Split the document at these points. The chunks should contain the original, unmodified text.\n5.  **Format the output**: Return a clean, comma-separated list of the exact headings or titles that mark the beginning of each new chunk. Do not include any other text, explanations, or formatting.\n\"```\n\n**Medical Textbooks (Example of specific user_instruction):**\n```bash\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy semantic --semantic-prompt \"This is a medical textbook that is structured as follows: disease / condition and discussion about it, then another disease / condition and discussion about it. Split should occur at the end of each discussion and before next disease / condition title.\"```\n\n**General Medical Documents (Example of specific user_instruction):**\n```bash\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy semantic --semantic-prompt \"Split this medical document at natural section boundaries, ensuring each chunk contains complete clinical information about a single condition, symptom, or treatment. Pay special attention to maintaining the integrity of diagnostic criteria and treatment protocols.\"\n</code></pre></p> <p>Technical Documentation (Example of specific user_instruction): <pre><code>compileo documents process --project-id 1 --document-ids 103 --chunk-strategy semantic --semantic-prompt \"Split this technical document at logical boundaries, ensuring each chunk contains complete explanations of single concepts, algorithms, or procedures. Keep code examples with their explanations.\"\n</code></pre></p> <p>Research Papers (Example of specific user_instruction): <pre><code>compileo documents process --project-id 1 --document-ids 104 --chunk-strategy semantic --semantic-prompt \"Divide this research paper at meaningful section boundaries, keeping complete methodologies, results, and discussions intact. Ensure each chunk represents a coherent scientific contribution.\"\n</code></pre></p>"},{"location":"userGuide/chunkcli/#use-cases_1","title":"Use Cases","text":"<ul> <li>Complex Documents: Documents requiring AI understanding of context</li> <li>Domain-Specific: Specialized content needing expert knowledge</li> <li>Quality Requirements: When semantic coherence is critical</li> <li>Research Applications: Academic and technical document processing</li> </ul>"},{"location":"userGuide/chunkcli/#schema-based-chunking","title":"Schema-Based Chunking","text":"<p>Apply custom rules combining patterns and delimiters for precise control over document splitting.</p>"},{"location":"userGuide/chunkcli/#schema-definition","title":"Schema Definition","text":"<p>Create JSON schemas with multiple rules. Note: The CLI automatically attempts to fix common JSON syntax errors in regex patterns (e.g., unescaped <code>\\s</code>, <code>\\n</code>) and literal control characters, but it is best practice to provide a fully escaped JSON string.</p> <pre><code>{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"# [A-Z\\\\\\\\s]+\"},\n    {\"type\": \"delimiter\", \"value\": \"\\\\\\\\n\\\\\\\\n\"}\n  ],\n  \"combine\": \"any\"\n}\n</code></pre>"},{"location":"userGuide/chunkcli/#basic-usage_2","title":"Basic Usage","text":"<pre><code># Markdown document chunking (Note the double-escaped backslashes for JSON)\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy schema --schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"^# \"}, {\"type\": \"delimiter\", \"value\": \"\\\\n\\\\n\"}], \"combine\": \"any\"}'\n\n# Structured document chunking\ncompileo documents process --project-id 1 --document-ids 102 --chunk-strategy schema --schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"&lt;chapter&gt;\"}, {\"type\": \"delimiter\", \"value\": \"&lt;/chapter&gt;\"}], \"combine\": \"any\"}'\n</code></pre>"},{"location":"userGuide/chunkcli/#rule-types","title":"Rule Types","text":"<p>Pattern Rules (Regex):```bash</p>"},{"location":"userGuide/chunkcli/#headers-in-markdown","title":"Headers in markdown","text":"<p>--schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"^## \"}], \"combine\": \"any\"}'</p>"},{"location":"userGuide/chunkcli/#numbered-sections","title":"Numbered sections","text":"<p>--schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"^[0-9]+\\.\"}], \"combine\": \"any\"}'</p>"},{"location":"userGuide/chunkcli/#xmlhtml-tags","title":"XML/HTML tags","text":"<p>--schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"\"}, {\"type\": \"pattern\", \"value\": \"\"}], \"combine\": \"any\"}' <pre><code>**Delimiter Rules:**\n```bash\n# Double line breaks\n--schema-definition '{\"rules\": [{\"type\": \"delimiter\", \"value\": \"\\n\\n\"}], \"combine\": \"any\"}'\n\n# Custom separators\n--schema-definition '{\"rules\": [{\"type\": \"delimiter\", \"value\": \"---\"}], \"combine\": \"any\"}'\n\n# Page breaks\n--schema-definition '{\"rules\": [{\"type\": \"delimiter\", \"value\": \"\\f\"}], \"combine\": \"any\"}'\n</code></pre>"},{"location":"userGuide/chunkcli/#combine-options","title":"Combine Options","text":"<p>\"any\": Split when any rule matches <pre><code>--schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"^# \"}, {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}], \"combine\": \"any\"}'\n</code></pre></p> <p>\"all\": Split only when all rules match at the same position <pre><code>--schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"^# \"}, {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}], \"combine\": \"all\"}'\n</code></pre></p>"},{"location":"userGuide/chunkcli/#use-cases_2","title":"Use Cases","text":"<ul> <li>Structured Documents: Documents with known formatting patterns</li> <li>Custom Formats: Proprietary document structures</li> <li>Precise Control: When exact splitting behavior is required</li> <li>Multi-Criteria: Complex splitting rules combining multiple conditions</li> </ul>"},{"location":"userGuide/chunkcli/#delimiter-based-chunking","title":"Delimiter-Based Chunking","text":"<p>Simple splitting on specified delimiter strings with enhanced support for multiple delimiters.</p>"},{"location":"userGuide/chunkcli/#basic-usage_3","title":"Basic Usage","text":"<pre><code># Split on double line breaks (default)\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy delimiter --chunk-size 1000 --overlap 100\n\n# Single delimiter\ncompileo documents process --project-id 1 --document-ids 102 --chunk-strategy delimiter --delimiters \"---\" --chunk-size 800 --overlap 50\n\n# Multiple delimiters (comma-separated)\ncompileo documents process --project-id 1 --document-ids 103 --chunk-strategy delimiter --delimiters \"#,---,\\n\\n\" --chunk-size 1000 --overlap 100\n\n# Markdown headers\ncompileo documents process --project-id 1 --document-ids 104 --chunk-strategy delimiter --delimiters \"#\" --chunk-size 1500 --overlap 150\n</code></pre>"},{"location":"userGuide/chunkcli/#delimiter-examples","title":"Delimiter Examples","text":"<p>Markdown Documents: <pre><code># Split on headers\n--delimiters \"#\"\n\n# Split on multiple header levels\n--delimiters \"#,##,###\"\n</code></pre></p> <p>Structured Text: <pre><code># Section breaks\n--delimiters \"SECTION:,CHAPTER\"\n\n# Custom separators\n--delimiters \"---,***\"\n</code></pre></p> <p>Mixed Content: <pre><code># Multiple patterns\n--delimiters \"\\n\\n,---,&lt;hr&gt;\"\n</code></pre></p>"},{"location":"userGuide/chunkcli/#use-cases_3","title":"Use Cases","text":"<ul> <li>Simple Structures: Documents with clear separator patterns</li> <li>Quick Processing: Fast chunking for basic requirements</li> <li>Known Formats: Documents with consistent delimiters</li> <li>Markdown Documents: Split on headers and sections</li> <li>Custom Formats: Any delimiter pattern you specify</li> </ul>"},{"location":"userGuide/chunkcli/#token-based-chunking","title":"Token-Based Chunking","text":"<p>Precise token counting using tiktoken library with overlap. Requires tiktoken package to be installed.</p>"},{"location":"userGuide/chunkcli/#basic-usage_4","title":"Basic Usage","text":"<pre><code># Standard token chunking\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy token --chunk-size 512 --overlap 50\n\n# Smaller chunks for specific models\ncompileo documents process --project-id 1 --document-ids 102 --chunk-strategy token --chunk-size 256 --overlap 25\n</code></pre>"},{"location":"userGuide/chunkcli/#error-handling","title":"Error Handling","text":"<p>Token chunking will fail explicitly if: - <code>tiktoken</code> library is not installed - Invalid tokenizer model specified - Strategy creation fails for any reason</p> <p>Error Output: <pre><code>Error: Failed to create token-based chunking strategy: No module named 'tiktoken'. Token-based chunking requires tiktoken library.\n</code></pre></p>"},{"location":"userGuide/chunkcli/#use-cases_4","title":"Use Cases","text":"<ul> <li>LLM Preparation: Documents for language model input with exact token limits</li> <li>API Limits: Respecting token limits of AI services</li> <li>Precise Control: Token-aware processing for API constraints</li> </ul>"},{"location":"userGuide/chunkcli/#job-management","title":"Job Management","text":""},{"location":"userGuide/chunkcli/#check-job-status","title":"Check Job Status","text":"<p>Monitor processing jobs in real-time:</p> <pre><code># Check upload status\ncompileo documents status --job-id doc_upload_12345 --type upload\n\n# Check parsing status\ncompileo documents status --job-id parse_job_12345 --type parse\n\n# Check chunking status\ncompileo documents status --job-id chunk_job_67890 --type process\n</code></pre> <p>Parameters: - <code>--job-id</code>: Job ID to check (required) - <code>--type</code>: Job type (<code>upload</code>, <code>process</code>, <code>parse</code>) (default: <code>process</code>)</p> <p>Example Output: <pre><code>Job Status: COMPLETED\nProgress: 100%\nCurrent Step: Processing complete\nDocuments Processed: 2\nTotal Chunks Created: 15\n</code></pre></p>"},{"location":"userGuide/chunkcli/#list-documents","title":"List Documents","text":"<p>View documents in a project:</p> <pre><code># List all documents\ncompileo documents list --project-id 1\n\n# Table format (default)\ncompileo documents list --project-id 1 --format table\n\n# JSON format\ncompileo documents list --project-id 1 --format json\n</code></pre>"},{"location":"userGuide/chunkcli/#delete-documents","title":"Delete Documents","text":"<p>Remove documents and associated chunks:</p> <pre><code># Delete with confirmation prompt\ncompileo documents delete 101\n\n# Delete without confirmation\ncompileo documents delete 102 --confirm\n</code></pre>"},{"location":"userGuide/chunkcli/#batch-processing-examples","title":"Batch Processing Examples","text":""},{"location":"userGuide/chunkcli/#process-multiple-documents","title":"Process Multiple Documents","text":"<pre><code># Upload multiple files\ncompileo documents upload --project-id 1 doc1.pdf doc2.pdf doc3.pdf\n\n# Process all uploaded documents\ncompileo documents process --project-id 1 --document-ids 101,102,103 --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n</code></pre>"},{"location":"userGuide/chunkcli/#automated-workflow","title":"Automated Workflow","text":"<pre><code>#!/bin/bash\n# Automated document processing workflow\n\nPROJECT_ID=1\n\n# Upload documents\nJOB_ID=$(compileo documents upload --project-id $PROJECT_ID *.pdf | grep \"Job ID:\" | cut -d' ' -f4)\n\n# Wait for upload completion\nwhile true; do\n    STATUS=$(compileo documents status --job-id $JOB_ID --type upload | grep \"Status:\" | cut -d' ' -f3)\n    if [ \"$STATUS\" = \"COMPLETED\" ]; then\n        break\n    fi\n    sleep 5\ndone\n\n# Get document IDs (simplified - would need parsing in real script)\nDOC_IDS=\"101,102,103\"\n\n# Process documents\ncompileo documents process --project-id $PROJECT_ID --document-ids $DOC_IDS --chunk-strategy semantic --semantic-prompt \"Split at natural topic boundaries...\"\n</code></pre>"},{"location":"userGuide/chunkcli/#performance-optimization","title":"Performance Optimization","text":""},{"location":"userGuide/chunkcli/#large-document-sets","title":"Large Document Sets","text":"<pre><code># Use character strategy for speed\ncompileo documents process --project-id 1 --document-ids 101,102,103,104,105 --chunk-strategy character --character-chunk-size 1000 --character-overlap 100\n\n# Process in smaller batches\ncompileo documents process --project-id 1 --document-ids 101,102 --chunk-strategy semantic --semantic-prompt \"...\"\ncompileo documents process --project-id 1 --document-ids 103,104 --chunk-strategy semantic --semantic-prompt \"...\"\n</code></pre>"},{"location":"userGuide/chunkcli/#memory-management","title":"Memory Management","text":"<pre><code># Smaller chunks for memory-constrained environments\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy character --character-chunk-size 256 --character-overlap 25\n\n# Use efficient parsers\ncompileo documents process --project-id 1 --document-ids 101 --parser pypdf --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n</code></pre>"},{"location":"userGuide/chunkcli/#error-handling_1","title":"Error Handling","text":""},{"location":"userGuide/chunkcli/#common-issues","title":"Common Issues","text":"<p>Invalid Schema: <pre><code># Error: Invalid JSON in schema-definition\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy schema --schema-definition '{\"invalid\": json}'\n# Fix: Validate JSON before using\n</code></pre></p> <p>Missing Documents: <pre><code># Error: Document IDs not found\ncompileo documents process --project-id 1 --document-ids 999\n# Fix: Check document IDs with list command\n</code></pre></p> <p>API Key Issues: <pre><code># Error: AI service authentication failed\ncompileo documents process --project-id 1 --document-ids 101 --chunk-strategy semantic --semantic-prompt \"...\"\n# Fix: Ensure API keys are configured\n</code></pre></p>"},{"location":"userGuide/chunkcli/#integration-with-scripts","title":"Integration with Scripts","text":""},{"location":"userGuide/chunkcli/#python-automation","title":"Python Automation","text":"<pre><code>import subprocess\nimport json\n\ndef process_documents(project_id, file_paths, chunk_strategy=\"character\", **kwargs):\n    # Upload files\n    upload_cmd = [\"compileo\", \"documents\", \"upload\", \"--project-id\", str(project_id)] + file_paths\n    result = subprocess.run(upload_cmd, capture_output=True, text=True)\n\n    # Extract job ID and wait for completion\n    # (Implementation would parse result and monitor status)\n\n    # Process documents\n    process_cmd = [\n        \"compileo\", \"documents\", \"process\",\n        \"--project-id\", str(project_id),\n        \"--document-ids\", \"101,102\",  # Would be extracted from upload result\n        \"--chunk-strategy\", chunk_strategy\n    ]\n\n    # Add strategy-specific parameters\n    if chunk_strategy == \"character\":\n        process_cmd.extend([\"--character-chunk-size\", str(kwargs.get(\"chunk_size\", 500))])\n        process_cmd.extend([\"--character-overlap\", str(kwargs.get(\"overlap\", 50))])\n\n    result = subprocess.run(process_cmd, capture_output=True, text=True)\n    return result\n</code></pre>"},{"location":"userGuide/chunkcli/#shell-scripting","title":"Shell Scripting","text":"<pre><code>#!/bin/bash\n# Batch processing script\n\nPROJECT_ID=$1\nSTRATEGY=$2\nINPUT_DIR=$3\n\n# Upload all files in directory\ncompileo documents upload --project-id $PROJECT_ID $INPUT_DIR/*.*\n\n# Get document IDs (would need implementation)\nDOC_IDS=$(compileo documents list --project-id $PROJECT_ID --format json | jq -r '.documents[].id' | tr '\\n' ',' | sed 's/,$//')\n\n# Process with specified strategy\ncase $STRATEGY in\n    \"character\")\n        compileo documents process --project-id $PROJECT_ID --document-ids $DOC_IDS --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n        ;;\n    \"semantic\")\n        compileo documents process --project-id $PROJECT_ID --document-ids $DOC_IDS --chunk-strategy semantic --semantic-prompt \"Split at natural topic boundaries...\"\n        ;;\n    \"schema\")\n        compileo documents process --project-id $PROJECT_ID --document-ids $DOC_IDS --chunk-strategy schema --schema-definition '{\"rules\": [{\"type\": \"pattern\", \"value\": \"^# \"}], \"combine\": \"any\"}'\n        ;;\nesac\n</code></pre> <p>This CLI provides powerful batch processing capabilities with all chunking strategies available through an intuitive command-line interface suitable for automation and scripting workflows.</p>"},{"location":"userGuide/chunkgui/","title":"Chunk Module GUI Usage Guide","text":"<p>The Compileo GUI provides an intuitive web interface for document chunking with multiple strategies. This guide covers how to use the chunking features through the Streamlit-based web application.</p>"},{"location":"userGuide/chunkgui/#chunking-strategy-options","title":"Chunking Strategy Options","text":"<pre><code>graph TD\n    A[Document Processing Tab] --&gt; B[Select Chunk Strategy]\n    B --&gt; C{Strategy Choice}\n    C --&gt; D[Character]\n    C --&gt; E[Semantic]\n    C --&gt; F[Schema]\n    C --&gt; G[Delimiter]\n    C --&gt; H[Token]\n\n    D --&gt; D1[Chunk Size Slider&lt;br/&gt;500-2000 chars]\n    D --&gt; D2[Overlap Slider&lt;br/&gt;0-500 chars]\n\n    E --&gt; E1[Prompt Text Area&lt;br/&gt;Custom AI instructions]\n\n    F --&gt; F1[Schema Text Area&lt;br/&gt;JSON rule definition]\n\n    G --&gt; G1[Delimiter Multi-Select&lt;br/&gt;Common separators]\n\n    H --&gt; H1[Chunk Size Slider&lt;br/&gt;256-2048 tokens]\n    H --&gt; H2[Overlap Slider&lt;br/&gt;0-512 tokens]\n</code></pre>"},{"location":"userGuide/chunkgui/#accessing-document-processing","title":"Accessing Document Processing","text":""},{"location":"userGuide/chunkgui/#navigation","title":"Navigation","text":"<ol> <li>Launch the Compileo GUI: <code>streamlit run src/compileo/features/gui/main.py</code></li> <li>Navigate to the Document Processing tab in the sidebar</li> <li>Select your target project from the dropdown</li> </ol>"},{"location":"userGuide/chunkgui/#chunk-management","title":"Chunk Management","text":"<p>The chunk management interface is available in the Document Processing tab under the \"\ud83d\udcd1 View &amp; Manage Chunks\" tab.</p> <p>Features: - View all chunks for a document with detailed metadata (ID, index, token count) - Expandable sections with full content preview, displaying chunk indices and token counts (e.g., <code>Chunk 1 (Tokens: ~150)</code>) instead of internal IDs - Flexible chunk deletion using simple index numbers (no UUIDs required):   - Single chunks: <code>1</code>   - Multiple chunks: <code>1, 3, 5</code>   - Ranges: <code>1-10</code>   - Combinations: <code>1-5, 8, 10-12</code> - Bulk document chunk deletion with confirmation - Refresh button for data reloading</p>"},{"location":"userGuide/chunkgui/#interface-overview","title":"Interface Overview","text":"<p>The Document Processing tab provides three main sections: - Parse Documents Tab: Upload and parse documents to markdown - Configure &amp; Chunk Documents Tab: Configure chunking strategies and process documents - View &amp; Manage Chunks Tab: View, inspect, and delete chunks</p>"},{"location":"userGuide/chunkgui/#configuration-modes","title":"Configuration Modes","text":"<p>The Configure &amp; Chunk Documents tab offers two configuration approaches:</p>"},{"location":"userGuide/chunkgui/#manual-configuration-default","title":"Manual Configuration (Default)","text":"<ul> <li>Strategy Selection: Choose from character, semantic, schema, delimiter, or token strategies</li> <li>Parameter Configuration: Set strategy-specific parameters manually</li> <li>Direct Processing: Immediate chunking with your chosen settings</li> </ul>"},{"location":"userGuide/chunkgui/#ai-assisted-configuration","title":"AI-Assisted Configuration","text":"<ul> <li>Goal Description: Describe your chunking objective (required)</li> <li>Document Preview: Browse document content with pagination</li> <li>Page-Aware Copy: Extract examples from specific pages</li> <li>AI Recommendations: Get intelligent strategy suggestions</li> <li>Refined Processing: Apply AI-recommended parameters</li> </ul>"},{"location":"userGuide/chunkgui/#ai-assisted-configuration-workflow","title":"AI-Assisted Configuration Workflow","text":"<p>The AI-assisted configuration provides intelligent chunking recommendations based on your document structure and goals.</p>"},{"location":"userGuide/chunkgui/#streamlined-design-principles","title":"Streamlined Design Principles","text":"<ul> <li>No Redundancy: Single document preview serves both browsing and example extraction</li> <li>Compact Layout: Example collection shown as editable summary, not verbose expandable sections</li> <li>Progressive Disclosure: Simple workflow with optional advanced features</li> <li>Contextual Actions: Copy buttons work on current page without complex page selection</li> <li>Side-by-Side Selection: Document and parsed file selectors in compact single row for efficient navigation</li> <li>File-Specific Content: Full parsed file content loading with GUI pagination for large document chunks</li> <li>Seamless Switching: Instant content updates when changing between parsed files within a document</li> </ul>"},{"location":"userGuide/chunkgui/#step-1-describe-your-chunking-goal","title":"Step 1: Describe Your Chunking Goal","text":"<p>Required Field: Describe what you want to achieve with chunking.</p> <pre><code>\"I want to split the document at every chapter, but each chapter has a different name and format\"\n</code></pre> <p>Examples of Good Goals: - \"Split medical records at patient visit boundaries\" - \"Divide legal documents at clause and section breaks\" - \"Chunk research papers at methodology, results, and discussion sections\" - \"Split technical documentation at feature and API boundaries\"</p>"},{"location":"userGuide/chunkgui/#step-2-choose-document-and-parsed-file-for-analysis","title":"Step 2: Choose Document and Parsed File for Analysis","text":"<ol> <li>Select Document: Choose a parsed document from the dropdown in the compact selection row</li> <li>Select Parsed File: Choose a specific parsed file chunk from the automatically loaded dropdown</li> <li>Browse Content: Use pagination controls to navigate through the full parsed file content (10,000 characters per page)</li> <li>View Metrics: See total characters, words, and lines for the selected parsed file</li> <li>Switch Files: Change between different parsed files within the same document seamlessly</li> <li>Select Target Documents: Choose which documents to apply the final chunking configuration to</li> </ol>"},{"location":"userGuide/chunkgui/#step-3-extract-examples-with-text-selection","title":"Step 3: Extract Examples with Text Selection","text":"<p>Use the interactive text selection to extract relevant examples from specific pages:</p>"},{"location":"userGuide/chunkgui/#direct-text-selection","title":"Direct Text Selection","text":"<ul> <li>Click and drag to select any portion of text in the content preview area</li> <li>Real-time feedback shows your current selection in a dedicated field</li> <li>Header highlighting automatically emphasizes potential headers (markdown # headers, ALL CAPS lines)</li> <li>Flexible selection allows choosing any text portion (headers, sections, samples, or custom selections)</li> </ul>"},{"location":"userGuide/chunkgui/#adding-selected-text-to-examples","title":"Adding Selected Text to Examples","text":"<ul> <li>\"Copy Selected Text to Examples\" button adds your current selection to the example collection</li> <li>Character count shows the size of your selection</li> <li>Multiple selections can be added from different pages</li> <li>Clear selection button removes current selection without adding to examples</li> </ul>"},{"location":"userGuide/chunkgui/#selection-tips","title":"Selection Tips","text":"<ul> <li>Headers first: Start by selecting document headers as they provide the best guidance for AI chunking</li> <li>Multiple examples: Collect examples from different parts of the document for better AI understanding</li> <li>Precise selection: Select exactly the text you want the AI to learn from</li> <li>Context matters: Include surrounding context when selecting examples to show boundary patterns</li> </ul>"},{"location":"userGuide/chunkgui/#step-4-build-example-collection","title":"Step 4: Build Example Collection","text":"<ol> <li>Navigate Pages: Use pagination to browse different sections of the document</li> <li>Select Text: Click and drag to select meaningful text portions (focus on headers and boundary patterns)</li> <li>Add to Examples: Click \"Copy Selected Text to Examples\" to add your selection</li> <li>Review Collection: See all collected examples in an expandable list</li> <li>Remove if Needed: Use individual remove buttons to refine your example collection</li> </ol> <p>Example Collection Summary: <pre><code>\ud83d\udcda Collected Examples (3 items)\n\u2022 Example 1 (47 chars): # Introduction\nThis chapter covers the basic concepts\n\u2022 Example 2 (38 chars): ## Methods\nThe methodology section explains our approach\n\u2022 Example 3 (52 chars): Each chapter begins with a level 1 header followed by content\n</code></pre></p>"},{"location":"userGuide/chunkgui/#step-5-get-ai-recommendations","title":"Step 5: Get AI Recommendations","text":"<ol> <li>Click \"Analyze &amp; Recommend\": AI analyzes your goal, document content, and collected examples</li> <li>Review Recommendations: AI suggests optimal chunking strategy and parameters</li> <li>Accept or Modify: Use AI-recommended settings or make adjustments</li> <li>Process Documents: Apply the configuration to your selected target documents</li> </ol>"},{"location":"userGuide/chunkgui/#ai-recommendation-examples","title":"AI Recommendation Examples","text":"<p>For Chapter-Based Splitting: - Strategy: Schema-based - Rules: <code>[{\"type\": \"pattern\", \"value\": \"^# \"}, {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}]</code> - Combine: <code>\"any\"</code></p> <p>For Semantic Document Splitting: - Strategy: Semantic - Prompt: Custom prompt based on your goal and examples - Model: Recommended AI model for your content type</p>"},{"location":"userGuide/chunkgui/#tips-for-better-ai-recommendations","title":"Tips for Better AI Recommendations","text":""},{"location":"userGuide/chunkgui/#goal-description-tips","title":"Goal Description Tips","text":"<ul> <li>Be Specific: \"Split at chapter boundaries\" vs \"Split at every chapter, where chapters start with '# Chapter X:'\"</li> <li>Include Context: Mention document type (medical, legal, technical)</li> <li>Specify Patterns: Reference actual formatting patterns you observe</li> </ul>"},{"location":"userGuide/chunkgui/#example-quality-tips","title":"Example Quality Tips","text":"<ul> <li>Focus on Headers: Select document headers first as they provide the clearest chunking guidance</li> <li>Include Context: When selecting text, include surrounding context to show boundary patterns</li> <li>Multiple Examples: Collect examples from different parts of the document for comprehensive AI understanding</li> <li>Precise Selection: Select exactly the text that demonstrates your desired chunking behavior</li> <li>Variety: Show different types of content boundaries and structural elements</li> </ul>"},{"location":"userGuide/chunkgui/#document-selection-tips","title":"Document Selection Tips","text":"<ul> <li>Representative Sample: Choose a document that represents your typical content</li> <li>Complete Parsing: Ensure documents are fully parsed before AI analysis</li> <li>Content Quality: Use well-structured source documents for better analysis</li> </ul>"},{"location":"userGuide/chunkgui/#character-based-chunking","title":"Character-Based Chunking","text":"<p>Split documents by character count with configurable overlap. Perfect for simple, fast processing.</p>"},{"location":"userGuide/chunkgui/#how-to-use","title":"How to Use","text":"<ol> <li>Select Strategy: Choose \"Character\" from the chunk strategy dropdown</li> <li>Configure Parameters:</li> <li>Chunk Size: Use the slider to set characters per chunk (500-2000 recommended)</li> <li>Overlap: Set character overlap between chunks (10-20% of chunk size)</li> <li>Upload Documents: Drag and drop PDF, DOCX, TXT, MD, CSV, JSON, or XML files</li> <li>Choose Parser: Select appropriate document parser (pypdf for PDFs, unstructured for Office docs)</li> <li>Process: Click the \"Process Documents\" button</li> </ol>"},{"location":"userGuide/chunkgui/#example-configuration","title":"Example Configuration","text":"<pre><code>Chunk Strategy: Character\nChunk Size: 1000 characters\nOverlap: 100 characters\nParser: pypdf\nFiles: medical_report.pdf\n</code></pre>"},{"location":"userGuide/chunkgui/#when-to-use","title":"When to Use","text":"<ul> <li>Large Document Sets: Process many documents quickly</li> <li>Memory Constraints: Predictable memory usage</li> <li>Simple Requirements: When semantic understanding isn't needed</li> <li>Testing: Deterministic results for validation</li> </ul>"},{"location":"userGuide/chunkgui/#visual-feedback","title":"Visual Feedback","text":"<ul> <li>Progress bar shows upload and processing status</li> <li>Real-time chunk count updates</li> <li>Success confirmation with processing statistics</li> </ul>"},{"location":"userGuide/chunkgui/#semantic-chunking","title":"Semantic Chunking","text":"<p>Use AI to intelligently split documents based on meaning and context with custom prompts. Supports multi-file documents with dynamic cross-file chunking for guaranteed boundary integrity.</p>"},{"location":"userGuide/chunkgui/#advanced-universal-cross-file-support","title":"\u2705 Advanced Universal Cross-File Support","text":"<p>The GUI automatically handles multi-file documents using universal forwarding logic that ensures semantic coherence across file boundaries:</p> <ul> <li>Universal Forwarding Rules: All chunking strategies use the same simple rule - if content remains at the end, forward it to the next file</li> <li>Strategy-Agnostic Detection: Removed complex per-strategy incomplete chunk detection code</li> <li>Automatic Content Spacing: Intelligent space insertion between forwarded content and main content prevents word concatenation</li> <li>Memory-Based State Management: Simplified ChunkState object maintains forwarded content between file processing</li> </ul> <p>Automatic Processing: Cross-file chunking is automatically applied to multi-file documents. The system dynamically forwards incomplete chunks as overlap content to subsequent files.</p> <p>Benefits: - Improved semantic chunking quality at file boundaries - Better search results with reduced duplication - More coherent chunks for AI processing - Simplified architecture with universal forwarding rules - All 5 chunking strategies (character, token, semantic, schema, delimiter) use identical logic</p>"},{"location":"userGuide/chunkgui/#how-to-use_1","title":"How to Use","text":"<ol> <li>Select Strategy: Choose \"Semantic\" from the chunk strategy dropdown</li> <li>Craft Your Prompt: Use the text area to provide custom instructions for the AI</li> <li>Upload Documents: Select documents requiring semantic understanding</li> <li>Choose AI Model: Select from available models (Gemini, Grok, Ollama)</li> <li>Process: Click \"Process Documents\"</li> </ol>"},{"location":"userGuide/chunkgui/#prompt-examples","title":"Prompt Examples","text":"<p>Medical Textbooks (Recommended): <pre><code>This is a medical textbook that is structured as follows: disease / condition and discussion about it, then another disease / condition and discussion about it. Split should occur at the end of each discussion and before next disease / condition title.\n</code></pre></p> <p>Medical Documents: <pre><code>Split this medical document at natural section boundaries, ensuring each chunk contains complete clinical information about a single condition, symptom, or treatment. Pay special attention to maintaining the integrity of diagnostic criteria and treatment protocols.\n</code></pre></p> <p>Legal Documents: <pre><code>Divide this legal document at section boundaries, keeping each complete legal clause, definition, or contractual obligation in a single chunk. Preserve the logical flow of legal arguments and obligations.\n</code></pre></p> <p>Technical Documentation: <pre><code>Split this technical document at logical boundaries, ensuring each chunk contains complete explanations of single concepts, algorithms, or procedures. Keep code examples with their explanations.\n</code></pre></p> <p>Research Papers: <pre><code>Divide this research paper at meaningful section boundaries, keeping complete methodologies, results, and discussions intact. Ensure each chunk represents a coherent scientific contribution.\n</code></pre></p>"},{"location":"userGuide/chunkgui/#advanced-prompt-techniques","title":"Advanced Prompt Techniques","text":"<p>Role-Playing: <pre><code>You are a medical editor. Split this clinical document at natural breaks where one medical topic ends and another begins. Ensure each chunk tells a complete clinical story.\n</code></pre></p> <p>Chain-of-Thought: <pre><code>Think step-by-step: 1) Identify topic changes, 2) Find natural break points, 3) Ensure content completeness, 4) Split at optimal boundaries. Split this document accordingly.\n</code></pre></p> <p>Specific Instructions: <pre><code>Split at section headers, but also consider content flow. If a section header appears mid-explanation, keep the explanation with its header. Prioritize content coherence over strict header boundaries.\n</code></pre></p>"},{"location":"userGuide/chunkgui/#when-to-use_1","title":"When to Use","text":"<ul> <li>Complex Documents: Content requiring AI understanding</li> <li>Domain Expertise: Specialized knowledge needed for splitting</li> <li>Quality Requirements: When semantic coherence is critical</li> <li>Custom Logic: Unique splitting requirements</li> </ul>"},{"location":"userGuide/chunkgui/#tips-for-better-results","title":"Tips for Better Results","text":"<ul> <li>Be Specific: Include domain context and desired behavior</li> <li>Provide Examples: Reference specific patterns in your documents</li> <li>Test Iteratively: Try different prompts and refine based on results</li> <li>Balance Detail: Too vague = poor splits, too specific = inflexible</li> </ul>"},{"location":"userGuide/chunkgui/#schema-based-chunking","title":"Schema-Based Chunking","text":"<p>Apply custom rules combining patterns and delimiters for precise document splitting control.</p>"},{"location":"userGuide/chunkgui/#how-to-use_2","title":"How to Use","text":"<ol> <li>Select Strategy: Choose \"Schema\" from the chunk strategy dropdown</li> <li>Define Schema: Use the text area to create JSON rules for splitting</li> <li>Upload Documents: Select documents matching your schema patterns</li> <li>Choose Parser: Select parser that preserves your target patterns</li> <li>Process: Click \"Process Documents\"</li> </ol>"},{"location":"userGuide/chunkgui/#schema-structure","title":"Schema Structure","text":"<pre><code>{\n  \"rules\": [\n    {\n      \"type\": \"pattern\",\n      \"value\": \"# [A-Z]+\"\n    },\n    {\n      \"type\": \"delimiter\",\n      \"value\": \"\\\\n\\\\n\"\n    }\n  ],\n  \"combine\": \"any\"\n}\n</code></pre>"},{"location":"userGuide/chunkgui/#rule-types","title":"Rule Types","text":""},{"location":"userGuide/chunkgui/#pattern-rules-regex","title":"Pattern Rules (Regex)","text":"<p>Match specific text patterns using regular expressions. Note: Regex now supports multiline mode! Use <code>^</code> to match the start of any line, not just the start of the document.</p> <p><pre><code>{\"type\": \"pattern\", \"value\": \"^# \"}\n</code></pre> - <code>^#</code>: Markdown headers at the start of a line - <code>^[0-9]+\\.</code>: Numbered sections at the start of a line - <code>&lt;chapter&gt;</code>: XML/HTML tags - <code>^[A-Z][A-Z\\s]+$</code>: ALL CAPS section headers</p>"},{"location":"userGuide/chunkgui/#delimiter-rules","title":"Delimiter Rules","text":"<p>Split on exact string matches:</p> <p><pre><code>{\"type\": \"delimiter\", \"value\": \"\\n\\n\"}\n</code></pre> - <code>\\n\\n</code>: Double line breaks - <code>---</code>: Horizontal rules - <code>&lt;hr&gt;</code>: HTML horizontal rules - <code>\\f</code>: Page breaks</p>"},{"location":"userGuide/chunkgui/#combine-options","title":"Combine Options","text":"<p>\"any\": Split when any rule matches <pre><code>{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"^# \"},\n    {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}\n  ],\n  \"combine\": \"any\"\n}\n</code></pre></p> <p>\"all\": Split only when all rules match at the same position <pre><code>{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"^# \"},\n    {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}\n  ],\n  \"combine\": \"all\"\n}\n</code></pre></p>"},{"location":"userGuide/chunkgui/#schema-examples","title":"Schema Examples","text":"<p>Markdown Documents: <pre><code>{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"^## \"},\n    {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}\n  ],\n  \"combine\": \"any\"\n}\n</code></pre></p> <p>Structured Text: <pre><code>{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"^SECTION [0-9]+\"},\n    {\"type\": \"delimiter\", \"value\": \"---\"}\n  ],\n  \"combine\": \"any\"\n}```\n\n**XML/HTML:**\n```json\n{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"&lt;div class=\\\"chapter\\\"&gt;\"},\n    {\"type\": \"pattern\", \"value\": \"&lt;section&gt;\"}\n  ],\n  \"combine\": \"any\"\n}\n</code></pre></p>"},{"location":"userGuide/chunkgui/#when-to-use_2","title":"When to Use","text":"<ul> <li>Structured Documents: Known formatting patterns</li> <li>Custom Formats: Proprietary document structures</li> <li>Precise Control: Exact splitting behavior required</li> <li>Multi-Criteria: Complex rules combining patterns and delimiters</li> </ul>"},{"location":"userGuide/chunkgui/#schema-validation-and-auto-correction","title":"Schema Validation and Auto-Correction","text":"<p>The GUI provides robust validation and automatic correction for schema definitions: - JSON Syntax: Checks for valid JSON structure. - Auto-Correction: Automatically fixes common JSON syntax errors in regex patterns (e.g., unescaped <code>\\s</code>, <code>\\n</code>) and literal control characters, ensuring the schema is always parseable. - Required Fields: Ensures <code>rules</code> and <code>combine</code> are present. - Rule Validation: Verifies rule types and values. - Error Messages: Clear feedback for fixing truly invalid schema issues.</p>"},{"location":"userGuide/chunkgui/#delimiter-based-chunking","title":"Delimiter-Based Chunking","text":"<p>Simple splitting on specified delimiter strings with size controls. Enhanced with dual-input interface for maximum flexibility.</p>"},{"location":"userGuide/chunkgui/#how-to-use_3","title":"How to Use","text":"<ol> <li>Select Strategy: Choose \"Delimiter\" from the chunk strategy dropdown</li> <li>Choose Delimiters: Use the dual-input interface:</li> <li>Quick Select: Choose from common delimiters with descriptive labels</li> <li>Custom Delimiter: Enter any custom delimiter pattern</li> <li>Configure Size: Set chunk size and overlap parameters</li> <li>Upload Documents: Select documents with clear delimiter patterns</li> <li>Process: Click \"Process Documents\"</li> </ol>"},{"location":"userGuide/chunkgui/#enhanced-delimiter-interface","title":"Enhanced Delimiter Interface","text":"<p>The delimiter strategy now provides two input methods for maximum flexibility:</p>"},{"location":"userGuide/chunkgui/#quick-select-common-delimiters","title":"Quick Select (Common Delimiters)","text":"<p>Choose from pre-configured delimiters with clear descriptions: - # (Markdown headers): Split at markdown headers (default) - --- (Horizontal rule): Split at horizontal rules - ### (H3 headers): Split at level 3 markdown headers - ## (H2 headers): Split at level 2 markdown headers - \\n\\n (Double newline): Split at paragraph breaks - \\n (Single newline): Split at line breaks - . : Split at sentence endings - ! : Split at exclamation marks - ? : Split at question marks</p>"},{"location":"userGuide/chunkgui/#custom-delimiter-field","title":"Custom Delimiter Field","text":"<p>Enter any delimiter pattern you need: - Markdown headers: <code>#</code>, <code>##</code>, <code>###</code> - HTML/XML tags: <code>&lt;div&gt;</code>, <code>&lt;/section&gt;</code>, <code>&lt;chapter&gt;</code> - Custom patterns: <code>SECTION:</code>, <code>CHAPTER</code>, <code>---</code> - Regex patterns: Any string pattern</p>"},{"location":"userGuide/chunkgui/#smart-combination","title":"Smart Combination","text":"<ul> <li>Automatic merging: Combines quick-select and custom delimiters</li> <li>Duplicate removal: Eliminates duplicate delimiters while preserving order</li> <li>User feedback: Shows selected delimiter count and validation messages</li> </ul>"},{"location":"userGuide/chunkgui/#configuration-options","title":"Configuration Options","text":"<pre><code>Chunk Strategy: Delimiter\nQuick Select: # (Markdown headers), --- (Horizontal rule)\nCustom Delimiter: &lt;section&gt;\nCombined Delimiters: ['#', '---', '&lt;section&gt;']\nChunk Size: 1000 characters\nOverlap: 100 characters\n</code></pre>"},{"location":"userGuide/chunkgui/#when-to-use_3","title":"When to Use","text":"<ul> <li>Simple Structures: Documents with clear separator patterns</li> <li>Known Formats: Consistent delimiter usage</li> <li>Quick Processing: Fast chunking for basic requirements</li> <li>Text Files: Plain text with obvious breaks</li> </ul>"},{"location":"userGuide/chunkgui/#token-based-chunking","title":"Token-Based Chunking","text":"<p>Traditional token counting with overlap for LLM compatibility.</p>"},{"location":"userGuide/chunkgui/#how-to-use_4","title":"How to Use","text":"<ol> <li>Select Strategy: Choose \"Token\" from the chunk strategy dropdown</li> <li>Configure Parameters:</li> <li>Chunk Size: Set token count per chunk (256-2048 recommended)</li> <li>Overlap: Set token overlap between chunks (10-25% of chunk size)</li> <li>Upload Documents: Select documents for LLM processing</li> <li>Choose Parser: Select appropriate document parser</li> <li>Process: Click \"Process Documents\"</li> </ol>"},{"location":"userGuide/chunkgui/#token-estimation","title":"Token Estimation","text":"<ul> <li>English Text: ~4 characters per token</li> <li>Code: ~2-3 characters per token</li> <li>Mathematical: ~1-2 characters per token</li> </ul>"},{"location":"userGuide/chunkgui/#example-configuration_1","title":"Example Configuration","text":"<pre><code>Chunk Strategy: Token\nChunk Size: 512 tokens\nOverlap: 50 tokens\nParser: gemini\nFiles: research_paper.pdf\n</code></pre>"},{"location":"userGuide/chunkgui/#when-to-use_4","title":"When to Use","text":"<ul> <li>LLM Input: Prepare documents for language model processing</li> <li>API Limits: Respect token limits of AI services</li> <li>Embeddings: Create chunks for vector embeddings</li> <li>Legacy Compatibility: Existing token-based workflows</li> </ul>"},{"location":"userGuide/chunkgui/#processing-workflow","title":"Processing Workflow","text":""},{"location":"userGuide/chunkgui/#step-by-step-process","title":"Step-by-Step Process","text":"<ol> <li>Project Selection</li> <li>Choose existing project or create new one</li> <li> <p>Verify project permissions and space</p> </li> <li> <p>Document Upload</p> </li> <li>Drag and drop multiple files</li> <li>Automatic file type detection</li> <li>Progress indicators for large uploads</li> <li> <p>Support for batch processing</p> </li> <li> <p>Strategy Configuration</p> </li> <li>Select chunking strategy from dropdown</li> <li>Dynamic form fields appear based on selection</li> <li>Real-time validation and helpful hints</li> <li> <p>Preview of expected behavior</p> </li> <li> <p>Parser Selection</p> </li> <li>Choose appropriate document parser</li> <li>Consider document type and complexity</li> <li> <p>Balance speed vs. accuracy</p> </li> <li> <p>Processing Execution</p> </li> <li>Click \"Process Documents\" to start</li> <li>Real-time progress monitoring</li> <li>Job status updates and estimated completion</li> <li> <p>Error handling and retry options</p> </li> <li> <p>Results Review</p> </li> <li>View processing statistics</li> <li>Check chunk quality and count</li> <li>Download results or proceed to next steps</li> <li>Error logs and troubleshooting information</li> </ol>"},{"location":"userGuide/chunkgui/#progress-monitoring","title":"Progress Monitoring","text":"<p>The GUI provides comprehensive progress tracking:</p> <ul> <li>Upload Progress: File-by-file upload status</li> <li>Processing Status: Current operation and completion percentage</li> <li>Job Management: Background job tracking with unique IDs</li> <li>Error Reporting: Clear error messages with suggested fixes</li> <li>Results Summary: Chunk counts, processing time, and quality metrics</li> </ul>"},{"location":"userGuide/chunkgui/#error-handling","title":"Error Handling","text":"<p>Common Issues and Solutions:</p> <ul> <li>Invalid Schema: Check JSON syntax and required fields</li> <li>Unsupported File Type: Verify file format compatibility</li> <li>API Key Missing: Configure AI service credentials in Settings</li> <li>Large File Timeout: Use character strategy or increase timeout</li> <li>Memory Issues: Reduce chunk size or process in batches</li> </ul>"},{"location":"userGuide/chunkgui/#advanced-features","title":"Advanced Features","text":""},{"location":"userGuide/chunkgui/#batch-processing","title":"Batch Processing","text":"<p>Process multiple documents simultaneously:</p> <ol> <li>Upload several files at once</li> <li>Configure chunking strategy once</li> <li>Process all documents with same settings</li> <li>Monitor collective progress</li> <li>Review batch results</li> </ol>"},{"location":"userGuide/chunkgui/#strategy-comparison","title":"Strategy Comparison","text":"<p>Test different strategies on the same document:</p> <ol> <li>Process same document with different strategies</li> <li>Compare chunk counts and quality</li> <li>Analyze processing time differences</li> <li>Choose optimal strategy for your use case</li> </ol>"},{"location":"userGuide/chunkgui/#custom-workflows","title":"Custom Workflows","text":"<p>Combine chunking with other Compileo features:</p> <ol> <li>Chunk \u2192 Extract: Use chunks for selective taxonomy extraction</li> <li>Chunk \u2192 Generate: Feed chunks to dataset generation</li> <li>Chunk \u2192 Quality: Analyze chunk quality metrics</li> <li>Chunk \u2192 Benchmark: Test chunking impact on model performance</li> </ol>"},{"location":"userGuide/chunkgui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/chunkgui/#strategy-selection-guide","title":"Strategy Selection Guide","text":"Document Type Recommended Strategy Reasoning Simple text files Character Fast, predictable, no AI required Markdown docs Schema Precise control with header patterns Legal documents Semantic Complex clause and obligation boundaries Medical records Semantic Clinical context and diagnosis integrity Research papers Semantic Methodological and discussion coherence Structured data Delimiter Known separators and formatting Code repositories Token LLM processing compatibility Mixed content Schema Flexible rule combinations"},{"location":"userGuide/chunkgui/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Large Documents: Use character strategy for speed</li> <li>Many Documents: Process in batches to manage memory</li> <li>AI Strategies: Choose appropriate models for your needs</li> <li>Quality vs Speed: Balance requirements with processing time</li> </ul>"},{"location":"userGuide/chunkgui/#quality-assurance","title":"Quality Assurance","text":"<ul> <li>Test Small First: Try strategies on sample documents</li> <li>Validate Results: Check chunk boundaries and content integrity</li> <li>Iterate Prompts: Refine semantic prompts based on results</li> <li>Monitor Metrics: Track processing time and chunk quality</li> </ul>"},{"location":"userGuide/chunkgui/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"userGuide/chunkgui/#dataset-generation-workflow","title":"Dataset Generation Workflow","text":"<ol> <li>Chunk Documents \u2192 Select appropriate chunking strategy</li> <li>Review Chunks \u2192 Verify quality and boundaries</li> <li>Generate Dataset \u2192 Use chunks as input for dataset creation</li> <li>Quality Check \u2192 Analyze dataset quality metrics</li> <li>Iterate \u2192 Refine chunking strategy if needed</li> </ol>"},{"location":"userGuide/chunkgui/#taxonomy-integration","title":"Taxonomy Integration","text":"<ol> <li>Process Documents \u2192 Create high-quality chunks</li> <li>Generate Taxonomy \u2192 Use chunks for AI taxonomy creation</li> <li>Selective Extraction \u2192 Apply taxonomy to chunks</li> <li>Refine Taxonomy \u2192 Improve based on extraction results</li> </ol>"},{"location":"userGuide/chunkgui/#benchmarking-connection","title":"Benchmarking Connection","text":"<ol> <li>Chunk Documents \u2192 Prepare data for model evaluation</li> <li>Generate Datasets \u2192 Create evaluation datasets</li> <li>Run Benchmarks \u2192 Test model performance on chunked data</li> <li>Analyze Results \u2192 Compare chunking strategy impact</li> </ol> <p>This GUI provides comprehensive document chunking capabilities with an intuitive interface, making advanced chunking strategies accessible to users of all technical levels while maintaining the power and flexibility needed for complex document processing workflows.</p>"},{"location":"userGuide/datasetgenapi/","title":"Dataset Generation in Compileo API","text":""},{"location":"userGuide/datasetgenapi/#overview","title":"Overview","text":"<p>The Compileo API provides comprehensive endpoints for generating high-quality datasets from processed document chunks. The dataset generation process supports multiple AI models, taxonomy integration, and various output formats with optional quality analysis and benchmarking.</p>"},{"location":"userGuide/datasetgenapi/#base-url-apiv1","title":"Base URL: <code>/api/v1</code>","text":""},{"location":"userGuide/datasetgenapi/#key-features","title":"Key Features","text":"<ul> <li>Multi-Model Support: Choose from Gemini, Grok, or Ollama for parsing, chunking, and classification</li> <li>Taxonomy Integration: Generate datasets aligned with existing taxonomies</li> <li>Flexible Output: JSONL and Parquet formats supported</li> <li>Quality Assurance: Optional quality analysis with configurable thresholds</li> <li>Performance Benchmarking: Optional AI model benchmarking after generation</li> <li>Asynchronous Processing: All generation runs as background jobs with real-time monitoring</li> </ul>"},{"location":"userGuide/datasetgenapi/#api-endpoints","title":"API Endpoints","text":""},{"location":"userGuide/datasetgenapi/#1-generate-dataset","title":"1. Generate Dataset","text":"<p>Submits a dataset generation job with comprehensive configuration options.</p> <ul> <li>Endpoint: <code>POST /datasets/generate</code></li> <li>Description: Initiates dataset generation from processed chunks with full configuration control</li> <li> <p>Request Body: <pre><code>{\n  \"project_id\": 1,\n  \"data_source\": \"Chunks Only\",\n  \"prompt_name\": \"default\",\n  \"custom_prompt\": \"Optional custom prompt content\",\n  \"selected_categories\": [\"category1\", \"category2\"],\n  \"generation_mode\": \"default\",\n  \"format_type\": \"jsonl\",\n  \"concurrency\": 1,\n  \"batch_size\": 50,\n  \"include_evaluation_sets\": false,\n  \"taxonomy_project\": null,\n  \"taxonomy_name\": null,\n  \"output_dir\": \".\",\n  \"analyze_quality\": true,\n  \"quality_threshold\": 0.7,\n  \"enable_versioning\": false,\n  \"dataset_name\": null,\n  \"run_benchmarks\": false,\n  \"benchmark_suite\": \"glue\",\n  \"parsing_model\": \"gemini\",\n  \"chunking_model\": \"gemini\",\n  \"classification_model\": \"gemini\",\n  \"datasets_per_chunk\": 3,\n  \"only_validated\": false\n}\n</code></pre></p> </li> <li> <p>Parameters:</p> </li> <li><code>project_id</code> (integer, required): Project containing processed chunks</li> <li><code>data_source</code> (string): Data source mode - \"Chunks Only\", \"Taxonomy\", \"Extract\" (default: \"Chunks Only\")</li> <li><code>extraction_file_id</code> (string, optional): Specific extraction job ID (UUID) when data_source is \"Extract\". Use <code>/api/v1/datasets/extraction-files/{project_id}</code> to get a list of available extraction jobs.</li> <li><code>selected_categories</code> (array of strings, optional): List of category names to filter by when data_source is \"Extract\"</li> <li><code>prompt_name</code> (string): Name of prompt template to use (default: \"default\")</li> <li><code>custom_prompt</code> (string, optional): Custom prompt content for generation</li> <li><code>generation_mode</code> (string): Generation mode - \"instruction following\", \"question and answer\", \"question\", \"answer\", \"summarization\"</li> <li><code>format_type</code> (string): Output format - \"jsonl\", \"parquet\", or plugin formats (e.g., \"anki\")</li> <li><code>concurrency</code> (integer): Number of parallel processing threads</li> <li><code>batch_size</code> (integer): Number of chunks to process per batch (0 = all at once)</li> <li><code>include_evaluation_sets</code> (boolean): Generate train/validation/test splits</li> <li><code>taxonomy_project</code> (string, optional): Project name containing taxonomy</li> <li><code>taxonomy_name</code> (string, optional): Name of taxonomy to align with</li> <li><code>output_dir</code> (string): Output directory path</li> <li><code>analyze_quality</code> (boolean): Enable quality analysis</li> <li><code>quality_threshold</code> (float): Quality threshold for pass/fail (0-1)</li> <li><code>enable_versioning</code> (boolean): Enable dataset versioning</li> <li><code>dataset_name</code> (string, optional): Name for versioned datasets</li> <li><code>run_benchmarks</code> (boolean): Run AI model benchmarks after generation</li> <li><code>benchmark_suite</code> (string): Benchmark suite - \"glue\", \"superglue\", \"mmlu\", \"medical\"</li> <li><code>parsing_model</code> (string): AI model for document parsing</li> <li><code>chunking_model</code> (string): AI model for text chunking</li> <li><code>classification_model</code> (string): AI model for content classification</li> <li><code>datasets_per_chunk</code> (integer): Maximum datasets per text chunk</li> <li><code>only_validated</code> (boolean): Filter extraction results to only include data that has passed validation</li> </ul> <p>Data Source Modes:</p> <ul> <li>\"Chunks Only\": Uses raw text chunks directly from processed documents. No taxonomy or extraction filtering required. Best for basic dataset generation from any content.</li> <li>\"Taxonomy\": Applies taxonomy definitions to enhance generation prompts. Works with all chunks in the project (no extraction dependency). Adds domain-specific context and terminology. This mode strictly bypasses extraction data loading for efficiency.</li> <li>\"Extract\": Uses extracted entities as the primary content source. Generates datasets focused on specific concepts/entities. Creates educational content about extracted terms.</li> </ul> <p>Batch Processing:   - <code>batch_size</code> controls memory usage by processing chunks in smaller groups   - Set to 0 to process all chunks at once (legacy behavior)   - Smaller batches reduce memory usage but may take longer   - Each batch creates a separate output file: <code>dataset_[job_id]_batch_[N].[format]</code>. Parquet exports are saved as binary <code>.parquet</code> files, while JSON/JSONL are saved as UTF-8 text files.</p> <p>Generation Mode Options:</p> <ul> <li>\"default\": Generates content based on the provided prompt name or custom prompt.</li> <li>\"instruction following\" (Recommended): Generates modern instruction-response pairs following Alpaca/Dolly standards for advanced AI fine-tuning</li> <li>\"question and answer\": Creates traditional Q&amp;A pairs from document content</li> <li>\"question\": Generates questions without corresponding answers</li> <li>\"answer\": Generates answers without explicit questions</li> <li> <p>\"summarization\": Creates concise summaries of document content</p> </li> <li> <p>Success Response (200 OK): <pre><code>{\n  \"job_id\": \"dataset-gen-uuid-123\",\n  \"message\": \"Dataset generation started successfully\",\n  \"status\": \"submitted\",\n  \"estimated_duration\": \"Processing in background\"\n}\n</code></pre></p> </li> </ul>"},{"location":"userGuide/datasetgenapi/#2-generate-evaluation-dataset","title":"2. Generate Evaluation Dataset","text":"<p>Creates comprehensive evaluation datasets with train/validation/test splits.</p> <ul> <li>Endpoint: <code>POST /datasets/generate-evaluation</code></li> <li>Description: Generates evaluation-ready datasets with multiple splits and quality analysis</li> <li>Request Body: Same as <code>/datasets/generate</code> but optimized for evaluation</li> <li>Success Response: Same format as dataset generation</li> </ul>"},{"location":"userGuide/datasetgenapi/#3-get-dataset-generation-status","title":"3. Get Dataset Generation Status","text":"<p>Monitors the progress of dataset generation jobs.</p> <ul> <li>Endpoint: <code>GET /datasets/generate/{job_id}/status</code></li> <li>Description: Returns current status and progress of dataset generation. This endpoint implements a persistent database fallback, ensuring status is available even after server restarts or worker instance changes.</li> <li>Path Parameters:</li> <li><code>job_id</code> (string, required): Dataset generation job ID</li> <li>Success Response (200 OK): <pre><code>{\n  \"job_id\": \"dataset-gen-uuid-123\",\n  \"status\": \"running\",\n  \"progress\": 65,\n  \"current_step\": \"Processing batch 2/3 (25 chunks)\",\n  \"estimated_completion\": \"2024-01-21T14:30:00Z\",\n  \"result\": {\n    \"batch_files\": [\n      {\n        \"batch_index\": 0,\n        \"file_path\": \"/storage/datasets/1/dataset_dataset-gen-uuid-123_batch_0.json\",\n        \"entries_count\": 15,\n        \"chunks_processed\": 5\n      },\n      {\n        \"batch_index\": 1,\n        \"file_path\": \"/storage/datasets/1/dataset_dataset-gen-uuid-123_batch_1.json\",\n        \"entries_count\": 18,\n        \"chunks_processed\": 5\n      }\n    ],\n    \"completed_batches\": 2,\n    \"total_batches\": 3,\n    \"total_entries_so_far\": 33\n  }\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#4-get-dataset","title":"4. Get Dataset","text":"<p>Retrieves generated dataset details and entries.</p> <ul> <li>Endpoint: <code>GET /datasets/{dataset_id}</code></li> <li>Description: Returns dataset metadata and summary information</li> <li>Path Parameters:</li> <li><code>dataset_id</code> (string, required): Dataset identifier</li> <li>Success Response (200 OK): <pre><code>{\n  \"id\": \"dataset_123\",\n  \"name\": \"Medical Diagnosis Dataset\",\n  \"entries\": [],\n  \"total_entries\": 1500,\n  \"created_at\": \"2024-01-21T12:00:00Z\",\n  \"format_type\": \"jsonl\",\n  \"batch_files\": [\n    {\n      \"batch_index\": 0,\n      \"file_path\": \"/storage/datasets/1/dataset_123_batch_0.json\",\n      \"entries_count\": 500,\n      \"chunks_processed\": 50\n    },\n    {\n      \"batch_index\": 1,\n      \"file_path\": \"/storage/datasets/1/dataset_123_batch_1.json\",\n      \"entries_count\": 500,\n      \"chunks_processed\": 50\n    },\n    {\n      \"batch_index\": 2,\n      \"file_path\": \"/storage/datasets/1/dataset_123_batch_2.json\",\n      \"entries_count\": 500,\n      \"chunks_processed\": 50\n    }\n  ],\n  \"total_batches\": 3,\n  \"quality_summary\": {\n    \"overall_score\": 0.85,\n    \"diversity_score\": 0.82,\n    \"bias_score\": 0.88\n  }\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#5-get-dataset-entries","title":"5. Get Dataset Entries","text":"<p>Retrieves dataset entries with pagination and filtering.</p> <ul> <li>Endpoint: <code>GET /datasets/{dataset_id}/entries</code></li> <li>Description: Returns paginated dataset entries with optional filtering</li> <li>Path Parameters:</li> <li><code>dataset_id</code> (string, required): Dataset identifier</li> <li>Query Parameters:</li> <li><code>page</code> (integer): Page number (default: 1)</li> <li><code>per_page</code> (integer): Entries per page (default: 50)</li> <li><code>filter_quality</code> (float, optional): Minimum quality score filter</li> <li><code>sort_by</code> (string): Sort field - \"quality\", \"category\", \"difficulty\"</li> <li>Success Response (200 OK): <pre><code>{\n  \"entries\": [\n    {\n      \"id\": \"entry_1\",\n      \"question\": \"What are the symptoms of myocardial infarction?\",\n      \"answer\": \"Chest pain, shortness of breath, diaphoresis...\",\n      \"category\": \"cardiology\",\n      \"quality_score\": 0.92,\n      \"difficulty\": \"intermediate\",\n      \"source_chunk\": \"chunk_45\",\n      \"metadata\": {\n        \"model\": \"gemini\",\n        \"taxonomy_level\": 2\n      }\n    }\n  ],\n  \"total\": 1500,\n  \"page\": 1,\n  \"per_page\": 50,\n  \"quality_summary\": {\n    \"average_score\": 0.85,\n    \"distribution\": {\"high\": 1200, \"medium\": 250, \"low\": 50}\n  }\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#6-update-dataset-entry","title":"6. Update Dataset Entry","text":"<p>Updates individual dataset entries for refinement.</p> <ul> <li>Endpoint: <code>PUT /datasets/{dataset_id}/entries/{entry_id}</code></li> <li>Description: Modifies question, answer, or metadata for dataset entries</li> <li>Path Parameters:</li> <li><code>dataset_id</code> (string, required): Dataset identifier</li> <li><code>entry_id</code> (string, required): Entry identifier</li> <li>Request Body: <pre><code>{\n  \"question\": \"Updated question text\",\n  \"answer\": \"Updated answer text\",\n  \"category\": \"updated_category\",\n  \"feedback\": \"User feedback on this entry\"\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#7-submit-dataset-feedback","title":"7. Submit Dataset Feedback","text":"<p>Collects user feedback on dataset quality and relevance.</p> <ul> <li>Endpoint: <code>POST /datasets/{dataset_id}/feedback</code></li> <li>Description: Submits bulk feedback for multiple dataset entries</li> <li>Request Body: <pre><code>{\n  \"entry_ids\": [\"entry_1\", \"entry_2\"],\n  \"feedback_type\": \"bulk_edit\",\n  \"comments\": \"General feedback on these entries\",\n  \"rating\": 4\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#8-get-extraction-files","title":"8. Get Extraction Files","text":"<p>Retrieves available extraction jobs for dataset generation.</p> <ul> <li>Endpoint: <code>GET /datasets/extraction-files/{project_id}</code></li> <li>Description: Returns list of completed extraction jobs for a project, used to populate extraction file selection dropdown.</li> <li>Path Parameters:</li> <li><code>project_id</code> (string, required): Project ID (UUID) to get extraction files for. Accepts both integer and string formats for compatibility.</li> <li>Success Response (200 OK): <pre><code>{\n  \"extraction_files\": [\n    {\n      \"id\": 123,\n      \"job_id\": 123,\n      \"status\": \"completed\",\n      \"created_at\": \"2024-01-15T10:30:00Z\",\n      \"extraction_type\": \"ner\",\n      \"entity_count\": 150,\n      \"display_name\": \"Job 123 - ner (150 entities)\"\n    },\n    {\n      \"id\": 124,\n      \"job_id\": 124,\n      \"status\": \"completed\",\n      \"created_at\": \"2024-01-16T14:20:00Z\",\n      \"extraction_type\": \"whole_text\",\n      \"entity_count\": 89,\n      \"display_name\": \"Job 124 - whole_text (89 entities)\"\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#9-regenerate-dataset-entries","title":"9. Regenerate Dataset Entries","text":"<p>Triggers regeneration of specific dataset entries.</p> <ul> <li>Endpoint: <code>POST /datasets/{dataset_id}/regenerate</code></li> <li>Description: Regenerates entries with updated parameters or models</li> <li>Request Body: <pre><code>{\n  \"entry_ids\": [\"entry_1\", \"entry_2\"],\n  \"regeneration_config\": {\n    \"model\": \"grok\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 500\n  }\n}\n</code></pre></li> </ul>"},{"location":"userGuide/datasetgenapi/#10-download-dataset","title":"10. Download Dataset","text":"<p>Downloads generated datasets in the specified format.</p> <ul> <li>Endpoint: <code>GET /datasets/{dataset_id}/download</code></li> <li>Description: Downloads dataset file (JSONL/Parquet)</li> <li>Path Parameters:</li> <li><code>dataset_id</code> (string, required): Dataset identifier</li> <li>Success Response: File download with appropriate content-type</li> </ul>"},{"location":"userGuide/datasetgenapi/#integration-with-job-management","title":"Integration with Job Management","text":"<p>All dataset generation operations return a <code>job_id</code> and run asynchronously. Dataset generation jobs are now stored in the database and persist across API server restarts. Use the dedicated dataset generation status endpoint to monitor progress:</p> <pre><code>POST /api/v1/datasets/generate\nContent-Type: application/json\n\n{\n  \"project_id\": 1,\n  \"classification_model\": \"gemini\",\n  \"analyze_quality\": true\n}\n\nResponse:\n{\n  \"job_id\": \"dataset-gen-uuid-789\",\n  \"message\": \"Dataset generation started\"\n}\n\n# Monitor progress (dataset jobs persist across server restarts)\nGET /api/v1/datasets/generate/dataset-gen-uuid-789/status\n</code></pre>"},{"location":"userGuide/datasetgenapi/#model-selection-guidelines","title":"Model Selection Guidelines","text":""},{"location":"userGuide/datasetgenapi/#parsing-models","title":"Parsing Models","text":"<ul> <li>Gemini: Best for complex document understanding and multi-format support</li> <li>Grok: Good for technical and structured content</li> <li>Ollama: Local processing, no API keys required</li> </ul>"},{"location":"userGuide/datasetgenapi/#classification-models","title":"Classification Models","text":"<ul> <li>Gemini: Superior taxonomy alignment and category classification</li> <li>Grok: Better for nuanced medical and technical categorization</li> <li>Ollama: Cost-effective for high-volume processing</li> </ul>"},{"location":"userGuide/datasetgenapi/#chunking-models","title":"Chunking Models","text":"<ul> <li>Gemini: Intelligent semantic chunking with context awareness</li> <li>Grok: Good for technical document segmentation</li> <li>Ollama: Fast processing for simple chunking strategies</li> </ul>"},{"location":"userGuide/datasetgenapi/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/datasetgenapi/#common-error-responses","title":"Common Error Responses","text":"<ul> <li>400 Bad Request: Invalid parameters or missing required fields</li> <li>404 Not Found: Project, taxonomy, or dataset not found</li> <li>503 Service Unavailable: Job queue manager not initialized (Redis unavailable)</li> </ul>"},{"location":"userGuide/datasetgenapi/#job-specific-errors","title":"Job-Specific Errors","text":"<p>Dataset generation jobs may fail with specific error messages: - <code>\"No chunks found for project\"</code>: Project has no processed chunks - <code>\"API key not configured\"</code>: Selected model requires API key - <code>\"Taxonomy validation failed\"</code>: Specified taxonomy not found or invalid - <code>\"Quality threshold not met\"</code>: Generated dataset failed quality checks</p>"},{"location":"userGuide/datasetgenapi/#best-practices","title":"Best Practices","text":"<ol> <li>Model Selection: Choose appropriate models based on content type and requirements</li> <li>Chunk Preparation: Ensure high-quality chunking before generation</li> <li>Quality Analysis: Enable quality analysis for production datasets</li> <li>Versioning: Use versioning for iterative dataset improvement</li> <li>Monitoring: Always monitor job progress using the jobs API</li> <li>Resource Planning: Consider concurrency limits and processing time for large datasets</li> </ol>"},{"location":"userGuide/datasetgencli/","title":"Dataset Generation in Compileo CLI","text":""},{"location":"userGuide/datasetgencli/#overview","title":"Overview","text":"<p>The Compileo Command Line Interface provides comprehensive dataset generation capabilities with full configuration control. Generate high-quality datasets from processed document chunks using various AI models, taxonomy integration, and quality assurance features.</p>"},{"location":"userGuide/datasetgencli/#key-features","title":"Key Features","text":"<ul> <li>Multi-Model Support: Choose from Gemini, Grok, or Ollama models</li> <li>Taxonomy Integration: Align datasets with existing project taxonomies</li> <li>Quality Assurance: Built-in quality analysis with configurable thresholds</li> <li>Performance Benchmarking: Optional AI model benchmarking after generation</li> <li>Flexible Output: JSONL and Parquet format support</li> <li>Versioning Support: Dataset versioning for iterative improvement</li> <li>Asynchronous Processing: Background job execution with progress monitoring</li> </ul>"},{"location":"userGuide/datasetgencli/#cli-command","title":"CLI Command","text":""},{"location":"userGuide/datasetgencli/#generate-dataset","title":"Generate Dataset","text":"<pre><code>compileo generate-dataset [OPTIONS]\n</code></pre> <p>Core Parameters: - <code>--project-id INTEGER</code> (required): Project ID containing processed chunks - <code>--data-source [Chunks Only|Taxonomy|Extract]</code>: Data source for generation (default: \"Chunks Only\") - <code>--prompt-name TEXT</code>: Prompt template name (default: \"default\") - <code>--format-type [jsonl|parquet]</code>: Output format (default: \"jsonl\") - <code>--concurrency INTEGER</code>: Parallel processing threads (default: 1) - <code>--batch-size INTEGER</code>: Number of chunks to process per batch (default: 50, 0 = all at once) - <code>--output-dir PATH</code>: Output directory (default: \".\")</p> <p>Model Selection: - <code>--parsing-model [gemini|grok|ollama]</code>: AI model for document parsing (default: \"gemini\") - <code>--chunking-model [gemini|grok|ollama]</code>: AI model for text chunking (default: \"gemini\") - <code>--classification-model [gemini|grok|ollama]</code>: AI model for content classification (default: \"gemini\")</p> <p>Taxonomy Integration: - <code>--taxonomy-project TEXT</code>: Project name containing taxonomy - <code>--taxonomy-name TEXT</code>: Taxonomy name to align with - <code>--extraction-file-id TEXT</code>: Specific extraction job ID (UUID) when using \"Extract\" data source - <code>--selected-categories TEXT</code>: Comma-separated list of category names to filter by (only for Extract mode)</p> <p>Quality Assurance: - <code>--analyze-quality</code>: Enable quality analysis - <code>--quality-threshold FLOAT</code>: Quality threshold (0-1, default: 0.7) - <code>--quality-config PATH</code>: Path to quality configuration JSON</p> <p>Benchmarking: - <code>--run-benchmarks</code>: Run AI model benchmarks after generation - <code>--benchmark-suite TEXT</code>: Benchmark suite (default: \"glue\") - <code>--benchmark-config PATH</code>: Path to benchmarking configuration JSON</p> <p>Advanced Options: - <code>--include-evaluation-sets</code>: Generate train/validation/test splits - <code>--enable-versioning</code>: Enable dataset versioning - <code>--dataset-name TEXT</code>: Name for versioned datasets - <code>--category-limits TEXT</code>: Comma-separated category limits per taxonomy level - <code>--specificity-level INTEGER</code>: Taxonomy specificity level (1-5, default: 1) - <code>--custom-audience TEXT</code>: Target audience description - <code>--custom-purpose TEXT</code>: Dataset purpose description - <code>--complexity-level [auto|basic|intermediate|advanced]</code>: Content complexity (default: \"intermediate\") - <code>--domain TEXT</code>: Knowledge domain (default: \"general\") - <code>--datasets-per-chunk INTEGER</code>: Maximum datasets per chunk (default: 3)</p>"},{"location":"userGuide/datasetgencli/#data-source-options","title":"Data Source Options","text":""},{"location":"userGuide/datasetgencli/#chunks-only-mode","title":"Chunks Only Mode","text":"<pre><code># Basic generation from raw chunks\ncompileo generate-dataset --project-id 1 --data-source \"Chunks Only\"\n</code></pre>"},{"location":"userGuide/datasetgencli/#taxonomy-mode","title":"Taxonomy Mode","text":"<pre><code># Taxonomy-enhanced generation (no extraction required)\ncompileo generate-dataset --project-id 1 --data-source \"Taxonomy\" --taxonomy-project medical --taxonomy-name icd_10\n</code></pre>"},{"location":"userGuide/datasetgencli/#extract-mode","title":"Extract Mode","text":"<pre><code># Entity-focused generation from extracted concepts\ncompileo generate-dataset --project-id 1 --data-source \"Extract\"\n\n# Use specific extraction job for targeted dataset generation\ncompileo generate-dataset --project-id 1 --data-source \"Extract\" --extraction-file-id 123\n\n# Filter by specific categories\ncompileo generate-dataset --project-id 1 --data-source \"Extract\" --selected-categories \"Medical Conditions,Treatments\"\n</code></pre>"},{"location":"userGuide/datasetgencli/#usage-examples","title":"Usage Examples","text":""},{"location":"userGuide/datasetgencli/#basic-dataset-generation","title":"Basic Dataset Generation","text":"<pre><code># Generate dataset from project 1\ncompileo generate-dataset --project-id 1\n\n# Specify output directory and format\ncompileo generate-dataset --project-id 1 --output-dir ./datasets --format-type jsonl\n\n# Use batch processing for memory efficiency\ncompileo generate-dataset --project-id 1 --batch-size 25 --concurrency 2\n</code></pre>"},{"location":"userGuide/datasetgencli/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Full configuration with quality analysis\ncompileo generate-dataset \\\n  --project-id 1 \\\n  --parsing-model gemini \\\n  --chunking-model grok \\\n  --classification-model gemini \\\n  --taxonomy-project medical_taxonomy \\\n  --taxonomy-name icd_10 \\\n  --analyze-quality \\\n  --quality-threshold 0.8 \\\n  --run-benchmarks \\\n  --benchmark-suite glue \\\n  --concurrency 3 \\\n  --output-dir ./medical_datasets\n</code></pre>"},{"location":"userGuide/datasetgencli/#evaluation-dataset-creation","title":"Evaluation Dataset Creation","text":"<pre><code># Generate evaluation-ready datasets with splits\ncompileo generate-dataset \\\n  --project-id 1 \\\n  --include-evaluation-sets \\\n  --enable-versioning \\\n  --dataset-name medical_qa_v1 \\\n  --analyze-quality \\\n  --quality-threshold 0.85\n</code></pre>"},{"location":"userGuide/datasetgencli/#custom-taxonomy-integration","title":"Custom Taxonomy Integration","text":"<pre><code># Use custom taxonomy with specific parameters\ncompileo generate-dataset \\\n  --project-id 1 \\\n  --taxonomy-project cardiology \\\n  --taxonomy-name heart_conditions \\\n  --specificity-level 3 \\\n  --category-limits 10,15,20 \\\n  --custom-audience \"medical residents\" \\\n  --custom-purpose \"board exam preparation\"\n</code></pre>"},{"location":"userGuide/datasetgencli/#model-selection-guidelines","title":"Model Selection Guidelines","text":""},{"location":"userGuide/datasetgencli/#when-to-use-each-model","title":"When to Use Each Model","text":"<p>Gemini: - Best for complex document understanding - Superior taxonomy alignment - Good for multi-format document processing - Recommended for production datasets</p> <p>Grok: - Excellent for technical and medical content - Better nuance detection in specialized domains - Good for complex categorization tasks - Cost-effective alternative to Gemini</p> <p>Ollama: - Local processing (no API keys required) - Best for development and testing - Limited by local hardware capabilities - Good for high-volume processing on local infrastructure</p>"},{"location":"userGuide/datasetgencli/#model-combinations","title":"Model Combinations","text":"<pre><code># Production setup - Gemini for all tasks\ncompileo generate-dataset --project-id 1 \\\n  --parsing-model gemini \\\n  --chunking-model gemini \\\n  --classification-model gemini\n\n# Balanced approach - Mix models for cost optimization\ncompileo generate-dataset --project-id 1 \\\n  --parsing-model gemini \\\n  --chunking-model grok \\\n  --classification-model gemini\n\n# Local development - Ollama for all tasks\ncompileo generate-dataset --project-id 1 \\\n  --parsing-model ollama \\\n  --chunking-model ollama \\\n  --classification-model ollama\n</code></pre>"},{"location":"userGuide/datasetgencli/#quality-assurance-integration","title":"Quality Assurance Integration","text":""},{"location":"userGuide/datasetgencli/#automatic-quality-analysis","title":"Automatic Quality Analysis","text":"<pre><code># Enable quality analysis with default settings\ncompileo generate-dataset --project-id 1 --analyze-quality\n\n# Custom quality threshold\ncompileo generate-dataset --project-id 1 \\\n  --analyze-quality \\\n  --quality-threshold 0.9\n\n# Use custom quality configuration\ncompileo generate-dataset --project-id 1 \\\n  --analyze-quality \\\n  --quality-config ./quality_config.json\n</code></pre>"},{"location":"userGuide/datasetgencli/#quality-configuration-file","title":"Quality Configuration File","text":"<p>Create a <code>quality_config.json</code>:</p> <pre><code>{\n  \"enabled\": true,\n  \"diversity\": {\n    \"enabled\": true,\n    \"threshold\": 0.6,\n    \"min_lexical_diversity\": 0.3\n  },\n  \"bias\": {\n    \"enabled\": true,\n    \"threshold\": 0.3\n  },\n  \"difficulty\": {\n    \"enabled\": true,\n    \"threshold\": 0.7,\n    \"target_difficulty\": \"intermediate\"\n  },\n  \"consistency\": {\n    \"enabled\": true,\n    \"threshold\": 0.8\n  },\n  \"output_format\": \"json\",\n  \"fail_on_any_failure\": false\n}\n</code></pre>"},{"location":"userGuide/datasetgencli/#benchmarking-integration","title":"Benchmarking Integration","text":""},{"location":"userGuide/datasetgencli/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code># Run GLUE benchmarks after generation\ncompileo generate-dataset --project-id 1 --run-benchmarks\n\n# Specify benchmark suite\ncompileo generate-dataset --project-id 1 \\\n  --run-benchmarks \\\n  --benchmark-suite superglue\n\n# Use custom benchmark configuration\ncompileo generate-dataset --project-id 1 \\\n  --run-benchmarks \\\n  --benchmark-config ./benchmark_config.json\n</code></pre>"},{"location":"userGuide/datasetgencli/#benchmark-configuration-file","title":"Benchmark Configuration File","text":"<p>Create a <code>benchmark_config.json</code>:</p> <pre><code>{\n  \"enabled\": true,\n  \"benchmark\": {\n    \"suites\": [\"glue\", \"mmlu\"],\n    \"model_path\": \"/path/to/model\",\n    \"batch_size\": 32\n  },\n  \"metrics\": {\n    \"enabled_metrics\": [\"accuracy\", \"f1\", \"bleu\"],\n    \"custom_metrics\": []\n  },\n  \"tracking\": {\n    \"enabled\": true,\n    \"storage_path\": \"benchmark_results\"\n  }\n}\n</code></pre>"},{"location":"userGuide/datasetgencli/#dataset-versioning","title":"Dataset Versioning","text":""},{"location":"userGuide/datasetgencli/#version-control-for-datasets","title":"Version Control for Datasets","text":"<pre><code># Enable versioning with custom name\ncompileo generate-dataset --project-id 1 \\\n  --enable-versioning \\\n  --dataset-name medical_qa_dataset\n\n# Version types: major, minor, patch\ncompileo dataset-version increment-version \\\n  --project-id 1 \\\n  --dataset-name medical_qa_dataset \\\n  --version-type minor \\\n  --description \"Added cardiology questions\"\n</code></pre>"},{"location":"userGuide/datasetgencli/#version-management","title":"Version Management","text":"<pre><code># List dataset versions\ncompileo dataset-version list-versions \\\n  --project-id 1 \\\n  --dataset-name medical_qa_dataset\n\n# Compare versions\ncompileo dataset-version compare-versions \\\n  --project-id 1 \\\n  --dataset-name medical_qa_dataset \\\n  --version1 1.0.0 \\\n  --version2 1.1.0\n\n# Rollback to previous version\ncompileo dataset-version rollback \\\n  --project-id 1 \\\n  --dataset-name medical_qa_dataset \\\n  --target-version 1.0.0\n</code></pre>"},{"location":"userGuide/datasetgencli/#job-monitoring-and-management","title":"Job Monitoring and Management","text":""},{"location":"userGuide/datasetgencli/#monitoring-dataset-generation","title":"Monitoring Dataset Generation","text":"<p>Dataset generation jobs are now stored in the database and persist across CLI sessions and server restarts. Use the dedicated dataset status commands:</p> <pre><code># Start dataset generation\ncompileo generate-dataset --project-id 1 --analyze-quality\n# Output: Job submitted with ID: dataset-gen-uuid-123\n\n# Monitor job status (jobs persist across sessions)\ncompileo generate-dataset --status dataset-gen-uuid-123\n\n# Poll for completion with timeout\ncompileo generate-dataset --status dataset-gen-uuid-123 --poll --timeout 300\n\n# Get general job statistics\ncompileo jobs stats\n</code></pre>"},{"location":"userGuide/datasetgencli/#managing-jobs","title":"Managing Jobs","text":"<pre><code># Cancel a running job\ncompileo jobs cancel dataset-gen-uuid-123 --confirm\n\n# Restart a failed job\ncompileo jobs restart dataset-gen-uuid-123 --confirm\n\n# Start worker process (required for job processing)\ncompileo jobs worker --redis-url redis://localhost:6379/0\n</code></pre> <p>Note: Dataset generation jobs use database persistence instead of the traditional job queue system, ensuring reliability across server restarts.</p>"},{"location":"userGuide/datasetgencli/#output-and-results","title":"Output and Results","text":""},{"location":"userGuide/datasetgencli/#generated-files","title":"Generated Files","text":"<p>Dataset generation creates per-batch output files:</p> <pre><code>output_dir/\n\u251c\u2500\u2500 dataset_[job_id]_batch_0.jsonl     # First batch results\n\u251c\u2500\u2500 dataset_[job_id]_batch_1.jsonl     # Second batch results\n\u251c\u2500\u2500 dataset_[job_id]_batch_N.jsonl     # Nth batch results\n\u251c\u2500\u2500 dataset_[job_id]_quality.json      # Quality analysis report (if enabled)\n\u251c\u2500\u2500 benchmark_results_[job_id]/        # Benchmarking results (if enabled)\n\u2502   \u251c\u2500\u2500 glue_results.json\n\u2502   \u2514\u2500\u2500 performance_metrics.json\n\u2514\u2500\u2500 dataset_[job_id]_metadata.json     # Generation metadata\n</code></pre> <p>Batch File Naming: - Files are created as each batch completes - Format: <code>dataset_[job_id]_batch_[batch_index].[format_type]</code> - Number of files equals number of batches (controlled by <code>--batch-size</code>) - Each file contains only the results from that specific batch</p>"},{"location":"userGuide/datasetgencli/#dataset-format","title":"Dataset Format","text":"<p>JSONL Format: <pre><code>{\"question\": \"What are the symptoms of myocardial infarction?\", \"answer\": \"Chest pain, shortness of breath, diaphoresis...\", \"category\": \"cardiology\", \"quality_score\": 0.92, \"difficulty\": \"intermediate\"}\n{\"question\": \"How is hypertension diagnosed?\", \"answer\": \"Blood pressure measurement above 130/80 mmHg...\", \"category\": \"cardiology\", \"quality_score\": 0.88, \"difficulty\": \"basic\"}\n</code></pre></p>"},{"location":"userGuide/datasetgencli/#quality-report","title":"Quality Report","text":"<pre><code>{\n  \"summary\": {\n    \"overall_score\": 0.85,\n    \"total_entries\": 1500,\n    \"passed_threshold\": 1275,\n    \"failed_entries\": 225\n  },\n  \"diversity\": {\n    \"lexical_diversity\": 0.78,\n    \"semantic_diversity\": 0.82,\n    \"topic_balance\": 0.79\n  },\n  \"bias\": {\n    \"demographic_bias\": 0.15,\n    \"content_bias\": 0.12\n  },\n  \"difficulty\": {\n    \"average_readability\": 0.72,\n    \"complexity_distribution\": {\n      \"basic\": 450,\n      \"intermediate\": 900,\n      \"advanced\": 150\n    }\n  }\n}\n</code></pre>"},{"location":"userGuide/datasetgencli/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/datasetgencli/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>\"No chunks found for project\" - Ensure documents have been uploaded and processed - Check that chunking has completed successfully</p> <p>\"API key not configured\" - Set required API keys in environment variables - Use Ollama for local processing without API keys</p> <p>\"Quality threshold not met\" - Lower quality threshold or improve source content - Review quality configuration settings</p> <p>\"Redis connection failed\" - Ensure Redis server is running - Check REDIS_URL environment variable</p>"},{"location":"userGuide/datasetgencli/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/datasetgencli/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Concurrency: Use appropriate concurrency based on available resources</li> <li>Model Selection: Balance cost and quality requirements</li> <li>Chunk Preparation: Ensure high-quality chunking before generation</li> </ol>"},{"location":"userGuide/datasetgencli/#quality-assurance","title":"Quality Assurance","text":"<ol> <li>Enable Quality Analysis: Always use <code>--analyze-quality</code> for production datasets</li> <li>Set Appropriate Thresholds: Balance quality with generation volume</li> <li>Review Quality Reports: Use quality metrics to guide dataset improvement</li> </ol>"},{"location":"userGuide/datasetgencli/#resource-management","title":"Resource Management","text":"<ol> <li>Monitor Job Queue: Use <code>compileo jobs stats</code> to track system load</li> <li>Worker Management: Ensure sufficient worker processes are running</li> <li>Version Control: Use versioning for iterative dataset improvement</li> </ol>"},{"location":"userGuide/datasetgencli/#integration-patterns","title":"Integration Patterns","text":"<pre><code># Complete pipeline: upload \u2192 process \u2192 generate \u2192 analyze\ncompileo documents upload --project-id 1 --file-paths document1.pdf document2.pdf\ncompileo documents process --project-id 1 --document-ids 1,2 --parser gemini\ncompileo generate-dataset --project-id 1 --analyze-quality --run-benchmarks\ncompileo analyze-quality dataset_*.jsonl --format markdown --output quality_report.md\n</code></pre> <p>This CLI provides comprehensive control over dataset generation with enterprise-grade features for quality assurance, benchmarking, and version management.</p>"},{"location":"userGuide/datasetgengui/","title":"Dataset Generation Wizard in Compileo GUI","text":""},{"location":"userGuide/datasetgengui/#overview","title":"Overview","text":"<p>The Compileo Dataset Creation Wizard provides a comprehensive, step-by-step interface for generating high-quality datasets from documents. The wizard features flexible navigation, automatic file upload, complete AI model selection, and intelligent data source handling for optimal dataset creation.</p>"},{"location":"userGuide/datasetgengui/#key-features","title":"Key Features","text":"<ul> <li>5-Step Guided Wizard: Intuitive workflow with database-mediated state recovery</li> <li>Automatic File Upload: Drag-and-drop with immediate processing</li> <li>Complete AI Model Selection: 4-model configuration (parsing, chunking, classification, generation)</li> <li>Full Chunking Strategy Parity: All Document Processing tab strategies available</li> <li>Database-Mediated Resilience: Progress is synchronized with the database, allowing workflows to survive session loss or browser refreshes.</li> <li>Smart Data Source Selection: Automatic taxonomy/chunks fallback</li> <li>Document Management: Upload and delete capabilities with error correction</li> <li>Real-time Progress Monitoring: Live job tracking with detailed status updates</li> <li>Interactive Refinement: Post-generation editing and quality improvement</li> <li>Multi-format Support: JSONL, Parquet, and extensible plugin formats</li> </ul>"},{"location":"userGuide/datasetgengui/#gui-workflow","title":"GUI Workflow","text":""},{"location":"userGuide/datasetgengui/#step-1-project-selection","title":"Step 1: Project Selection","text":"<p>Location: Dataset Creation Wizard \u2192 Step 1 Purpose: Choose or create a project for dataset generation</p> <p>Features: - Existing Project Selection: Browse and select from available projects - New Project Creation: Create project with name and description - Project Statistics: View document count, chunk count, and processing status - Flexible Navigation: Click any step tab to navigate non-linearly</p> <p>Navigation: - Use dropdown to select existing project - Click \"Create New Project\" for new projects - Click \"Next\" or use step tabs to navigate</p>"},{"location":"userGuide/datasetgengui/#step-2-parse-chunk-taxonomy","title":"Step 2: Parse &amp; Chunk &amp; Taxonomy","text":"<p>Location: Dataset Creation Wizard \u2192 Step 2 Purpose: Automated end-to-end processing of project documents.</p> <p>Features: - Unified Pipeline: Combined Upload \u2192 Parse \u2192 Chunk \u2192 Auto Taxonomy generation. - AI Model Selection: Configure models for parsing, chunking, classification, and generation in one place. - Smart Chunking: Full parity with standalone Document Processing strategies. - Existing Chunk Detection: Proactively checks the database to see if documents already have processed chunks. - Automatic Taxonomy Generation: Automatically triggers AI taxonomy creation after chunking completes.</p>"},{"location":"userGuide/datasetgengui/#step-3-edit-taxonomy","title":"Step 3: Edit Taxonomy","text":"<p>Location: Dataset Creation Wizard \u2192 Step 3 Purpose: Pick and refine the hierarchical structure for your dataset.</p> <p>Features: - Taxonomy Selection: Choose from existing taxonomies or the one just auto-generated. - Reactive Editor: A simplified version of the Hybrid Taxonomy Editor. - Real-time Sync: Changes (adding/removing categories, renaming) are instantly persisted to the project database. - Management Tools: Regenerate or delete taxonomies directly from the wizard.</p>"},{"location":"userGuide/datasetgengui/#step-4-generation-parameters","title":"Step 4: Generation Parameters","text":"<p>Location: Dataset Creation Wizard \u2192 Step 4 Purpose: Configure dataset generation mode, format, and high-level prompt parameters.</p> <p>Features: - High-Level Prompts: Tailor content by specifying Target Audience, Purpose, and Domain. - Complexity Control: Select difficulty levels (beginner to expert) or let AI decide. - Auto-Persistence: Parameters are automatically saved to the database when clicking \"Next\", allowing you to resume later. - Dataset Generation Modes: Instruction Following, Q&amp;A, Questions Only, Answers Only, and Summarization. - Plugin Support: Support for specialized formats like Anki via the plugin system.</p>"},{"location":"userGuide/datasetgengui/#step-5-review-generate","title":"Step 5: Review &amp; Generate","text":"<p>Location: Dataset Creation Wizard \u2192 Step 5 Purpose: Final review, execution, and download.</p> <p>Features: - Comprehensive Summary: Review all settings, including AI models and high-level prompts. - Database Fallbacks: Automatically retrieves missing configuration (like chunking strategy) from the database if session state is lost. - Real-time Monitoring: Follow the generation progress in detail, from category aggregation to final formatting. - Instant Download: Upon successful completion, a \"Download Generated Dataset\" link appears. This system is database-mediated, ensuring the correct version and file format are provided. Purpose: Review and improve generated datasets</p> <p>Features: - Dataset Review: Paginated view of generated entries - Inline Editing: Modify questions, answers, and metadata - Quality Filtering: Filter by quality scores and categories - Bulk Operations: Apply changes to multiple entries - Feedback System: Provide feedback for AI improvement - Regeneration: Regenerate specific entries with modifications</p> <p>Refinement Capabilities: - Entry Modification: Edit content and metadata - Category Updates: Reassign categorization - Quality Assessment: Rate and filter entries - Batch Processing: Apply changes across multiple entries - Export Options: Download refined datasets</p>"},{"location":"userGuide/datasetgengui/#interactive-dataset-refinement","title":"Interactive Dataset Refinement","text":""},{"location":"userGuide/datasetgengui/#post-generation-editing","title":"Post-Generation Editing","text":"<p>Location: Dataset Management \u2192 Refinement Interface Purpose: Manually improve generated datasets through interactive editing</p> <p>Features: - Entry Browser: Paginated view of all dataset entries - Inline Editing: Direct editing of questions, answers, and metadata - Bulk Operations: Apply changes to multiple entries simultaneously - Quality Filtering: Filter entries by quality score or category - Feedback Submission: Provide feedback for AI learning</p> <p>Editing Capabilities: - Question Refinement: Improve clarity and specificity - Answer Enhancement: Correct or expand answer content - Category Assignment: Update or correct categorization - Metadata Updates: Modify difficulty levels and tags</p>"},{"location":"userGuide/datasetgengui/#quality-dashboard","title":"Quality Dashboard","text":"<p>Location: Quality Dashboard Purpose: Visualize and analyze dataset quality metrics</p> <p>Dashboard Components: - Overall Quality Score: Aggregate quality metric with trend - Quality Distribution: Charts showing high/medium/low quality entries - Diversity Metrics: Lexical and semantic diversity visualizations - Bias Detection: Demographic and content bias indicators - Difficulty Analysis: Readability and complexity distributions</p> <p>Interactive Features: - Drill-down: Click charts to see specific entries - Filtering: Filter dashboard by category, quality score, etc. - Export: Download quality reports in various formats - Alerts: Notifications for quality threshold violations</p>"},{"location":"userGuide/datasetgengui/#benchmarking-visualization","title":"Benchmarking Visualization","text":""},{"location":"userGuide/datasetgengui/#performance-analytics","title":"Performance Analytics","text":"<p>Location: Benchmarking Dashboard Purpose: Analyze AI model performance on generated datasets</p> <p>Visualization Types: - Performance Charts: Accuracy, F1, BLEU scores by model - Comparison Tables: Side-by-side model performance - Trend Analysis: Performance changes over time - Benchmark Suite Results: GLUE, SuperGLUE, MMLU, Medical results</p> <p>Interactive Features: - Model Filtering: Compare specific model combinations - Metric Selection: Choose which metrics to display - Historical Tracking: View performance trends over dataset versions - Export Capabilities: Download performance reports</p>"},{"location":"userGuide/datasetgengui/#settings-and-configuration","title":"Settings and Configuration","text":""},{"location":"userGuide/datasetgengui/#api-key-management","title":"API Key Management","text":"<p>Location: Settings \u2192 API Keys Purpose: Configure AI model API keys for dataset generation</p> <p>Key Management: - Gemini API Key: Google AI Studio key configuration - Grok API Key: xAI API key setup - HuggingFace Token: For HuggingFace model access - Status Indicators: Real-time validation of key functionality</p> <p>Security Features: - Encrypted Storage: Secure key storage and transmission - Validation Testing: Automatic key validation on save - Access Logging: Audit trail for key usage</p>"},{"location":"userGuide/datasetgengui/#generation-preferences","title":"Generation Preferences","text":"<p>Location: Settings \u2192 Dataset Generation Purpose: Set default preferences for dataset generation</p> <p>Default Settings: - Preferred Models: Set default AI models for each task - Quality Thresholds: Default quality requirements - Output Preferences: Default formats and directories - Concurrency Limits: Default parallel processing limits</p>"},{"location":"userGuide/datasetgengui/#job-monitoring-and-management","title":"Job Monitoring and Management","text":""},{"location":"userGuide/datasetgengui/#real-time-job-tracking","title":"Real-time Job Tracking","text":"<p>Location: Sidebar \u2192 Job Queue Purpose: Monitor active and completed dataset generation jobs</p> <p>Job Queue Features: - Active Jobs: Currently running generation jobs - Job Progress: Real-time progress bars and status - Queue Statistics: Pending, running, completed job counts - Job Controls: Cancel or restart jobs directly from sidebar - Persistent Jobs: Dataset generation jobs are stored in a persistent database. Tracking remains reliable across browser refreshes, server restarts, and multi-worker environments, eliminating \"404 Not Found\" status errors.</p>"},{"location":"userGuide/datasetgengui/#job-history-and-details","title":"Job History and Details","text":"<p>Location: Job Management Page Purpose: Comprehensive job history and detailed status information</p> <p>Job Management: - Job History: Complete list of past generation jobs (persisted across sessions) - Detailed Status: Step-by-step job execution details - Performance Metrics: CPU, memory, and timing statistics - Error Logs: Detailed error information for failed jobs - Retry Options: Restart failed jobs with modified parameters - Database Reliability: Job status and progress are maintained even if the server restarts during processing</p>"},{"location":"userGuide/datasetgengui/#best-practices-for-gui-usage","title":"Best Practices for GUI Usage","text":""},{"location":"userGuide/datasetgengui/#project-preparation","title":"Project Preparation","text":"<ol> <li>Document Upload: Ensure all documents are uploaded and processed</li> <li>Chunking Completion: Verify chunking has completed successfully</li> <li>Taxonomy Setup: Create or select appropriate taxonomy</li> <li>API Key Configuration: Verify all required API keys are configured</li> </ol>"},{"location":"userGuide/datasetgengui/#model-selection-strategy","title":"Model Selection Strategy","text":"<ol> <li>Content Analysis: Choose models based on document complexity</li> <li>Cost Consideration: Balance quality needs with API costs</li> <li>Local vs Cloud: Use Ollama for development, cloud models for production</li> <li>Testing: Start with small datasets to validate model performance</li> </ol>"},{"location":"userGuide/datasetgengui/#plugin-format-usage","title":"Plugin Format Usage","text":"<ol> <li>Format Discovery: Plugin-provided formats appear automatically in the format dropdown after plugin installation</li> <li>Dual Output: Plugin formats generate both standard JSON and custom format files for reliability</li> <li>Format Compatibility: Verify plugin formats meet your target system's requirements (e.g., Anki import specifications)</li> <li>Plugin Management: Uninstall unused plugins to clean up format options in the GUI</li> </ol>"},{"location":"userGuide/datasetgengui/#quality-assurance-workflow","title":"Quality Assurance Workflow","text":"<ol> <li>Enable Quality Analysis: Always enable for production datasets</li> <li>Set Appropriate Thresholds: Balance quality with generation volume</li> <li>Review Quality Reports: Use dashboards to identify improvement areas</li> <li>Iterative Refinement: Use refinement interface for continuous improvement</li> </ol>"},{"location":"userGuide/datasetgengui/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Concurrency Tuning: Adjust based on available resources and API limits</li> <li>Chunk Size Optimization: Balance chunk size with generation quality</li> <li>Monitoring: Use job monitoring to identify bottlenecks</li> <li>Resource Planning: Monitor API usage and costs</li> </ol>"},{"location":"userGuide/datasetgengui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/datasetgengui/#common-issues","title":"Common Issues","text":"<p>\"No chunks found for project\" - Solution: Ensure documents have been uploaded and processed through the document pipeline - Check: Document status in project view should show \"processed\"</p> <p>\"API key not configured\" - Solution: Configure required API keys in Settings \u2192 API Keys - Check: Status indicators should show green for configured keys</p> <p>\"Quality threshold not met\" - Solution: Lower quality threshold or improve source document quality - Alternative: Disable quality analysis for initial dataset generation</p> <p>\"Job stuck in pending\" - Solution: Dataset generation jobs are now database-backed and persist across server restarts. If a job appears stuck, it may be processing or waiting for resources. - Check: Use Job Management page to view detailed status and error logs</p>"},{"location":"userGuide/datasetgengui/#performance-issues","title":"Performance Issues","text":"<p>Slow Generation - Increase concurrency setting (if API limits allow) - Use more efficient models (Ollama for local processing) - Reduce datasets per chunk</p> <p>Memory Issues - Reduce concurrency setting - Process smaller batches - Monitor system resources in Job Management</p> <p>API Rate Limits - Reduce concurrency to stay within API limits - Implement delays between requests - Monitor API usage in settings</p>"},{"location":"userGuide/datasetgengui/#integration-with-cli","title":"Integration with CLI","text":"<p>The GUI provides seamless integration with CLI tools for advanced workflows:</p> <pre><code># Generate dataset via GUI, then analyze quality via CLI\ncompileo analyze-quality dataset_from_gui.jsonl --format markdown --output quality_report.md\n\n# Use CLI for bulk operations after GUI refinement\ncompileo dataset-version increment-version --project-id 1 --dataset-name refined_dataset --version-type minor\n</code></pre> <p>The GUI offers an intuitive, visual approach to dataset generation while maintaining full compatibility with CLI tools for advanced users and automation scenarios.</p>"},{"location":"userGuide/datasetqualapi/","title":"Dataset Quality API in Compileo","text":""},{"location":"userGuide/datasetqualapi/#overview","title":"Overview","text":"<p>The Compileo Dataset Quality API provides comprehensive quality assessment capabilities for AI training datasets. It evaluates datasets across multiple quality dimensions including diversity, bias detection, difficulty assessment, and consistency checking.</p>"},{"location":"userGuide/datasetqualapi/#base-url-apiv1quality","title":"Base URL: <code>/api/v1/quality</code>","text":""},{"location":"userGuide/datasetqualapi/#1-analyze-dataset-quality","title":"1. Analyze Dataset Quality","text":"<p>Endpoint: <code>POST /analyze</code></p> <p>Description: Performs comprehensive quality analysis on a dataset using configured metrics.</p> <p>Request Body: <pre><code>{\n  \"dataset\": [\n    {\n      \"question\": \"What is the capital of France?\",\n      \"answer\": \"Paris\",\n      \"metadata\": {\n        \"difficulty\": \"easy\",\n        \"topic\": \"geography\"\n      }\n    }\n  ],\n  \"config\": {\n    \"enabled_metrics\": [\"diversity\", \"bias\", \"difficulty\", \"consistency\"],\n    \"thresholds\": {\n      \"diversity\": 0.7,\n      \"bias\": 0.8,\n      \"difficulty\": 0.6,\n      \"consistency\": 0.9\n    }\n  },\n  \"quality_model\": \"gemini\"\n}\n</code></pre></p> <p>Parameters: - <code>dataset</code> (array, required): Array of dataset items - <code>config</code> (object, optional): Quality analysis configuration   - <code>enabled_metrics</code> (array): Metrics to run   - <code>thresholds</code> (object): Custom thresholds per metric - <code>quality_model</code> (string, optional): AI model for quality analysis (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>openai</code>) (default: <code>gemini</code>)</p> <p>Success Response (200 OK): <pre><code>{\n  \"enabled\": true,\n  \"dataset_size\": 100,\n  \"metrics_run\": [\"diversity\", \"bias\", \"difficulty\", \"consistency\"],\n  \"results\": {\n    \"diversity\": {\n      \"name\": \"diversity\",\n      \"score\": 0.85,\n      \"threshold\": 0.7,\n      \"passed\": true,\n      \"details\": {\n        \"lexical_diversity\": 0.78,\n        \"semantic_diversity\": 0.92,\n        \"topic_coverage\": 0.85\n      }\n    }\n  },\n  \"summary\": {\n    \"overall_score\": 0.82,\n    \"passed\": true,\n    \"passed_metrics\": 4,\n    \"failed_metrics\": 0,\n    \"total_metrics\": 4,\n    \"issues\": []\n  }\n}\n</code></pre></p>"},{"location":"userGuide/datasetqualapi/#2-get-quality-metrics","title":"2. Get Quality Metrics","text":"<p>Endpoint: <code>GET /metrics</code></p> <p>Description: Retrieves information about available quality metrics and their configurations.</p> <p>Success Response (200 OK): <pre><code>{\n  \"available_metrics\": [\n    {\n      \"name\": \"diversity\",\n      \"description\": \"Evaluates lexical and semantic diversity\",\n      \"default_threshold\": 0.7,\n      \"enabled\": true\n    },\n    {\n      \"name\": \"bias\",\n      \"description\": \"Detects demographic and content bias\",\n      \"default_threshold\": 0.8,\n      \"enabled\": true\n    },\n    {\n      \"name\": \"difficulty\",\n      \"description\": \"Assesses question/answer complexity\",\n      \"default_threshold\": 0.6,\n      \"enabled\": true\n    },\n    {\n      \"name\": \"consistency\",\n      \"description\": \"Checks factual and logical consistency\",\n      \"default_threshold\": 0.9,\n      \"enabled\": true\n    }\n  ],\n  \"default_config\": {\n    \"enabled\": true,\n    \"fail_on_any_failure\": false,\n    \"output_format\": \"json\"\n  }\n}\n</code></pre></p>"},{"location":"userGuide/datasetqualapi/#3-validate-dataset","title":"3. Validate Dataset","text":"<p>Endpoint: <code>POST /validate</code></p> <p>Description: Quick validation check to ensure dataset format and basic quality requirements.</p> <p>Request Body: <pre><code>{\n  \"dataset\": [\n    {\n      \"question\": \"Sample question?\",\n      \"answer\": \"Sample answer\",\n      \"metadata\": {}\n    }\n  ],\n  \"strict_mode\": false\n}\n</code></pre></p> <p>Success Response (200 OK): <pre><code>{\n  \"valid\": true,\n  \"issues\": [],\n  \"warnings\": [\n    \"Dataset size is small (1 items). Consider larger datasets for reliable analysis.\"\n  ],\n  \"recommendations\": [\n    \"Add more diverse examples\",\n    \"Include metadata for better analysis\"\n  ]\n}\n</code></pre></p>"},{"location":"userGuide/datasetqualapi/#quality-metrics-details","title":"Quality Metrics Details","text":""},{"location":"userGuide/datasetqualapi/#diversity-metric","title":"Diversity Metric","text":"<p>Evaluates content variety and coverage: - Lexical Diversity: Vocabulary richness and variety - Semantic Diversity: Meaning and concept coverage - Topic Coverage: Subject matter distribution</p>"},{"location":"userGuide/datasetqualapi/#bias-metric","title":"Bias Metric","text":"<p>Detects potential biases in content: - Demographic Bias: Gender, ethnicity, age representation - Content Bias: Topic or perspective imbalance - Language Bias: Formal/informal tone distribution</p>"},{"location":"userGuide/datasetqualapi/#difficulty-metric","title":"Difficulty Metric","text":"<p>Assesses complexity levels: - Reading Level: Text complexity analysis - Cognitive Load: Reasoning requirements - Domain Knowledge: Required expertise level</p>"},{"location":"userGuide/datasetqualapi/#consistency-metric","title":"Consistency Metric","text":"<p>Validates logical and factual coherence: - Factual Consistency: Accuracy verification - Logical Consistency: Reasoning validation - Format Consistency: Structure uniformity</p>"},{"location":"userGuide/datasetqualapi/#configuration-options","title":"Configuration Options","text":""},{"location":"userGuide/datasetqualapi/#metric-thresholds","title":"Metric Thresholds","text":"<pre><code>{\n  \"diversity\": {\n    \"threshold\": 0.7,\n    \"min_lexical_diversity\": 0.6,\n    \"min_semantic_diversity\": 0.7\n  },\n  \"bias\": {\n    \"threshold\": 0.8,\n    \"demographic_keywords\": [\"gender\", \"ethnicity\", \"age\"]\n  },\n  \"difficulty\": {\n    \"threshold\": 0.6,\n    \"target_difficulty\": \"intermediate\"\n  },\n  \"consistency\": {\n    \"threshold\": 0.9,\n    \"check_factual_consistency\": true\n  }\n}\n</code></pre>"},{"location":"userGuide/datasetqualapi/#analysis-settings","title":"Analysis Settings","text":"<ul> <li>fail_on_any_failure: Stop on first failed metric</li> <li>output_format: json, text, or markdown</li> <li>detailed_reporting: Include per-item analysis</li> </ul>"},{"location":"userGuide/datasetqualapi/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/datasetqualapi/#common-error-responses","title":"Common Error Responses","text":"<p>400 Bad Request: <pre><code>{\n  \"detail\": \"Invalid dataset format. Expected array of objects with question/answer fields.\"\n}\n</code></pre></p> <p>422 Unprocessable Entity: <pre><code>{\n  \"detail\": \"Dataset too small for reliable analysis. Minimum 10 items required.\"\n}\n</code></pre></p> <p>500 Internal Server Error: <pre><code>{\n  \"detail\": \"Quality analysis failed due to metric execution error\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetqualapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/datasetqualapi/#dataset-preparation","title":"Dataset Preparation","text":"<ul> <li>Ensure consistent question/answer format</li> <li>Include relevant metadata for better analysis</li> <li>Use diverse, representative samples</li> <li>Validate data quality before analysis</li> </ul>"},{"location":"userGuide/datasetqualapi/#metric-selection","title":"Metric Selection","text":"<ul> <li>Enable all metrics for comprehensive analysis</li> <li>Adjust thresholds based on use case requirements</li> <li>Consider domain-specific quality requirements</li> </ul>"},{"location":"userGuide/datasetqualapi/#result-interpretation","title":"Result Interpretation","text":"<ul> <li>Review individual metric scores and details</li> <li>Address failed metrics before dataset use</li> <li>Use summary scores for quick quality assessment</li> <li>Consider metric weights for custom scoring</li> </ul>"},{"location":"userGuide/datasetqualapi/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Analyze large datasets in batches</li> <li>Cache results for repeated analysis</li> <li>Use appropriate metric subsets for quick checks</li> </ul>"},{"location":"userGuide/datasetqualgui/","title":"Dataset Quality Analysis in Compileo GUI","text":""},{"location":"userGuide/datasetqualgui/#overview","title":"Overview","text":"<p>The Compileo GUI provides comprehensive dataset quality analysis capabilities through an intuitive web interface. The quality analysis evaluates datasets across multiple dimensions including diversity, bias detection, difficulty assessment, and consistency validation.</p>"},{"location":"userGuide/datasetqualgui/#accessing-quality-analysis","title":"Accessing Quality Analysis","text":"<ol> <li>Navigate to the Application: Open Compileo in your web browser</li> <li>Select Quality Analysis: Click on \"\ud83d\udcca Quality Metrics\" in the sidebar or main navigation</li> <li>Choose Analysis Type: Select from Analysis, Metrics Dashboard, or History tabs</li> </ol>"},{"location":"userGuide/datasetqualgui/#interface-components","title":"Interface Components","text":""},{"location":"userGuide/datasetqualgui/#analysis-tab","title":"Analysis Tab","text":"<p>The main analysis interface for running quality assessments on datasets.</p>"},{"location":"userGuide/datasetqualgui/#dataset-selection","title":"Dataset Selection","text":"<ul> <li>Available Datasets: Dropdown showing all JSONL dataset files in the test_outputs directory</li> <li>Dataset Information: Displays dataset name and number of entries</li> <li>File Format: Supports JSONL format with question/answer structure</li> </ul>"},{"location":"userGuide/datasetqualgui/#analysis-configuration","title":"Analysis Configuration","text":"<p>Basic Settings: - Quality Metrics Selection:   - Diversity Analysis: Evaluates lexical and semantic variety   - Bias Detection: Identifies potential demographic and content biases   - Difficulty Assessment: Measures question complexity and readability - Overall Quality Threshold: Minimum passing score (0.0-1.0, default 0.7) - Output Format: JSON, HTML, or PDF report formats</p> <p>Advanced Settings: - Diversity Threshold: Minimum diversity score required - Bias Threshold: Maximum acceptable bias score - Target Difficulty: Desired difficulty level for the dataset</p>"},{"location":"userGuide/datasetqualgui/#analysis-execution","title":"Analysis Execution","text":"<ol> <li>Configure Settings: Select metrics and adjust thresholds</li> <li>Start Analysis: Click \"\ud83d\ude80 Start Quality Analysis\"</li> <li>Monitor Progress: View real-time status updates</li> <li>Review Results: Access completed analysis in dashboard</li> </ol>"},{"location":"userGuide/datasetqualgui/#metrics-dashboard-tab","title":"Metrics Dashboard Tab","text":"<p>Interactive visualization and detailed breakdown of quality analysis results.</p>"},{"location":"userGuide/datasetqualgui/#overall-summary-metrics","title":"Overall Summary Metrics","text":"<ul> <li>Overall Quality Score: Weighted average across all enabled metrics</li> <li>Dataset Size: Total number of entries analyzed</li> <li>Passed/Failed Metrics: Count of metrics meeting thresholds</li> <li>Failed Metrics Count: Number of metrics below threshold</li> </ul>"},{"location":"userGuide/datasetqualgui/#individual-metrics-visualization","title":"Individual Metrics Visualization","text":"<ul> <li>Metrics Bar Chart: Color-coded bars showing scores for each metric</li> <li>Green bars: Passed metrics</li> <li>Red bars: Failed metrics</li> <li>Orange threshold line: Default quality threshold</li> <li>Detailed Results Table: Tabular view with scores, thresholds, and pass/fail status</li> </ul>"},{"location":"userGuide/datasetqualgui/#metric-specific-breakdowns","title":"Metric-Specific Breakdowns","text":"<p>Diversity Analysis: - Lexical Diversity: Vocabulary richness and variety - Semantic Diversity: Concept and meaning coverage - Topic Balance: Subject matter distribution - Radar Chart: Visual representation of diversity profile</p> <p>Bias Detection: - Overall Bias Score: Composite bias measurement - Bias Indicators: Breakdown by demographic categories - Content Balance: Topic and perspective distribution</p> <p>Difficulty Assessment: - Average Difficulty: Mean complexity score - Complexity Score: Cognitive load assessment - Readability Score: Text comprehension metrics - Distribution Chart: Pie chart showing difficulty level distribution</p>"},{"location":"userGuide/datasetqualgui/#history-tab","title":"History Tab","text":"<p>Review past quality analysis runs and track performance over time.</p>"},{"location":"userGuide/datasetqualgui/#analysis-history-table","title":"Analysis History Table","text":"<ul> <li>Job ID: Unique identifier for each analysis run</li> <li>Status: Completion status (completed, failed, running)</li> <li>Summary Scores: Overall quality metrics</li> <li>Completion Timestamp: When analysis finished</li> </ul>"},{"location":"userGuide/datasetqualgui/#summary-statistics","title":"Summary Statistics","text":"<ul> <li>Total Analyses: Number of quality assessments run</li> <li>Completed Analyses: Successfully finished evaluations</li> <li>Failed Analyses: Analyses that encountered errors</li> </ul>"},{"location":"userGuide/datasetqualgui/#quality-metrics-explained","title":"Quality Metrics Explained","text":""},{"location":"userGuide/datasetqualgui/#diversity-metrics","title":"Diversity Metrics","text":"<p>Evaluates content variety and representation: - Lexical Diversity: Measures vocabulary richness and word variety - Semantic Diversity: Assesses concept coverage and meaning variety - Topic Coverage: Analyzes subject matter distribution and balance</p>"},{"location":"userGuide/datasetqualgui/#bias-detection","title":"Bias Detection","text":"<p>Identifies potential biases in dataset content: - Demographic Bias: Gender, ethnicity, age, and cultural representation - Content Bias: Topic selection and perspective balance - Language Bias: Formal/informal tone and register distribution</p>"},{"location":"userGuide/datasetqualgui/#difficulty-assessment","title":"Difficulty Assessment","text":"<p>Measures question and content complexity: - Reading Level: Text complexity using readability formulas - Cognitive Load: Reasoning and comprehension requirements - Domain Expertise: Required knowledge level for correct answers</p>"},{"location":"userGuide/datasetqualgui/#consistency-validation","title":"Consistency Validation","text":"<p>Ensures logical and factual coherence: - Factual Consistency: Accuracy of information and claims - Logical Consistency: Reasoning validity and coherence - Format Consistency: Structural uniformity across entries</p>"},{"location":"userGuide/datasetqualgui/#analysis-workflow","title":"Analysis Workflow","text":""},{"location":"userGuide/datasetqualgui/#preparing-for-analysis","title":"Preparing for Analysis","text":"<ol> <li>Generate Dataset: Create or upload dataset in JSONL format</li> <li>Review Content: Ensure proper question/answer structure</li> <li>Check Size: Verify sufficient entries for reliable analysis (minimum 10-50 recommended)</li> </ol>"},{"location":"userGuide/datasetqualgui/#running-quality-analysis","title":"Running Quality Analysis","text":"<ol> <li>Select Dataset: Choose from available JSONL files</li> <li>Configure Metrics: Enable desired quality checks</li> <li>Set Thresholds: Adjust passing criteria as needed</li> <li>Execute Analysis: Start background processing</li> <li>Monitor Progress: Track real-time status updates</li> </ol>"},{"location":"userGuide/datasetqualgui/#interpreting-results","title":"Interpreting Results","text":"<ol> <li>Review Overall Score: Check if dataset meets quality requirements</li> <li>Analyze Failed Metrics: Identify specific quality issues</li> <li>Examine Details: Review metric-specific breakdowns</li> <li>Address Issues: Modify dataset based on recommendations</li> </ol>"},{"location":"userGuide/datasetqualgui/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/datasetqualgui/#common-issues","title":"Common Issues","text":"<ul> <li>Dataset Not Found: Ensure JSONL files exist in test_outputs directory</li> <li>Invalid Format: Verify question/answer structure in dataset</li> <li>Analysis Timeout: Large datasets may take time to process</li> <li>Metric Failures: Individual metrics may fail while others succeed</li> </ul>"},{"location":"userGuide/datasetqualgui/#recovery-actions","title":"Recovery Actions","text":"<ul> <li>Restart Analysis: Failed analyses can be restarted</li> <li>Modify Configuration: Adjust thresholds or disable problematic metrics</li> <li>Check Dataset: Validate data format and content</li> <li>Contact Support: For persistent technical issues</li> </ul>"},{"location":"userGuide/datasetqualgui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/datasetqualgui/#dataset-preparation","title":"Dataset Preparation","text":"<ul> <li>Use consistent question/answer formats</li> <li>Include diverse, representative content</li> <li>Add metadata for enhanced analysis</li> <li>Validate data quality before analysis</li> </ul>"},{"location":"userGuide/datasetqualgui/#analysis-configuration_1","title":"Analysis Configuration","text":"<ul> <li>Enable all relevant metrics for comprehensive evaluation</li> <li>Set appropriate thresholds based on use case</li> <li>Consider domain-specific quality requirements</li> <li>Use advanced settings for fine-tuned analysis</li> </ul>"},{"location":"userGuide/datasetqualgui/#result-utilization","title":"Result Utilization","text":"<ul> <li>Address failed metrics before dataset deployment</li> <li>Use detailed breakdowns for targeted improvements</li> <li>Track quality trends across dataset versions</li> <li>Compare quality scores between datasets</li> </ul>"},{"location":"userGuide/datasetqualgui/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Analyze datasets during off-peak hours for large files</li> <li>Use appropriate metric subsets for quick assessments</li> <li>Cache results for repeated analysis of same datasets</li> <li>Monitor system resources during analysis</li> </ul>"},{"location":"userGuide/datasetqualgui/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"userGuide/datasetqualgui/#dataset-generation-workflow","title":"Dataset Generation Workflow","text":"<ol> <li>Generate Dataset: Create training data using dataset generation tools</li> <li>Run Quality Analysis: Evaluate generated content quality</li> <li>Review Results: Identify areas for improvement</li> <li>Iterate Generation: Refine prompts and parameters based on analysis</li> <li>Validate Improvements: Re-run analysis to confirm quality gains</li> </ol>"},{"location":"userGuide/datasetqualgui/#benchmarking-integration","title":"Benchmarking Integration","text":"<ol> <li>Complete Quality Analysis: Ensure dataset meets quality standards</li> <li>Run Benchmarking: Evaluate model performance on quality-assessed data</li> <li>Correlate Results: Compare quality metrics with benchmark performance</li> <li>Optimize Dataset: Use insights to improve both quality and performance</li> </ol>"},{"location":"userGuide/datasetqualgui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/datasetqualgui/#analysis-not-starting","title":"Analysis Not Starting","text":"<ul> <li>Verify dataset file exists and is readable</li> <li>Check JSONL format and content structure</li> <li>Ensure sufficient system resources available</li> </ul>"},{"location":"userGuide/datasetqualgui/#unexpected-results","title":"Unexpected Results","text":"<ul> <li>Review dataset content for consistency issues</li> <li>Check metric thresholds are appropriate for content type</li> <li>Validate analysis configuration settings</li> </ul>"},{"location":"userGuide/datasetqualgui/#performance-issues","title":"Performance Issues","text":"<ul> <li>Reduce dataset size for faster analysis</li> <li>Disable unnecessary metrics for quick checks</li> <li>Run analysis during low-usage periods</li> </ul> <p>This quality analysis interface provides comprehensive evaluation capabilities to ensure datasets meet the highest standards for AI training and evaluation.</p>"},{"location":"userGuide/datasetsapi/","title":"Datasets Module API Usage Guide","text":"<p>The Compileo Datasets API provides comprehensive REST endpoints for dataset generation, management, and quality assessment. This API enables users to create training datasets from processed documents using various AI models and quality control mechanisms.</p>"},{"location":"userGuide/datasetsapi/#base-url-apiv1datasets","title":"Base URL: <code>/api/v1/datasets</code>","text":""},{"location":"userGuide/datasetsapi/#dataset-generation","title":"Dataset Generation","text":""},{"location":"userGuide/datasetsapi/#post-generate","title":"POST <code>/generate</code>","text":"<p>Generate a dataset from processed document chunks using AI models.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"prompt_name\": \"qa_pairs\",\n    \"custom_prompt\": null,\n    \"generation_mode\": \"default\",\n    \"format_type\": \"jsonl\",\n    \"concurrency\": 1,\n    \"include_evaluation_sets\": false,\n    \"taxonomy_project\": null,\n    \"taxonomy_name\": null,\n    \"output_dir\": \".\",\n    \"analyze_quality\": true,\n    \"quality_threshold\": 0.7,\n    \"enable_versioning\": false,\n    \"dataset_name\": \"medical_qa_dataset\",\n    \"run_benchmarks\": false,\n    \"benchmark_suite\": \"glue\",\n    \"parsing_model\": \"gemini\",\n    \"chunking_model\": \"gemini\",\n    \"classification_model\": \"gemini\",\n    \"datasets_per_chunk\": 3\n  }'\n</code></pre></p> <p>Request Body: - <code>project_id</code>: Project containing processed documents (required) - <code>prompt_name</code>: Name of prompt template to use (default: \"qa_pairs\") - <code>custom_prompt</code>: Custom prompt content (optional) - <code>generation_mode</code>: Generation mode - \"default\", \"question\", \"answer\", \"summarization\" (default: \"default\") - <code>format_type</code>: Output format - \"jsonl\" or \"parquet\" (default: \"jsonl\") - <code>concurrency</code>: Number of concurrent processing threads (default: 1) - <code>include_evaluation_sets</code>: Generate train/validation/test splits (default: false) - <code>taxonomy_project</code>: Project containing taxonomy for categorization (optional) - <code>taxonomy_name</code>: Name of taxonomy to use for categorization (optional) - <code>output_dir</code>: Output directory path (default: \".\") - <code>analyze_quality</code>: Enable quality analysis of generated data (default: true) - <code>quality_threshold</code>: Minimum quality score threshold (default: 0.7) - <code>enable_versioning</code>: Enable dataset versioning (default: false) - <code>dataset_name</code>: Custom name for the dataset (optional) - <code>run_benchmarks</code>: Run benchmark tests on generated dataset (default: false) - <code>benchmark_suite</code>: Benchmark suite to use (default: \"glue\") - <code>parsing_model</code>: AI model for document parsing (default: \"gemini\") - <code>chunking_model</code>: AI model for text chunking (default: \"gemini\") - <code>classification_model</code>: AI model for content classification (default: \"gemini\") - <code>datasets_per_chunk</code>: Maximum datasets to generate per text chunk (default: 3)</p> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"message\": \"Dataset generation started\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#evaluation-dataset-generation","title":"Evaluation Dataset Generation","text":""},{"location":"userGuide/datasetsapi/#post-generate-evaluation","title":"POST <code>/generate-evaluation</code>","text":"<p>Generate comprehensive evaluation datasets with train/validation/test splits.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/generate-evaluation\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"prompt_name\": \"evaluation_qa\",\n    \"format_type\": \"jsonl\",\n    \"concurrency\": 2,\n    \"include_evaluation_sets\": true,\n    \"analyze_quality\": true,\n    \"quality_threshold\": 0.8,\n    \"classification_model\": \"gemini\",\n    \"datasets_per_chunk\": 5\n  }'\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440001\",\n  \"message\": \"Evaluation dataset generation started\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#job-status-monitoring","title":"Job Status Monitoring","text":""},{"location":"userGuide/datasetsapi/#get-generatejob_idstatus","title":"GET <code>/generate/{job_id}/status</code>","text":"<p>Get the status of a dataset generation job.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/datasets/generate/550e8400-e29b-41d4-a716-446655440000/status\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"running\",\n  \"progress\": 65,\n  \"current_step\": \"Processing chunk 45 of 100\",\n  \"estimated_completion\": \"2024-01-21T11:45:00Z\",\n  \"result\": null,\n  \"error\": null\n}\n</code></pre></p> <p>Status Values: - <code>pending</code>: Job queued and waiting to start - <code>running</code>: Job currently executing - <code>completed</code>: Job finished successfully - <code>failed</code>: Job failed with an error</p>"},{"location":"userGuide/datasetsapi/#dataset-retrieval","title":"Dataset Retrieval","text":""},{"location":"userGuide/datasetsapi/#get-dataset_id","title":"GET <code>/{dataset_id}</code>","text":"<p>Get detailed information about a specific dataset.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/datasets/dataset_550e8400-e29b-41d4-a716-446655440000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": \"dataset_550e8400-e29b-41d4-a716-446655440000\",\n  \"name\": \"medical_qa_dataset\",\n  \"entries\": [\n    {\n      \"id\": \"entry_1\",\n      \"question\": \"What are the common symptoms of diabetes?\",\n      \"answer\": \"Common symptoms of diabetes include frequent urination, excessive thirst, unexplained weight loss, increased hunger, fatigue, slow-healing sores, frequent infections, blurred vision, and tingling or numbness in hands or feet.\",\n      \"category\": \"Endocrinology\",\n      \"quality_score\": 0.92,\n      \"difficulty\": \"intermediate\",\n      \"source_chunk\": \"chunk_001.md\",\n      \"metadata\": {\n        \"model\": \"gemini\",\n        \"generation_time\": \"2024-01-21T10:35:22Z\",\n        \"chunk_length\": 1250\n      }\n    }\n  ],\n  \"total_entries\": 150,\n  \"created_at\": \"2024-01-21T10:30:00Z\",\n  \"format_type\": \"jsonl\",\n  \"quality_summary\": {\n    \"average_score\": 0.87,\n    \"distribution\": {\n      \"high\": 120,\n      \"medium\": 25,\n      \"low\": 5\n    },\n    \"categories\": {\n      \"Endocrinology\": 45,\n      \"Cardiology\": 38,\n      \"Neurology\": 32,\n      \"Other\": 35\n    }\n  }\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#dataset-entries-management","title":"Dataset Entries Management","text":""},{"location":"userGuide/datasetsapi/#get-dataset_identries","title":"GET <code>/{dataset_id}/entries</code>","text":"<p>Get dataset entries with pagination and filtering.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/datasets/dataset_550e8400-e29b-41d4-a716-446655440000/entries?page=1&amp;per_page=50&amp;filter_quality=0.8&amp;sort_by=quality\"\n</code></pre></p> <p>Query Parameters: - <code>page</code>: Page number (default: 1) - <code>per_page</code>: Entries per page (default: 50) - <code>filter_quality</code>: Minimum quality score filter (optional) - <code>sort_by</code>: Sort field - \"quality\", \"category\", \"difficulty\" (default: \"quality\")</p> <p>Response: <pre><code>{\n  \"entries\": [\n    {\n      \"id\": \"entry_1\",\n      \"question\": \"What are the common symptoms of diabetes?\",\n      \"answer\": \"Common symptoms of diabetes include frequent urination...\",\n      \"category\": \"Endocrinology\",\n      \"quality_score\": 0.92,\n      \"difficulty\": \"intermediate\",\n      \"source_chunk\": \"chunk_001.md\",\n      \"metadata\": {\n        \"model\": \"gemini\",\n        \"generation_time\": \"2024-01-21T10:35:22Z\"\n      }\n    }\n  ],\n  \"total\": 150,\n  \"page\": 1,\n  \"per_page\": 50,\n  \"quality_summary\": {\n    \"average_score\": 0.87,\n    \"distribution\": {\n      \"high\": 120,\n      \"medium\": 25,\n      \"low\": 5\n    }\n  }\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#put-dataset_identriesentry_id","title":"PUT <code>/{dataset_id}/entries/{entry_id}</code>","text":"<p>Update a specific dataset entry.</p> <p>Request: <pre><code>curl -X PUT \"http://localhost:8000/api/v1/datasets/dataset_550e8400-e29b-41d4-a716-446655440000/entries/entry_1\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"question\": \"What are the most common symptoms of diabetes mellitus?\",\n    \"answer\": \"The most common symptoms include polydipsia (excessive thirst), polyuria (frequent urination), unexplained weight loss, and fatigue.\",\n    \"category\": \"Endocrinology\",\n    \"feedback\": \"Improved medical terminology\"\n  }'\n</code></pre></p> <p>Request Body: - <code>question</code>: Updated question text (optional) - <code>answer</code>: Updated answer text (optional) - <code>category</code>: Updated category (optional) - <code>feedback</code>: User feedback/comments (optional)</p> <p>Response: <pre><code>{\n  \"message\": \"Entry entry_1 updated successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#dataset-enhancement","title":"Dataset Enhancement","text":""},{"location":"userGuide/datasetsapi/#post-dataset_idfeedback","title":"POST <code>/{dataset_id}/feedback</code>","text":"<p>Submit feedback for multiple dataset entries.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/dataset_550e8400-e29b-41d4-a716-446655440000/feedback\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"entry_ids\": [\"entry_1\", \"entry_2\", \"entry_3\"],\n    \"feedback_type\": \"bulk_edit\",\n    \"comments\": \"Improved medical accuracy and terminology\",\n    \"rating\": 4\n  }'\n</code></pre></p> <p>Request Body: - <code>entry_ids</code>: Array of entry IDs to provide feedback for (required) - <code>feedback_type</code>: Type of feedback - \"bulk_edit\", \"quality_issue\", \"content_error\" (default: \"bulk_edit\") - <code>comments</code>: Feedback comments (optional) - <code>rating</code>: Quality rating 1-5 (optional)</p> <p>Response: <pre><code>{\n  \"message\": \"Feedback submitted for 3 entries\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#post-dataset_idregenerate","title":"POST <code>/{dataset_id}/regenerate</code>","text":"<p>Regenerate specific dataset entries with improved configuration.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/dataset_550e8400-e29b-41d4-a716-446655440000/regenerate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"entry_ids\": [\"entry_1\", \"entry_5\", \"entry_12\"],\n    \"regeneration_config\": {\n      \"model\": \"gemini-2.5-flash\",\n      \"temperature\": 0.7,\n      \"max_tokens\": 500,\n      \"improve_quality\": true\n    }\n  }'\n</code></pre></p> <p>Request Body: - <code>entry_ids</code>: Array of entry IDs to regenerate (required) - <code>regeneration_config</code>: Configuration for regeneration (required)   - <code>model</code>: AI model to use   - <code>temperature</code>: Generation temperature   - <code>max_tokens</code>: Maximum token length   - <code>improve_quality</code>: Enable quality improvements</p> <p>Response: <pre><code>{\n  \"job_id\": \"regenerate_550e8400-e29b-41d4-a716-446655440002\",\n  \"message\": \"Regeneration started for 3 entries\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#dataset-export","title":"Dataset Export","text":""},{"location":"userGuide/datasetsapi/#get-dataset_iddownload","title":"GET <code>/{dataset_id}/download</code>","text":"<p>Download the complete dataset file. The <code>dataset_id</code> should be the unique database ID (UUID) returned in the generation result.</p> <p>Features: - Database Mediation: Resolves physical file paths using the database for high integrity. - Automatic Zipping: If the dataset consists of multiple batches, they are automatically archived into a single ZIP file. - Extension Detection: Intelligently applies the correct file extension based on metadata.</p> <p>Request: <pre><code>curl -O \"http://localhost:8000/api/v1/datasets/f0efd610-ffdb-4d38-8506-835320f7270d/download\"\n</code></pre></p> <p>Response: Returns the dataset file (or ZIP archive) with appropriate headers for browser download.</p>"},{"location":"userGuide/datasetsapi/#configuration-endpoints","title":"Configuration Endpoints","text":""},{"location":"userGuide/datasetsapi/#get-confighigh-level-prompts","title":"GET <code>/config/high-level-prompts</code>","text":"<p>Retrieve default options for high-level prompt configuration.</p> <p>Response: <pre><code>{\n  \"audience_defaults\": [\"healthcare professionals\", \"students\", ...],\n  \"purpose_defaults\": [\"patient education\", \"research\", ...],\n  \"complexity_options\": [\"beginner\", \"intermediate\", \"advanced\", \"expert\"],\n  \"domain_defaults\": [\"general\", ...]\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#get-configdefault-prompts","title":"GET <code>/config/default-prompts</code>","text":"<p>Retrieve default prompt templates for each generation mode.</p> <p>Response: <pre><code>{\n  \"prompts\": {\n    \"instruction following\": \"...\",\n    \"question and answer\": \"...\",\n    \"summarization\": \"...\"\n  }\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#dataset-parameters-management","title":"Dataset Parameters Management","text":""},{"location":"userGuide/datasetsapi/#post-parameters","title":"POST <code>/parameters</code>","text":"<p>Save dataset generation parameters for a project.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/parameters\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"purpose\": \"medical_qa_training\",\n    \"audience\": \"medical_students\",\n    \"extraction_rules\": \"clinical_guidelines\",\n    \"dataset_format\": \"jsonl\",\n    \"question_style\": \"factual\",\n    \"answer_style\": \"comprehensive\",\n    \"negativity_ratio\": 0.1,\n    \"data_augmentation\": \"synonym_replacement\",\n    \"custom_audience\": \"3rd_year_medical_students\",\n    \"custom_purpose\": \"board_exam_preparation\",\n    \"complexity_level\": \"intermediate\",\n    \"domain\": \"medicine\"\n  }'\n</code></pre></p> <p>Request Body: - <code>project_id</code>: Project ID (required) - <code>purpose</code>: Dataset purpose (required) - <code>audience</code>: Target audience (required) - <code>extraction_rules</code>: Rules for data extraction (default: \"default\") - <code>dataset_format</code>: Output format (required) - <code>question_style</code>: Question style - \"factual\", \"analytical\", \"case_based\" (default: \"factual\") - <code>answer_style</code>: Answer style - \"concise\", \"comprehensive\", \"step_by_step\" (default: \"comprehensive\") - <code>negativity_ratio</code>: Ratio of negative examples (default: 0.1) - <code>data_augmentation</code>: Augmentation techniques (default: \"none\") - <code>custom_audience</code>: Custom audience description (optional) - <code>custom_purpose</code>: Custom purpose description (optional) - <code>complexity_level</code>: Complexity level - \"basic\", \"intermediate\", \"advanced\" (default: \"intermediate\") - <code>domain</code>: Content domain (default: \"general\")</p> <p>Response: <pre><code>{\n  \"message\": \"Dataset parameters saved successfully\",\n  \"parameter_id\": 101,\n  \"project_id\": 1\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#get-parametersproject_id","title":"GET <code>/parameters/{project_id}</code>","text":"<p>Get the latest dataset generation parameters for a project.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/datasets/parameters/b357e573-89a5-4b40-8e1b-4c075a1835a6\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"parameters\": {\n    \"id\": \"uuid-1\",\n    \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n    \"purpose\": \"fine-tuning\",\n    \"audience\": \"researchers\",\n    \"dataset_format\": \"jsonl\",\n    \"complexity_level\": \"advanced\",\n    \"domain\": \"medicine\",\n    \"created_at\": \"2024-01-21T12:00:00Z\"\n  }\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/datasetsapi/#1-dataset-generation-strategy","title":"1. Dataset Generation Strategy","text":"<p>Model Selection: <pre><code># Choose appropriate models based on use case\ngeneration_configs = {\n    \"high_quality\": {\n        \"classification_model\": \"gemini-2.5-flash\",\n        \"concurrency\": 1,\n        \"datasets_per_chunk\": 2\n    },\n    \"high_volume\": {\n        \"classification_model\": \"ollama\",\n        \"concurrency\": 4,\n        \"datasets_per_chunk\": 5\n    },\n    \"balanced\": {\n        \"classification_model\": \"gemini\",\n        \"concurrency\": 2,\n        \"datasets_per_chunk\": 3\n    }\n}\n</code></pre></p> <p>Quality Thresholds: <pre><code># Set appropriate quality thresholds\nquality_configs = {\n    \"research\": {\"quality_threshold\": 0.9, \"analyze_quality\": True},\n    \"training\": {\"quality_threshold\": 0.8, \"analyze_quality\": True},\n    \"prototyping\": {\"quality_threshold\": 0.6, \"analyze_quality\": False}\n}\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#2-job-monitoring-and-management","title":"2. Job Monitoring and Management","text":"<p>Progress Tracking: <pre><code>import requests\nimport time\n\ndef monitor_dataset_generation(job_id):\n    \"\"\"Monitor dataset generation with progress updates.\"\"\"\n    while True:\n        response = requests.get(f'http://localhost:8000/api/v1/datasets/generate/{job_id}/status')\n        status = response.json()\n\n        print(f\"Progress: {status['progress']}% - {status['current_step']}\")\n\n        if status['status'] == 'completed':\n            print(\"Dataset generation completed!\")\n            return status['result']\n        elif status['status'] == 'failed':\n            print(f\"Generation failed: {status['error']}\")\n            return None\n\n        time.sleep(10)  # Check every 10 seconds\n</code></pre></p> <p>Batch Processing: <pre><code>def generate_multiple_datasets(project_ids, config):\n    \"\"\"Generate datasets for multiple projects.\"\"\"\n    jobs = {}\n\n    # Start all jobs\n    for project_id in project_ids:\n        job_config = config.copy()\n        job_config['project_id'] = project_id\n\n        response = requests.post('http://localhost:8000/api/v1/datasets/generate', json=job_config)\n        jobs[project_id] = response.json()['job_id']\n\n    # Monitor all jobs\n    results = {}\n    for project_id, job_id in jobs.items():\n        result = monitor_dataset_generation(job_id)\n        results[project_id] = result\n\n    return results\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#3-quality-assessment-and-improvement","title":"3. Quality Assessment and Improvement","text":"<p>Quality Analysis: <pre><code>def analyze_dataset_quality(dataset_id):\n    \"\"\"Analyze the quality distribution of a dataset.\"\"\"\n    response = requests.get(f'http://localhost:8000/api/v1/datasets/{dataset_id}')\n    dataset = response.json()\n\n    quality_summary = dataset['quality_summary']\n\n    print(f\"Average Quality Score: {quality_summary['average_score']:.2f}\")\n    print(\"Quality Distribution:\")\n    for level, count in quality_summary['distribution'].items():\n        print(f\"  {level.capitalize()}: {count}\")\n\n    print(\"Category Distribution:\")\n    for category, count in quality_summary['categories'].items():\n        print(f\"  {category}: {count}\")\n\n    return quality_summary\n</code></pre></p> <p>Targeted Improvements: <pre><code>def improve_low_quality_entries(dataset_id, min_quality=0.7):\n    \"\"\"Identify and regenerate low-quality entries.\"\"\"\n    # Get all entries\n    response = requests.get(f'http://localhost:8000/api/v1/datasets/{dataset_id}/entries?per_page=1000')\n    entries = response.json()['entries']\n\n    # Find low-quality entries\n    low_quality = [e for e in entries if e['quality_score'] &lt; min_quality]\n    low_quality_ids = [e['id'] for e in low_quality]\n\n    if low_quality_ids:\n        print(f\"Found {len(low_quality_ids)} low-quality entries to regenerate\")\n\n        # Regenerate low-quality entries\n        regenerate_response = requests.post(\n            f'http://localhost:8000/api/v1/datasets/{dataset_id}/regenerate',\n            json={\n                \"entry_ids\": low_quality_ids,\n                \"regeneration_config\": {\n                    \"model\": \"gemini-2.5-flash\",\n                    \"temperature\": 0.3,  # Lower temperature for higher quality\n                    \"improve_quality\": True\n                }\n            }\n        )\n\n        return regenerate_response.json()\n    else:\n        print(\"No low-quality entries found\")\n        return None\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#4-taxonomy-integration","title":"4. Taxonomy Integration","text":"<p>Taxonomy-Guided Generation: <pre><code>def generate_taxonomy_aware_dataset(project_id, taxonomy_name):\n    \"\"\"Generate dataset with taxonomy-based categorization.\"\"\"\n\n    # First, ensure taxonomy exists\n    taxonomy_response = requests.get('http://localhost:8000/api/v1/taxonomy/')\n    taxonomies = taxonomy_response.json()['taxonomies']\n    taxonomy = next((t for t in taxonomies if t['name'] == taxonomy_name), None)\n\n    if not taxonomy:\n        raise ValueError(f\"Taxonomy '{taxonomy_name}' not found\")\n\n    # Generate dataset with taxonomy\n    dataset_config = {\n        \"project_id\": project_id,\n        \"taxonomy_project\": taxonomy['project_id'],\n        \"taxonomy_name\": taxonomy_name,\n        \"generation_mode\": \"question\",\n        \"analyze_quality\": True,\n        \"datasets_per_chunk\": 3\n    }\n\n    response = requests.post('http://localhost:8000/api/v1/datasets/generate', json=dataset_config)\n    return response.json()\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#5-version-management","title":"5. Version Management","text":"<p>Versioned Datasets: <pre><code>def create_versioned_dataset(project_id, base_name, version=\"1.0.0\"):\n    \"\"\"Create a versioned dataset with proper naming.\"\"\"\n\n    dataset_config = {\n        \"project_id\": project_id,\n        \"dataset_name\": f\"{base_name}_v{version}\",\n        \"enable_versioning\": True,\n        \"generation_mode\": \"default\",\n        \"format_type\": \"jsonl\",\n        \"analyze_quality\": True\n    }\n\n    response = requests.post('http://localhost:8000/api/v1/datasets/generate', json=dataset_config)\n    job_id = response.json()['job_id']\n\n    # Wait for completion\n    result = monitor_dataset_generation(job_id)\n\n    return result\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#6-performance-optimization","title":"6. Performance Optimization","text":"<p>Concurrent Processing: <pre><code>import concurrent.futures\nimport requests\n\ndef batch_generate_datasets(projects, config_template):\n    \"\"\"Generate datasets for multiple projects concurrently.\"\"\"\n\n    def generate_single_dataset(project):\n        config = config_template.copy()\n        config['project_id'] = project['id']\n        config['dataset_name'] = f\"{project['name']}_dataset\"\n\n        response = requests.post('http://localhost:8000/api/v1/datasets/generate', json=config)\n        return {\n            'project': project['name'],\n            'job_id': response.json()['job_id']\n        }\n\n    # Process up to 3 projects concurrently\n    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n        futures = [executor.submit(generate_single_dataset, project) for project in projects]\n        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n\n    return results\n</code></pre></p>"},{"location":"userGuide/datasetsapi/#7-error-handling-and-recovery","title":"7. Error Handling and Recovery","text":"<p>Robust Dataset Operations: <pre><code>def safe_dataset_generation(project_id, config, max_retries=3):\n    \"\"\"Generate dataset with error handling and retries.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            # Start generation\n            response = requests.post('http://localhost:8000/api/v1/datasets/generate', json=config)\n            response.raise_for_status()\n            job_id = response.json()['job_id']\n\n            # Monitor completion\n            result = monitor_dataset_generation(job_id)\n\n            if result and result.get('error'):\n                if attempt &lt; max_retries - 1:\n                    print(f\"Attempt {attempt + 1} failed: {result['error']}. Retrying...\")\n                    time.sleep(5 * (attempt + 1))  # Exponential backoff\n                    continue\n                else:\n                    raise Exception(f\"Dataset generation failed after {max_retries} attempts: {result['error']}\")\n\n            return result\n\n        except requests.exceptions.RequestException as e:\n            if attempt &lt; max_retries - 1:\n                print(f\"Network error on attempt {attempt + 1}: {e}. Retrying...\")\n                time.sleep(5 * (attempt + 1))\n                continue\n            else:\n                raise Exception(f\"Dataset generation failed after {max_retries} attempts due to network errors\")\n\n    return None\n</code></pre></p> <p>This API provides comprehensive dataset generation and management capabilities with support for quality assessment, taxonomy integration, versioning, and performance optimization suitable for both development and production workflows.</p>"},{"location":"userGuide/datasetversioningapi/","title":"Dataset Versioning Module API Usage Guide","text":"<p>The Compileo Dataset Versioning API provides comprehensive version control for datasets, enabling tracking of dataset changes, comparisons between versions, and safe rollbacks when needed.</p>"},{"location":"userGuide/datasetversioningapi/#base-url-apiv1datasetsversions","title":"Base URL: <code>/api/v1/datasets/versions</code>","text":""},{"location":"userGuide/datasetversioningapi/#list-dataset-versions","title":"List Dataset Versions","text":""},{"location":"userGuide/datasetversioningapi/#get","title":"GET <code>/</code>","text":"<p>Get all versions of a specific dataset.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/datasets/versions/?project_id=1&amp;dataset_name=my_dataset\"\n</code></pre></p> <p>Query Parameters: - <code>project_id</code>: Project ID (required) - <code>dataset_name</code>: Dataset name (required) - <code>active_only</code>: Return only active versions (default: true)</p> <p>Response: <pre><code>{\n  \"versions\": [\n    {\n      \"version\": \"1.0.0\",\n      \"total_entries\": 1000,\n      \"is_active\": true,\n      \"created_at\": \"2024-01-15T10:30:00Z\",\n      \"description\": \"Initial dataset version\"\n    },\n    {\n      \"version\": \"1.0.1\",\n      \"total_entries\": 1050,\n      \"is_active\": true,\n      \"created_at\": \"2024-01-16T14:20:00Z\",\n      \"description\": \"Added quality improvements\"\n    }\n  ],\n  \"total\": 2\n}\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#compare-dataset-versions","title":"Compare Dataset Versions","text":""},{"location":"userGuide/datasetversioningapi/#post-compare","title":"POST <code>/compare</code>","text":"<p>Compare two versions of a dataset to see differences.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/versions/compare\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"dataset_name\": \"my_dataset\",\n    \"version1\": \"1.0.0\",\n    \"version2\": \"1.0.1\"\n  }'\n</code></pre></p> <p>Request Body: <pre><code>{\n  \"project_id\": 1,\n  \"dataset_name\": \"my_dataset\",\n  \"version1\": \"1.0.0\",\n  \"version2\": \"1.0.1\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"comparison\": {\n    \"entries_added\": 50,\n    \"entries_removed\": 0,\n    \"entries_modified\": 25,\n    \"total_entries_v1\": 1000,\n    \"total_entries_v2\": 1050,\n    \"quality_score_change\": 0.05,\n    \"categories_added\": [\"cardiology\"],\n    \"categories_removed\": [],\n    \"metadata_changes\": {\n      \"generation_parameters\": \"updated\",\n      \"model_version\": \"improved\"\n    }\n  },\n  \"summary\": \"Version 1.0.1 added 50 entries and improved quality score by 5%\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#rollback-dataset-version","title":"Rollback Dataset Version","text":""},{"location":"userGuide/datasetversioningapi/#post-rollback","title":"POST <code>/rollback</code>","text":"<p>Rollback a dataset to a previous version.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/versions/rollback\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"dataset_name\": \"my_dataset\",\n    \"target_version\": \"1.0.0\"\n  }'\n</code></pre></p> <p>Request Body: <pre><code>{\n  \"project_id\": 1,\n  \"dataset_name\": \"my_dataset\",\n  \"target_version\": \"1.0.0\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Successfully rolled back to version 1.0.0\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#increment-dataset-version","title":"Increment Dataset Version","text":""},{"location":"userGuide/datasetversioningapi/#post-increment","title":"POST <code>/increment</code>","text":"<p>Increment the version number of a dataset (semantic versioning).</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/datasets/versions/increment\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"dataset_name\": \"my_dataset\",\n    \"version_type\": \"patch\",\n    \"description\": \"Fixed minor issues\"\n  }'\n</code></pre></p> <p>Request Body: <pre><code>{\n  \"project_id\": 1,\n  \"dataset_name\": \"my_dataset\",\n  \"version_type\": \"patch\",\n  \"description\": \"Fixed minor issues\"\n}\n</code></pre></p> <p>Version Types: - <code>major</code>: Breaking changes (1.0.0 \u2192 2.0.0) - <code>minor</code>: New features (1.0.0 \u2192 1.1.0) - <code>patch</code>: Bug fixes (1.0.0 \u2192 1.0.1)</p> <p>Response: <pre><code>{\n  \"new_version\": \"1.0.1\",\n  \"version_id\": 123,\n  \"message\": \"Version incremented to 1.0.1\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#get-latest-version","title":"Get Latest Version","text":""},{"location":"userGuide/datasetversioningapi/#get-latest","title":"GET <code>/latest</code>","text":"<p>Get the latest version of a dataset.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/datasets/versions/latest?project_id=1&amp;dataset_name=my_dataset\"\n</code></pre></p> <p>Query Parameters: - <code>project_id</code>: Project ID (required) - <code>dataset_name</code>: Dataset name (required)</p> <p>Response: <pre><code>{\n  \"latest_version\": \"1.0.1\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/datasetversioningapi/#1-version-management-strategy","title":"1. Version Management Strategy","text":"<p>Semantic Versioning: <pre><code># Use semantic versioning for clear change tracking\n# major.minor.patch\n# - major: breaking changes\n# - minor: new features\n# - patch: bug fixes\n\nversion_types = {\n    'major': '1.0.0 \u2192 2.0.0',  # Breaking changes\n    'minor': '1.0.0 \u2192 1.1.0',  # New features\n    'patch': '1.0.0 \u2192 1.0.1'   # Bug fixes\n}\n</code></pre></p> <p>Version Increment Timing: <pre><code>def should_increment_version(change_type):\n    \"\"\"Determine when to increment versions.\"\"\"\n    if change_type == 'breaking_change':\n        return 'major'\n    elif change_type == 'new_feature':\n        return 'minor'\n    elif change_type in ['bug_fix', 'improvement']:\n        return 'patch'\n    else:\n        return 'patch'  # Default\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#2-rollback-safety","title":"2. Rollback Safety","text":"<p>Safe Rollback Process: <pre><code>def safe_rollback(project_id, dataset_name, target_version):\n    \"\"\"Safely rollback with validation.\"\"\"\n    # 1. Create backup of current version\n    backup_version = create_backup(project_id, dataset_name)\n\n    # 2. Validate target version exists and is healthy\n    if not validate_version(project_id, dataset_name, target_version):\n        raise ValueError(f\"Target version {target_version} is invalid\")\n\n    # 3. Perform rollback\n    rollback_result = rollback_to_version(project_id, dataset_name, target_version)\n\n    # 4. Validate rollback success\n    if not validate_rollback(project_id, dataset_name, target_version):\n        # Restore from backup if rollback failed\n        restore_backup(project_id, dataset_name, backup_version)\n        raise RuntimeError(\"Rollback failed, restored from backup\")\n\n    return rollback_result\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#3-version-comparison-and-analysis","title":"3. Version Comparison and Analysis","text":"<p>Automated Version Analysis: <pre><code>def analyze_version_changes(project_id, dataset_name, version1, version2):\n    \"\"\"Analyze changes between versions.\"\"\"\n    comparison = compare_versions(project_id, dataset_name, version1, version2)\n\n    insights = {\n        'growth_rate': calculate_growth_rate(comparison),\n        'quality_trend': analyze_quality_trend(comparison),\n        'category_evolution': track_category_changes(comparison),\n        'recommendations': generate_recommendations(comparison)\n    }\n\n    return insights\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#4-integration-with-dataset-generation","title":"4. Integration with Dataset Generation","text":"<p>Version-Aware Dataset Generation: <pre><code>def generate_dataset_with_versioning(project_id, generation_params):\n    \"\"\"Generate dataset with automatic versioning.\"\"\"\n    # Generate new dataset\n    dataset = generate_dataset(project_id, generation_params)\n\n    # Analyze changes from previous version\n    if has_previous_version(project_id, generation_params['dataset_name']):\n        comparison = compare_with_previous_version(project_id, generation_params['dataset_name'])\n\n        # Determine version increment type\n        version_type = determine_version_increment(comparison)\n\n        # Increment version\n        new_version = increment_version(project_id, generation_params['dataset_name'], version_type)\n\n        # Associate dataset with version\n        associate_dataset_with_version(dataset, new_version)\n\n    return dataset\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#5-monitoring-and-alerts","title":"5. Monitoring and Alerts","text":"<p>Version Health Monitoring: <pre><code>def monitor_version_health():\n    \"\"\"Monitor version health and send alerts.\"\"\"\n    issues = []\n\n    # Check for datasets without versions\n    unversioned = find_unversioned_datasets()\n    if unversioned:\n        issues.append(f\"Unversioned datasets: {unversioned}\")\n\n    # Check for rapid version changes (potential issues)\n    rapid_changes = detect_rapid_version_changes()\n    if rapid_changes:\n        issues.append(f\"Rapid version changes detected: {rapid_changes}\")\n\n    # Check for large version gaps\n    gaps = detect_version_gaps()\n    if gaps:\n        issues.append(f\"Version gaps detected: {gaps}\")\n\n    if issues:\n        send_alert(\"Version Health Issues\", issues)\n\n    return issues\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#6-backup-and-recovery","title":"6. Backup and Recovery","text":"<p>Version-Based Backup Strategy: <pre><code>def create_version_backup(project_id, dataset_name, version):\n    \"\"\"Create backup of specific version.\"\"\"\n    # Export version data\n    version_data = export_version(project_id, dataset_name, version)\n\n    # Create backup record\n    backup_id = create_backup_record(version_data, version)\n\n    # Store in backup location\n    store_backup(backup_id, version_data)\n\n    return backup_id\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/datasetversioningapi/#common-error-responses","title":"Common Error Responses","text":"<p>Version Not Found: <pre><code>{\n  \"detail\": \"Version 1.0.2 not found for dataset 'my_dataset'\"\n}\n</code></pre></p> <p>Invalid Version Format: <pre><code>{\n  \"detail\": \"Invalid version format. Expected semantic version (major.minor.patch)\"\n}\n</code></pre></p> <p>Rollback Failed: <pre><code>{\n  \"detail\": \"Cannot rollback: target version is corrupted\"\n}\n</code></pre></p>"},{"location":"userGuide/datasetversioningapi/#integration-examples","title":"Integration Examples","text":""},{"location":"userGuide/datasetversioningapi/#python-client","title":"Python Client","text":"<pre><code>import requests\n\nclass DatasetVersioningClient:\n    def __init__(self, base_url=\"http://localhost:8000\"):\n        self.base_url = base_url\n\n    def list_versions(self, project_id, dataset_name):\n        response = requests.get(\n            f\"{self.base_url}/api/v1/datasets/versions/\",\n            params={\"project_id\": project_id, \"dataset_name\": dataset_name}\n        )\n        return response.json()\n\n    def compare_versions(self, project_id, dataset_name, v1, v2):\n        response = requests.post(\n            f\"{self.base_url}/api/v1/datasets/versions/compare\",\n            json={\n                \"project_id\": project_id,\n                \"dataset_name\": dataset_name,\n                \"version1\": v1,\n                \"version2\": v2\n            }\n        )\n        return response.json()\n\n    def rollback(self, project_id, dataset_name, target_version):\n        response = requests.post(\n            f\"{self.base_url}/api/v1/datasets/versions/rollback\",\n            json={\n                \"project_id\": project_id,\n                \"dataset_name\": dataset_name,\n                \"target_version\": target_version\n            }\n        )\n        return response.json()\n</code></pre>"},{"location":"userGuide/datasetversioningapi/#cli-integration","title":"CLI Integration","text":"<pre><code># List versions\ncompileo dataset-version list-versions --project-id 1 --dataset-name my_dataset\n\n# Compare versions\ncompileo dataset-version compare-versions --project-id 1 --dataset-name my_dataset --version1 1.0.0 --version2 1.0.1\n\n# Rollback\ncompileo dataset-version rollback --project-id 1 --dataset-name my_dataset --target-version 1.0.0\n\n# Increment version\ncompileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type patch --description \"Bug fixes\"\n</code></pre> <p>This API provides essential version control capabilities for dataset management, ensuring data integrity and enabling safe experimentation with dataset improvements.</p>"},{"location":"userGuide/datasetversioningcli/","title":"Dataset Versioning Module CLI Usage Guide","text":"<p>The Compileo Dataset Versioning CLI provides comprehensive command-line tools for managing dataset versions, enabling tracking of dataset changes, comparisons between versions, and safe rollbacks when needed.</p>"},{"location":"userGuide/datasetversioningcli/#commands-overview","title":"Commands Overview","text":"<pre><code>compileo dataset-version [COMMAND] [OPTIONS]\n</code></pre> <p>Available commands: - <code>list-versions</code> - List all versions of a dataset - <code>compare-versions</code> - Compare two dataset versions - <code>rollback</code> - Rollback dataset to a previous version - <code>increment-version</code> - Increment the version number of a dataset</p>"},{"location":"userGuide/datasetversioningcli/#list-dataset-versions","title":"List Dataset Versions","text":""},{"location":"userGuide/datasetversioningcli/#compileo-dataset-version-list-versions","title":"<code>compileo dataset-version list-versions</code>","text":"<p>List all versions of a specific dataset.</p> <p>Usage: <pre><code>compileo dataset-version list-versions --project-id PROJECT_ID --dataset-name DATASET_NAME [--format FORMAT]\n</code></pre></p> <p>Options: - <code>--project-id INTEGER</code> - Project ID (required) - <code>--dataset-name TEXT</code> - Dataset name (required) - <code>--format [table|json]</code> - Output format (default: table)</p> <p>Examples:</p> <p>List versions in table format: <pre><code>compileo dataset-version list-versions --project-id 1 --dataset-name my_dataset\n</code></pre></p> <p>Output: <pre><code>Versions for dataset 'my_dataset':\n--------------------------------------------------------------------------------\nVersion      Entries  Active   Created              Description\n--------------------------------------------------------------------------------\n1.0.0        1000     \u2713        2024-01-15 10:30    Initial dataset version\n1.0.1        1050     \u2713        2024-01-16 14:20    Added quality improvements\n1.0.2        1100     \u2713        2024-01-17 09:15    Fixed minor issues\n</code></pre></p> <p>List versions in JSON format: <pre><code>compileo dataset-version list-versions --project-id 1 --dataset-name my_dataset --format json\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#compare-dataset-versions","title":"Compare Dataset Versions","text":""},{"location":"userGuide/datasetversioningcli/#compileo-dataset-version-compare-versions","title":"<code>compileo dataset-version compare-versions</code>","text":"<p>Compare two versions of a dataset to see differences.</p> <p>Usage: <pre><code>compileo dataset-version compare-versions --project-id PROJECT_ID --dataset-name DATASET_NAME --version1 VERSION1 --version2 VERSION2\n</code></pre></p> <p>Options: - <code>--project-id INTEGER</code> - Project ID (required) - <code>--dataset-name TEXT</code> - Dataset name (required) - <code>--version1 TEXT</code> - First version to compare (required) - <code>--version2 TEXT</code> - Second version to compare (required)</p> <p>Examples:</p> <p>Compare two versions: <pre><code>compileo dataset-version compare-versions --project-id 1 --dataset-name my_dataset --version1 1.0.0 --version2 1.0.1\n</code></pre></p> <p>Output: <pre><code>Comparison between 1.0.0 and 1.0.1:\n============================================================\n\nVersion 1.0.0:\n  Entries: 1000\n  Created: 2024-01-15 10:30:00\n  Changes: 0\n\nVersion 1.0.1:\n  Entries: 1050\n  Created: 2024-01-16 14:20:00\n  Changes: 50\n\nComparison:\n  Entries difference: +50\n  Version relationship: 1.0.1 is newer than 1.0.0\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#rollback-dataset-version","title":"Rollback Dataset Version","text":""},{"location":"userGuide/datasetversioningcli/#compileo-dataset-version-rollback","title":"<code>compileo dataset-version rollback</code>","text":"<p>Rollback a dataset to a previous version.</p> <p>Usage: <pre><code>compileo dataset-version rollback --project-id PROJECT_ID --dataset-name DATASET_NAME --target-version VERSION [--confirm]\n</code></pre></p> <p>Options: - <code>--project-id INTEGER</code> - Project ID (required) - <code>--dataset-name TEXT</code> - Dataset name (required) - <code>--target-version TEXT</code> - Version to rollback to (required) - <code>--confirm</code> - Skip confirmation prompt</p> <p>Examples:</p> <p>Rollback with confirmation: <pre><code>compileo dataset-version rollback --project-id 1 --dataset-name my_dataset --target-version 1.0.0\n</code></pre></p> <p>Output: <pre><code>Are you sure you want to rollback dataset 'my_dataset' to version 1.0.0? This will deactivate all newer versions. [y/N]: y\nSuccessfully rolled back dataset 'my_dataset' to version 1.0.0\n</code></pre></p> <p>Rollback without confirmation: <pre><code>compileo dataset-version rollback --project-id 1 --dataset-name my_dataset --target-version 1.0.0 --confirm\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#increment-dataset-version","title":"Increment Dataset Version","text":""},{"location":"userGuide/datasetversioningcli/#compileo-dataset-version-increment-version","title":"<code>compileo dataset-version increment-version</code>","text":"<p>Increment the version number of a dataset (semantic versioning).</p> <p>Usage: <pre><code>compileo dataset-version increment-version --project-id PROJECT_ID --dataset-name DATASET_NAME --version-type TYPE [--description DESCRIPTION]\n</code></pre></p> <p>Options: - <code>--project-id INTEGER</code> - Project ID (required) - <code>--dataset-name TEXT</code> - Dataset name (required) - <code>--version-type [major|minor|patch]</code> - Type of version increment (default: patch) - <code>--description TEXT</code> - Description of the version change</p> <p>Version Types: - <code>major</code>: Breaking changes (1.0.0 \u2192 2.0.0) - <code>minor</code>: New features (1.0.0 \u2192 1.1.0) - <code>patch</code>: Bug fixes (1.0.0 \u2192 1.0.1)</p> <p>Examples:</p> <p>Increment patch version: <pre><code>compileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type patch --description \"Fixed minor issues\"\n</code></pre></p> <p>Output: <pre><code>Successfully incremented dataset 'my_dataset' to version 1.0.1\n</code></pre></p> <p>Increment minor version: <pre><code>compileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type minor --description \"Added new features\"\n</code></pre></p> <p>Increment major version: <pre><code>compileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type major --description \"Breaking changes\"\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/datasetversioningcli/#1-version-management-strategy","title":"1. Version Management Strategy","text":"<p>Semantic Versioning: <pre><code># Use semantic versioning for clear change tracking\n# major.minor.patch\n# - major: breaking changes\n# - minor: new features\n# - patch: bug fixes\n\n# Examples:\ncompileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type major --description \"API breaking changes\"\ncompileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type minor --description \"Added new classification features\"\ncompileo dataset-version increment-version --project-id 1 --dataset-name my_dataset --version-type patch --description \"Fixed null pointer exception\"\n</code></pre></p> <p>Version Increment Timing: <pre><code># Increment versions when:\n# - Adding new features or capabilities\n# - Fixing bugs that affect output quality\n# - Making breaking changes to data format\n# - Improving model performance significantly\n\n# Don't increment for:\n# - Internal code refactoring\n# - Documentation updates\n# - Minor performance optimizations\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#2-rollback-safety","title":"2. Rollback Safety","text":"<p>Safe Rollback Process: <pre><code># 1. Always backup current state before rollback\ncp -r datasets/ datasets_backup/\n\n# 2. Check what will be affected\ncompileo dataset-version compare-versions --project-id 1 --dataset-name my_dataset --version1 current --version2 target_version\n\n# 3. Perform rollback\ncompileo dataset-version rollback --project-id 1 --dataset-name my_dataset --target-version 1.0.0\n\n# 4. Verify rollback success\ncompileo dataset-version list-versions --project-id 1 --dataset-name my_dataset\n</code></pre></p> <p>Rollback Recovery: <pre><code># If rollback fails, restore from backup\ncp -r datasets_backup/ datasets/\n\n# Then investigate the failure\n# Check logs for error details\ntail -f logs/compileo.log\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#3-version-comparison-and-analysis","title":"3. Version Comparison and Analysis","text":"<p>Regular Version Audits: <pre><code># Compare recent versions to track progress\ncompileo dataset-version compare-versions --project-id 1 --dataset-name my_dataset --version1 1.0.0 --version2 1.1.0\n\n# Monitor version growth over time\ncompileo dataset-version list-versions --project-id 1 --dataset-name my_dataset --format json | jq '.[] | select(.is_active) | .version + \": \" + (.total_entries | tostring) + \" entries\"'\n</code></pre></p> <p>Quality Tracking Across Versions: <pre><code># Track quality improvements\n# Run quality analysis on different versions\n# Compare quality scores between versions\n# Identify versions with significant quality changes\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#4-integration-with-dataset-generation","title":"4. Integration with Dataset Generation","text":"<p>Version-Aware Dataset Generation: <pre><code># Generate dataset with automatic versioning\ncompileo generate-dataset --project-id 1 --enable-versioning --dataset-name my_dataset\n\n# After generation, check new version\ncompileo dataset-version list-versions --project-id 1 --dataset-name my_dataset\n\n# Compare with previous version\ncompileo dataset-version compare-versions --project-id 1 --dataset-name my_dataset --version1 1.0.0 --version2 1.0.1\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#5-backup-and-recovery","title":"5. Backup and Recovery","text":"<p>Version-Based Backup Strategy: <pre><code># Create backups before major changes\nmkdir -p backups/$(date +%Y%m%d)\ncp -r datasets/ backups/$(date +%Y%m%d)/\n\n# Tag backup with version\necho \"1.0.0\" &gt; backups/$(date +%Y%m%d)/version.txt\n\n# Restore from version-specific backup\nVERSION=\"1.0.0\"\nBACKUP_DIR=$(find backups/ -name \"version.txt\" -exec grep -l \"$VERSION\" {} \\; | head -1 | xargs dirname)\ncp -r $BACKUP_DIR/datasets/* datasets/\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#6-monitoring-and-alerts","title":"6. Monitoring and Alerts","text":"<p>Version Health Monitoring: <pre><code>#!/bin/bash\n# version_health_check.sh\n\nDATASET_NAME=\"my_dataset\"\nPROJECT_ID=\"1\"\n\n# Check for datasets without recent versions\nLAST_VERSION=$(compileo dataset-version list-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME --format json | jq -r '.[0].version')\nDAYS_SINCE_LAST=$(echo $(( ($(date +%s) - $(date -d \"$(compileo dataset-version list-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME --format json | jq -r '.[0].created_at')\" +%s)) / 86400 )))\n\nif [ \"$DAYS_SINCE_LAST\" -gt 30 ]; then\n    echo \"WARNING: No new versions created in $DAYS_SINCE_LAST days\"\nfi\n\n# Check version growth\nENTRIES=$(compileo dataset-version list-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME --format json | jq -r '.[0].total_entries')\nif [ \"$ENTRIES\" -lt 100 ]; then\n    echo \"WARNING: Latest version has only $ENTRIES entries\"\nfi\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/datasetversioningcli/#common-error-messages","title":"Common Error Messages","text":"<p>Dataset Not Found: <pre><code>Error: No versions found for dataset 'nonexistent_dataset' in project 1\n</code></pre></p> <p>Version Not Found: <pre><code>Error: Version 1.0.3 not found for dataset 'my_dataset'\n</code></pre></p> <p>Invalid Version Format: <pre><code>Error: Invalid version format. Expected semantic version (major.minor.patch)\n</code></pre></p> <p>Rollback Failed: <pre><code>Error: Cannot rollback: target version is corrupted or rollback failed\n</code></pre></p>"},{"location":"userGuide/datasetversioningcli/#integration-examples","title":"Integration Examples","text":""},{"location":"userGuide/datasetversioningcli/#automated-version-management-script","title":"Automated Version Management Script","text":"<pre><code>#!/bin/bash\n# auto_version.sh - Automatically manage dataset versions\n\nPROJECT_ID=\"$1\"\nDATASET_NAME=\"$2\"\nCHANGE_TYPE=\"$3\"  # major, minor, patch\nDESCRIPTION=\"$4\"\n\nif [ -z \"$PROJECT_ID\" ] || [ -z \"$DATASET_NAME\" ] || [ -z \"$CHANGE_TYPE\" ]; then\n    echo \"Usage: $0 &lt;project_id&gt; &lt;dataset_name&gt; &lt;change_type&gt; [description]\"\n    exit 1\nfi\n\n# Generate dataset (your generation command here)\necho \"Generating dataset...\"\n# compileo generate-dataset --project-id $PROJECT_ID --dataset-name $DATASET_NAME\n\n# Run quality analysis\necho \"Running quality analysis...\"\nQUALITY_SCORE=$(compileo analyze-quality generated_dataset.jsonl --format json | jq -r '.summary.overall_score')\n\n# Determine version increment based on quality\nif (( $(echo \"$QUALITY_SCORE &gt; 0.9\" | bc -l) )); then\n    VERSION_TYPE=\"minor\"\n    DESC=\"High quality dataset generated (score: $QUALITY_SCORE)\"\nelif (( $(echo \"$QUALITY_SCORE &gt; 0.7\" | bc -l) )); then\n    VERSION_TYPE=\"patch\"\n    DESC=\"Dataset generated with acceptable quality (score: $QUALITY_SCORE)\"\nelse\n    echo \"Quality score too low ($QUALITY_SCORE), skipping version increment\"\n    exit 1\nfi\n\n# Increment version\ncompileo dataset-version increment-version \\\n    --project-id $PROJECT_ID \\\n    --dataset-name $DATASET_NAME \\\n    --version-type $VERSION_TYPE \\\n    --description \"${DESC}\"\n\necho \"Dataset version incremented successfully\"\n</code></pre>"},{"location":"userGuide/datasetversioningcli/#version-comparison-report","title":"Version Comparison Report","text":"<pre><code>#!/bin/bash\n# version_report.sh - Generate version comparison reports\n\nPROJECT_ID=\"$1\"\nDATASET_NAME=\"$2\"\n\necho \"=== Dataset Version Report ===\"\necho \"Dataset: $DATASET_NAME (Project: $PROJECT_ID)\"\necho \"Generated: $(date)\"\necho\n\n# List all versions\necho \"All Versions:\"\ncompileo dataset-version list-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME\necho\n\n# Compare last two versions\nLATEST=$(compileo dataset-version list-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME --format json | jq -r '.[0].version')\nPREVIOUS=$(compileo dataset-version list-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME --format json | jq -r '.[1].version')\n\nif [ \"$PREVIOUS\" != \"null\" ]; then\n    echo \"Comparison ($LATEST vs $PREVIOUS):\"\n    compileo dataset-version compare-versions --project-id $PROJECT_ID --dataset-name $DATASET_NAME --version1 $PREVIOUS --version2 $LATEST\nfi\n</code></pre> <p>This CLI provides essential version control capabilities for dataset management, ensuring data integrity and enabling safe experimentation with dataset improvements.</p>"},{"location":"userGuide/dockerInstall/","title":"Docker Installation Guide","text":"<p>This guide provides step-by-step instructions for installing and running Compileo using Docker and Docker Compose.</p>"},{"location":"userGuide/dockerInstall/#prerequisites","title":"Prerequisites","text":"<p>Before installing Compileo via Docker, ensure you have the following:</p>"},{"location":"userGuide/dockerInstall/#system-requirements","title":"System Requirements","text":"<ul> <li>Docker: Version 20.10 or later</li> <li>Docker Compose: Version 2.0 or later</li> <li>At least 4GB RAM (8GB recommended)</li> <li>At least 10GB free disk space</li> </ul>"},{"location":"userGuide/dockerInstall/#installing-docker","title":"Installing Docker","text":""},{"location":"userGuide/dockerInstall/#ubuntudebian","title":"Ubuntu/Debian","text":"<pre><code># Update package index\nsudo apt update\n\n# Install Docker\nsudo apt install docker.io docker-compose-plugin\n\n# Start and enable Docker service\nsudo systemctl start docker\nsudo systemctl enable docker\n\n# Add your user to the docker group (optional, allows running without sudo)\nsudo usermod -aG docker $USER\n# Log out and back in for the group change to take effect\n</code></pre>"},{"location":"userGuide/dockerInstall/#macos","title":"macOS","text":"<pre><code># Install Docker Desktop from https://www.docker.com/products/docker-desktop\n# Or using Homebrew\nbrew install --cask docker\n</code></pre>"},{"location":"userGuide/dockerInstall/#windows","title":"Windows","text":"<pre><code># Install Docker Desktop from https://www.docker.com/products/docker-desktop\n</code></pre>"},{"location":"userGuide/dockerInstall/#verify-installation","title":"Verify Installation","text":"<pre><code># Check Docker version\ndocker --version\n\n# Check Docker Compose version\ndocker compose version\n\n# Test Docker installation\ndocker run hello-world\n</code></pre>"},{"location":"userGuide/dockerInstall/#quick-start","title":"Quick Start","text":""},{"location":"userGuide/dockerInstall/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/your-org/compileo.git\ncd compileo\n</code></pre>"},{"location":"userGuide/dockerInstall/#2-configure-environment","title":"2. Configure Environment","text":"<p>Copy the example environment file and edit it:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code> with your configuration:</p> <pre><code># Redis (required for job queuing)\nREDIS_URL=redis://redis:6379/0\n\n# API Base URL (for GUI to connect to API)\nAPI_BASE_URL=http://compileo-api:8000\n\n# API Keys (comma-separated list)\nCOMPILEO_API_KEYS=your-api-key-1,your-api-key-2\n\n# AI Model API Keys (optional, for enhanced features)\nGEMINI_API_KEY=your-gemini-api-key\nGROK_API_KEY=your-grok-api-key\nHUGGINGFACE_API_KEY=your-huggingface-api-key\nOPENAI_API_KEY=your-openai-api-key\n\n# Database (SQLite is used by default)\nDATABASE_URL=sqlite:///database.db\n\n# Logging\nLOG_LEVEL=INFO\n</code></pre>"},{"location":"userGuide/dockerInstall/#3-build-and-start-services","title":"3. Build and Start Services","text":"<pre><code># Build and start all services\ndocker compose up --build\n\n# Or run in background\ndocker compose up --build -d\n</code></pre>"},{"location":"userGuide/dockerInstall/#4-access-the-application","title":"4. Access the Application","text":"<p>Once the services are running:</p> <ul> <li>Web GUI: http://localhost:8501</li> <li>API Documentation: http://localhost:8000/docs</li> <li>API Base URL: http://localhost:8000</li> </ul>"},{"location":"userGuide/dockerInstall/#service-overview","title":"Service Overview","text":"<p>Compileo runs as three main services:</p>"},{"location":"userGuide/dockerInstall/#compileo-api","title":"compileo-api","text":"<ul> <li>Port: 8000</li> <li>Purpose: REST API backend with FastAPI</li> <li>Features: Document processing, dataset generation, job queuing</li> <li>Health Check: http://localhost:8000/docs</li> </ul>"},{"location":"userGuide/dockerInstall/#compileo-gui","title":"compileo-gui","text":"<ul> <li>Port: 8501</li> <li>Purpose: Web interface built with Streamlit</li> <li>Features: Dataset creation wizard, project management, quality analysis</li> <li>Health Check: http://localhost:8501/healthz</li> </ul>"},{"location":"userGuide/dockerInstall/#redis","title":"redis","text":"<ul> <li>Port: 6380 (external), 6379 (internal)</li> <li>Purpose: Job queuing and caching</li> <li>Features: RQ job queue, session storage</li> </ul>"},{"location":"userGuide/dockerInstall/#configuration","title":"Configuration","text":""},{"location":"userGuide/dockerInstall/#environment-variables","title":"Environment Variables","text":"Variable Description Default Required <code>REDIS_URL</code> Redis connection URL <code>redis://redis:6379/0</code> Yes <code>API_BASE_URL</code> API base URL for GUI <code>http://compileo-api:8000</code> Yes <code>COMPILEO_API_KEYS</code> API authentication keys - No <code>GEMINI_API_KEY</code> Google Gemini API key - No <code>GROK_API_KEY</code> xAI Grok API key - No <code>HUGGINGFACE_API_KEY</code> HuggingFace API key - No <code>OPENAI_API_KEY</code> OpenAI API key - No <code>DATABASE_URL</code> Database connection URL <code>sqlite:///database.db</code> No <code>LOG_LEVEL</code> Logging level <code>INFO</code> No"},{"location":"userGuide/dockerInstall/#api-keys-setup","title":"API Keys Setup","text":"<p>For enhanced features, configure API keys in the <code>.env</code> file:</p> <pre><code># Google Gemini (for document classification)\nGEMINI_API_KEY=your-gemini-api-key\n\n# xAI Grok (for advanced reasoning)\nGROK_API_KEY=your-grok-api-key\n\n# HuggingFace (for model downloads)\nHUGGINGFACE_API_KEY=your-huggingface-api-key\n\n# OpenAI (for GPT models)\nOPENAI_API_KEY=your-openai-api-key\n</code></pre>"},{"location":"userGuide/dockerInstall/#volume-management","title":"Volume Management","text":"<p>Compileo uses Docker volumes for data persistence:</p> <ul> <li><code>compileo_storage</code>: Application data and SQLite database</li> <li><code>compileo_plugins</code>: Plugin storage</li> <li><code>compileo_hf_models</code>: HuggingFace model cache (Pre-population recommended for large models)</li> <li><code>redis_data</code>: Redis persistence</li> </ul>"},{"location":"userGuide/dockerInstall/#pre-populating-huggingface-models","title":"Pre-populating HuggingFace Models","text":"<p>If the HuggingFace parser hangs during model download in Docker, you can manually copy model weights from a local cache:</p> <pre><code>docker cp src/compileo/features/ingestion/hf_models/models--nanonets--Nanonets-OCR2-3B compileo-compileo-api-1:/app/src/compileo/features/ingestion/hf_models/\ndocker exec -u 0 compileo-compileo-api-1 chown -R compileo:compileo /app/src/compileo/features/ingestion/hf_models/\n</code></pre> <p>To view volume data: <pre><code># List volumes\ndocker volume ls | grep compileo\n\n# Inspect volume contents\ndocker run --rm -v compileo_compileo_storage:/data alpine ls -la /data\n</code></pre></p>"},{"location":"userGuide/dockerInstall/#usage","title":"Usage","text":""},{"location":"userGuide/dockerInstall/#starting-services","title":"Starting Services","text":"<pre><code># Start all services\ndocker compose up\n\n# Start in background\ndocker compose up -d\n\n# Start specific service\ndocker compose up compileo-gui\n\n# Start with rebuild\ndocker compose up --build\n</code></pre>"},{"location":"userGuide/dockerInstall/#stopping-services","title":"Stopping Services","text":"<pre><code># Stop all services\ndocker compose down\n\n# Stop and remove volumes (WARNING: deletes data)\ndocker compose down -v\n</code></pre>"},{"location":"userGuide/dockerInstall/#viewing-logs","title":"Viewing Logs","text":"<pre><code># View all logs\ndocker compose logs\n\n# View specific service logs\ndocker compose logs compileo-api\n\n# Follow logs in real-time\ndocker compose logs -f compileo-gui\n</code></pre>"},{"location":"userGuide/dockerInstall/#updating","title":"Updating","text":"<pre><code># Pull latest changes\ngit pull\n\n# Rebuild and restart\ndocker compose up --build -d\n</code></pre>"},{"location":"userGuide/dockerInstall/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/dockerInstall/#common-issues","title":"Common Issues","text":""},{"location":"userGuide/dockerInstall/#services-wont-start","title":"Services Won't Start","text":"<p>Check system resources: <pre><code># Check available memory\nfree -h\n\n# Check disk space\ndf -h\n\n# Check Docker system\ndocker system df\n</code></pre></p> <p>Check logs: <pre><code>docker compose logs\n</code></pre></p>"},{"location":"userGuide/dockerInstall/#gui-cant-connect-to-api","title":"GUI Can't Connect to API","text":"<p>Verify API_BASE_URL: <pre><code># Check environment variable\ndocker compose exec compileo-gui env | grep API_BASE_URL\n</code></pre></p> <p>Test API connectivity: <pre><code># From GUI container\ndocker compose exec compileo-gui curl -f http://compileo-api:8000/docs\n\n# From host\ncurl -f http://localhost:8000/docs\n</code></pre></p>"},{"location":"userGuide/dockerInstall/#redis-connection-issues","title":"Redis Connection Issues","text":"<p>Check Redis status: <pre><code>docker compose ps redis\ndocker compose logs redis\n</code></pre></p> <p>Test Redis connectivity: <pre><code>docker compose exec redis redis-cli ping\n</code></pre></p>"},{"location":"userGuide/dockerInstall/#permission-issues","title":"Permission Issues","text":"<p>Fix volume permissions: <pre><code># Reset volumes (WARNING: deletes data)\ndocker compose down -v\ndocker compose up --build -d\n</code></pre></p>"},{"location":"userGuide/dockerInstall/#build-failures","title":"Build Failures","text":"<p>Clear Docker cache: <pre><code>docker system prune -a\ndocker compose build --no-cache\n</code></pre></p>"},{"location":"userGuide/dockerInstall/#performance-issues","title":"Performance Issues","text":""},{"location":"userGuide/dockerInstall/#memory-usage","title":"Memory Usage","text":"<pre><code># Check container memory usage\ndocker stats\n\n# Limit memory usage in docker-compose.yml\nservices:\n  compileo-api:\n    deploy:\n      resources:\n        limits:\n          memory: 4G\n        reservations:\n          memory: 2G\n</code></pre>"},{"location":"userGuide/dockerInstall/#gpu-support","title":"GPU Support","text":"<p>For NVIDIA GPU support, ensure NVIDIA Container Toolkit is installed:</p> <pre><code># Install NVIDIA Docker support\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2\nsudo systemctl restart docker\n</code></pre>"},{"location":"userGuide/dockerInstall/#data-management","title":"Data Management","text":""},{"location":"userGuide/dockerInstall/#backup-data","title":"Backup Data","text":"<pre><code># Backup volumes\ndocker run --rm -v compileo_compileo_storage:/data -v $(pwd):/backup alpine tar czf /backup/backup.tar.gz -C /data .\n</code></pre>"},{"location":"userGuide/dockerInstall/#restore-data","title":"Restore Data","text":"<pre><code># Restore volumes\ndocker run --rm -v compileo_compileo_storage:/data -v $(pwd):/backup alpine tar xzf /backup/backup.tar.gz -C /data\n</code></pre>"},{"location":"userGuide/dockerInstall/#reset-database","title":"Reset Database","text":"<pre><code># Stop services\ndocker compose down\n\n# Remove database volume\ndocker volume rm compileo_compileo_storage\n\n# Restart services (creates new database)\ndocker compose up -d\n</code></pre>"},{"location":"userGuide/dockerInstall/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"userGuide/dockerInstall/#custom-docker-compose","title":"Custom Docker Compose","text":"<p>Create a <code>docker-compose.override.yml</code> for custom configuration:</p> <pre><code>version: '3.8'\n\nservices:\n  compileo-api:\n    environment:\n      - LOG_LEVEL=DEBUG\n    ports:\n      - \"8001:8000\"  # Custom port\n\n  compileo-gui:\n    ports:\n      - \"8502:8501\"  # Custom port\n</code></pre>"},{"location":"userGuide/dockerInstall/#development-setup","title":"Development Setup","text":"<p>For development with live code reloading:</p> <pre><code>version: '3.8'\n\nservices:\n  compileo-api:\n    volumes:\n      - ./src:/app/src  # Mount source code\n    environment:\n      - PYTHONPATH=/app\n    command: uvicorn src.compileo.api.main:app --host 0.0.0.0 --port 8000 --reload\n\n  compileo-gui:\n    volumes:\n      - ./src:/app/src  # Mount source code\n    environment:\n      - PYTHONPATH=/app\n</code></pre>"},{"location":"userGuide/dockerInstall/#production-deployment","title":"Production Deployment","text":"<p>For production deployments:</p> <pre><code>version: '3.8'\n\nservices:\n  compileo-api:\n    environment:\n      - LOG_LEVEL=WARNING\n    deploy:\n      resources:\n        limits:\n          memory: 8G\n          cpus: '2.0'\n    restart: unless-stopped\n\n  compileo-gui:\n    environment:\n      - LOG_LEVEL=WARNING\n    deploy:\n      resources:\n        limits:\n          memory: 4G\n          cpus: '1.0'\n    restart: unless-stopped\n\n  redis:\n    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru\n</code></pre>"},{"location":"userGuide/dockerInstall/#getting-help","title":"Getting Help","text":""},{"location":"userGuide/dockerInstall/#documentation","title":"Documentation","text":"<ul> <li>API Documentation - Interactive API docs</li> <li>GUI User Guide - Web interface guide</li> <li>Configuration Guide - Advanced configuration</li> </ul>"},{"location":"userGuide/dockerInstall/#common-commands","title":"Common Commands","text":"<pre><code># View service status\ndocker compose ps\n\n# Execute commands in containers\ndocker compose exec compileo-api bash\ndocker compose exec compileo-gui bash\n\n# View resource usage\ndocker stats\n\n# Clean up\ndocker system prune\ndocker volume prune\n</code></pre>"},{"location":"userGuide/dockerInstall/#support","title":"Support","text":"<p>If you encounter issues: 1. Check the troubleshooting section above 2. Review the logs: <code>docker compose logs</code> 3. Check the GitHub Issues for similar problems 4. Create a new issue with your Docker version, OS, and complete error logs</p>"},{"location":"userGuide/dockerInstall/#next-steps","title":"Next Steps","text":"<p>Once Compileo is running:</p> <ol> <li>Access the Web GUI at http://localhost:8501</li> <li>Create your first project using the dataset creation wizard</li> <li>Upload documents (PDF, DOCX, TXT, etc.)</li> <li>Configure AI models in the Settings page</li> <li>Generate datasets using the guided workflow</li> </ol> <p>For detailed usage instructions, see the GUI User Guide.</p>"},{"location":"userGuide/documentsapi/","title":"Documents Module API Usage Guide","text":"<p>The Compileo Documents API provides comprehensive REST endpoints for document management, including upload, parsing, processing, and content retrieval. This guide covers all available API endpoints with examples and best practices.</p>"},{"location":"userGuide/documentsapi/#base-url-apiv1documents","title":"Base URL: <code>/api/v1/documents</code>","text":""},{"location":"userGuide/documentsapi/#document-upload","title":"Document Upload","text":""},{"location":"userGuide/documentsapi/#post-upload","title":"POST <code>/upload</code>","text":"<p>Upload one or more documents to a project.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/documents/upload\" \\\n  -F \"project_id=1\" \\\n  -F \"files=@document1.pdf\" \\\n  -F \"files=@document2.docx\"\n</code></pre></p> <p>Request Body (multipart/form-data): - <code>files</code>: Document files (multiple allowed) - <code>project_id</code>: Target project ID</p> <p>Response: <pre><code>{\n  \"job_id\": \"doc_upload_12345\",\n  \"message\": \"Documents uploaded successfully\",\n  \"files_count\": 2,\n  \"uploaded_files\": [\n    {\n      \"id\": 101,\n      \"project_id\": 1,\n      \"file_name\": \"document1.pdf\",\n      \"source_file_path\": \"storage/uploads/1/document1.pdf\",\n      \"parsed_file_path\": null,\n      \"created_at\": \"2024-01-21T12:00:00Z\",\n      \"status\": \"uploaded\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#document-parsing","title":"Document Parsing","text":""},{"location":"userGuide/documentsapi/#post-parse","title":"POST <code>/parse</code>","text":"<p>Parse documents to paginated markdown files without chunking. The consolidated <code>.md</code> file is no longer created to avoid storage duplication.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/documents/parse\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"document_ids\": [101, 102],\n    \"parser\": \"gemini\"\n  }'\n</code></pre></p> <p>Request Body: - <code>project_id</code>: Project containing documents - <code>document_ids</code>: Array of document IDs to parse - <code>parser</code>: Parser type (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>)</p> <p>Response: <pre><code>{\n  \"job_id\": \"parse_job_12345\",\n  \"message\": \"Successfully parsed 2 documents to markdown\",\n  \"parsed_documents\": 2,\n  \"status\": \"completed\"\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#document-processing","title":"Document Processing","text":""},{"location":"userGuide/documentsapi/#post-process","title":"POST <code>/process</code>","text":"<p>Process documents with parsing and chunking in a single operation.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/documents/process\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"document_ids\": [101, 102],\n    \"parser\": \"gemini\",\n    \"chunk_strategy\": \"character\",\n    \"character_chunk_size\": 500,\n    \"character_overlap\": 50\n  }'\n</code></pre></p> <p>Request Body: - <code>project_id</code>: Project containing documents - <code>document_ids</code>: Array of document IDs to process - <code>parser</code>: Parser type - <code>chunk_strategy</code>: Chunking strategy (<code>token</code>, <code>character</code>, <code>semantic</code>, <code>delimiter</code>, <code>schema</code>) - <code>chunk_size</code>: Chunk size (tokens/characters) - <code>overlap</code>: Overlap between chunks - <code>skip_parsing</code>: Skip parsing if documents are already parsed</p> <p>Response: <pre><code>{\n  \"job_id\": \"process_job_12345\",\n  \"message\": \"Successfully processed 2 documents, created 15 chunks\",\n  \"processed_documents\": 2,\n  \"total_chunks\": 15,\n  \"status\": \"completed\"\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#document-content-retrieval","title":"Document Content Retrieval","text":""},{"location":"userGuide/documentsapi/#get-document_idcontent","title":"GET <code>/{document_id}/content</code>","text":"<p>Retrieve paginated parsed document content with pagination support. The consolidated <code>.md</code> file is no longer created.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/documents/101/content?parsed_file=101_1.md\"\n</code></pre></p> <p>Query Parameters: - <code>parsed_file</code>: Name of specific paginated parsed file to retrieve full content (e.g., \"document_1.md\"). If omitted, returns a list of available paginated parsed files. - <code>page</code>: Page number for pagination (only used when <code>parsed_file</code> is not specified). - <code>page_size</code>: Number of characters per page (only used when <code>parsed_file</code> is not specified).</p> <p>Response (when <code>parsed_file</code> is specified): <pre><code>{\n  \"document_id\": 101,\n  \"file_name\": \"document1.pdf\",\n  \"content\": \"Full content of the parsed file...\",\n  \"total_length\": 10891,\n  \"word_count\": 2156,\n  \"line_count\": 234,\n  \"current_file\": \"101_1.md\",\n  \"parsed_file\": \"101_1.md\"\n}\n</code></pre></p> <p>Response (when <code>parsed_file</code> is omitted): <pre><code>{\n  \"document_id\": 101,\n  \"file_name\": \"document1.pdf\",\n  \"parsed_files\": [\n    \"101_1.md\",\n    \"101_2.md\",\n    \"101_3.md\"\n  ],\n  \"total_files\": 3,\n  \"manifest_path\": \"storage/parsed/101/manifest.json\"\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#document-management","title":"Document Management","text":""},{"location":"userGuide/documentsapi/#get","title":"GET <code>/</code>","text":"<p>List documents with optional project filtering.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/documents/?project_id=1\"\n</code></pre></p> <p>Query Parameters: - <code>project_id</code>: Filter by project ID (optional)</p> <p>Response: <pre><code>{\n  \"documents\": [\n    {\n      \"id\": 101,\n      \"project_id\": 1,\n      \"file_name\": \"document1.pdf\",\n      \"source_file_path\": \"storage/uploads/1/document1.pdf\",\n      \"parsed_file_path\": null,\n      \"created_at\": \"2024-01-21T12:00:00Z\",\n      \"status\": \"parsed\"\n    }\n  ],\n  \"total\": 1\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#delete-document_id","title":"DELETE <code>/{document_id}</code>","text":"<p>Delete a document and all associated files.</p> <p>Request: <pre><code>curl -X DELETE \"http://localhost:8000/api/v1/documents/101\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Document 'document1.pdf' and all associated files deleted successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#job-status-monitoring","title":"Job Status Monitoring","text":""},{"location":"userGuide/documentsapi/#get-uploadjob_idstatus","title":"GET <code>/upload/{job_id}/status</code>","text":"<p>Check upload job status.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/documents/upload/doc_upload_12345/status\"\n</code></pre></p>"},{"location":"userGuide/documentsapi/#get-processjob_idstatus","title":"GET <code>/process/{job_id}/status</code>","text":"<p>Check processing job status.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/documents/process/process_job_12345/status\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"process_job_12345\",\n  \"status\": \"completed\",\n  \"progress\": 100,\n  \"current_step\": \"Processing complete\",\n  \"estimated_completion\": \"2024-01-21T12:05:00Z\",\n  \"processed_files\": []\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#pdf-preprocessing","title":"PDF Preprocessing","text":""},{"location":"userGuide/documentsapi/#post-split-pdf","title":"POST <code>/split-pdf</code>","text":"<p>Split large PDFs into smaller chunks for processing.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/documents/split-pdf\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"pdf_path\": \"large_document.pdf\",\n    \"pages_per_split\": 200,\n    \"overlap_pages\": 1\n  }'\n</code></pre></p> <p>Request Body: - <code>pdf_path</code>: Path to PDF file - <code>pages_per_split</code>: Pages per split (default: 200) - <code>overlap_pages</code>: Overlapping pages (default: 1)</p> <p>Response: <pre><code>{\n  \"split_files\": [\"split_1.pdf\", \"split_2.pdf\"],\n  \"message\": \"Successfully split PDF into 2 files\",\n  \"total_splits\": 2,\n  \"manifest_path\": \"manifest.json\"\n}\n</code></pre></p>"},{"location":"userGuide/documentsapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/documentsapi/#1-batch-processing","title":"1. Batch Processing","text":"<pre><code>import requests\n\n# Upload multiple files\nfiles = [\n    ('files', open('doc1.pdf', 'rb')),\n    ('files', open('doc2.pdf', 'rb'))\n]\nresponse = requests.post(\n    'http://localhost:8000/api/v1/documents/upload',\n    files=files,\n    data={'project_id': 1}\n)\n\n# Process all uploaded documents\ndoc_ids = [doc['id'] for doc in response.json()['uploaded_files']]\nprocess_response = requests.post(\n    'http://localhost:8000/api/v1/documents/process',\n    json={\n        'project_id': 1,\n        'document_ids': doc_ids,\n        'parser': 'gemini',\n        'chunk_strategy': 'character',\n        'character_chunk_size': 500\n    }\n)\n</code></pre>"},{"location":"userGuide/documentsapi/#2-progress-monitoring","title":"2. Progress Monitoring","text":"<pre><code>import time\n\njob_id = process_response.json()['job_id']\n\nwhile True:\n    status_response = requests.get(\n        f'http://localhost:8000/api/v1/documents/process/{job_id}/status'\n    )\n    status = status_response.json()\n\n    if status['status'] == 'completed':\n        print(\"Processing completed!\")\n        break\n    elif status['status'] == 'failed':\n        print(\"Processing failed!\")\n        break\n\n    print(f\"Progress: {status['progress']}%\")\n    time.sleep(5)\n</code></pre>"},{"location":"userGuide/documentsapi/#3-content-pagination","title":"3. Content Pagination","text":"<pre><code># Get document content (list paginated files)\ncontent_response = requests.get(\n    'http://localhost:8000/api/v1/documents/101/content'\n)\ndata = content_response.json()\nprint(f\"Available parsed files: {data['parsed_files']}\")\n\n# Get content of a specific paginated file\nfile_content_response = requests.get(\n    f\"http://localhost:8000/api/v1/documents/101/content?parsed_file={data['parsed_files'][0]}\"\n)\nfile_data = file_content_response.json()\nprint(f\"Content of {file_data['parsed_file']}: {file_data['content'][:200]}...\")\n</code></pre>"},{"location":"userGuide/documentsapi/#4-error-handling","title":"4. Error Handling","text":"<pre><code>try:\n    response = requests.post('http://localhost:8000/api/v1/documents/upload', ...)\n    response.raise_for_status()\n    # Process successful response\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 404:\n        print(\"Project not found\")\n    elif e.response.status_code == 400:\n        print(\"Invalid file type\")\n    else:\n        print(f\"API error: {e}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Network error: {e}\")\n</code></pre> <p>This API provides comprehensive document management capabilities with support for multiple file formats, various parsing options, and flexible chunking strategies suitable for both development and production workflows.</p>"},{"location":"userGuide/documentscli/","title":"Documents Module CLI Usage Guide","text":"<p>The Compileo Documents CLI provides comprehensive command-line tools for document management, including upload, parsing, processing, and content retrieval. This guide covers all available CLI commands with examples and best practices.</p>"},{"location":"userGuide/documentscli/#command-overview","title":"Command Overview","text":"<pre><code>graph TD\n    A[Documents CLI] --&gt; B[Upload]\n    A --&gt; C[Parse]\n    A --&gt; D[Process]\n    A --&gt; E[Chunk]\n    A --&gt; F[Content]\n    A --&gt; G[List]\n    A --&gt; H[Delete]\n    A --&gt; I[Status]\n    A --&gt; J[Split PDF]\n\n    B --&gt; B1[Multiple files]\n    C --&gt; C1[Markdown conversion]\n    D --&gt; D1[Parse + Chunk]\n    E --&gt; E1[Chunk only]\n    F --&gt; F1[Pagination support]\n</code></pre>"},{"location":"userGuide/documentscli/#document-upload","title":"Document Upload","text":"<p>Upload one or more documents to a project:</p> <pre><code>compileo documents upload --project-id 1 document1.pdf document2.docx document3.txt\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID to upload documents to (required) - <code>file_paths</code>: One or more document file paths (required)</p> <p>Supported Formats: PDF, DOCX, TXT, MD, CSV, JSON, XML</p> <p>Example Output: <pre><code>\ud83d\udce4 Uploading 3 documents to project 1...\n\u2705 Documents uploaded successfully. Job ID: doc_upload_12345\n\ud83d\udcca Files uploaded: 3\n\u2705 Upload completed. 3 documents processed.\n</code></pre></p>"},{"location":"userGuide/documentscli/#document-parsing","title":"Document Parsing","text":"<p>Parse documents to markdown format without chunking:</p> <pre><code>compileo documents parse --project-id 1 --document-ids 101,102,103 --parser gemini\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID containing documents (required) - <code>--document-ids</code>: Comma-separated list of document IDs (required) - <code>--parser</code>: Parser type (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>)</p> <p>Example Output: <pre><code>\ud83d\udcc4 Parsing 3 documents in project 1 with gemini\n\u2705 Parsing started. Job ID: parse_job_12345\n\u23f3 Waiting for parsing completion...\n\u2705 Parsing completed successfully!\n\ud83d\udcca Results: 3 documents parsed to markdown\n</code></pre></p>"},{"location":"userGuide/documentscli/#document-processing","title":"Document Processing","text":"<p>Process documents with both parsing and chunking in a single operation:</p> <pre><code>compileo documents process --project-id 1 --document-ids 101,102 --parser gemini --chunk-strategy character --character-chunk-size 500 --character-overlap 50\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID containing documents (required) - <code>--document-ids</code>: Comma-separated list of document IDs (required) - <code>--parser</code>: Parser type (default: <code>gemini</code>) - <code>--chunk-strategy</code>: Chunking strategy (<code>token</code>, <code>character</code>, <code>semantic</code>, <code>delimiter</code>, <code>schema</code>) - <code>--chunk-size</code>: Chunk size (default: 512) - <code>--overlap</code>: Overlap between chunks (default: 50) - <code>--skip-parsing</code>: Skip parsing if documents are already parsed</p> <p>Chunking Strategy Options: - Character: <code>--character-chunk-size</code>, <code>--character-overlap</code> - Semantic: <code>--semantic-prompt</code> - Schema: <code>--schema-definition</code> - Delimiter: Custom delimiter splitting</p> <p>Example Output: <pre><code>\u2699\ufe0f Processing 2 documents in project 1\n\ud83d\udd0d Parser: gemini\n\u2702\ufe0f Chunk Strategy: character\n\u2705 Processing started. Job ID: process_job_12345\n\u23f3 Waiting for processing completion...\n\u2705 Processing completed successfully!\n\ud83d\udcca Results: 2 documents processed, 15 chunks created\n</code></pre></p>"},{"location":"userGuide/documentscli/#separate-chunking","title":"Separate Chunking","text":"<p>Chunk already parsed documents using specific chunking strategies:</p> <pre><code>compileo documents chunk --project-id 1 --document-ids 101,102 --chunk-strategy semantic --semantic-prompt \"Split at natural topic boundaries...\"\n</code></pre> <p>Parameters: Same as process command but focused on chunking only.</p>"},{"location":"userGuide/documentscli/#content-viewing","title":"Content Viewing","text":"<p>View parsed document content with pagination support:</p> <pre><code>compileo documents content 101 --page 1 --page-size 3000\n</code></pre> <p>Parameters: - <code>document_id</code>: Document ID to view (required) - <code>--page</code>: Page number (default: 1) - <code>--page-size</code>: Characters per page (default: 3000) - <code>--output</code>: Save content to file instead of displaying</p> <p>Example Output: <pre><code>Document 101 - Page 1 of 5\nTotal: 14,258 characters, 2,350 words, 280 lines\n[Content displayed here...]\n\nPage 1 of 5\nNext: compileo documents content 101 --page 2\n</code></pre></p>"},{"location":"userGuide/documentscli/#document-management","title":"Document Management","text":""},{"location":"userGuide/documentscli/#list-documents","title":"List Documents","text":"<p>View documents in a project:</p> <pre><code># List all documents in project\ncompileo documents list --project-id 1\n\n# JSON format output\ncompileo documents list --project-id 1 --format json\n</code></pre> <p>Parameters: - <code>--project-id</code>: Filter by project ID - <code>--format</code>: Output format (<code>table</code>, <code>json</code>)</p>"},{"location":"userGuide/documentscli/#delete-documents","title":"Delete Documents","text":"<p>Remove documents and associated chunks:</p> <pre><code># Delete with confirmation prompt\ncompileo documents delete 101\n\n# Delete without confirmation\ncompileo documents delete 102 --confirm\n</code></pre>"},{"location":"userGuide/documentscli/#job-status-monitoring","title":"Job Status Monitoring","text":"<p>Check the status of upload, parsing, or processing jobs:</p> <pre><code># Check upload status\ncompileo documents status --job-id doc_upload_12345 --type upload\n\n# Check processing status\ncompileo documents status --job-id process_job_12345 --type process\n</code></pre> <p>Parameters: - <code>--job-id</code>: Job ID to check (required) - <code>--type</code>: Job type (<code>upload</code>, <code>process</code>, <code>parse</code>)</p> <p>Example Output: <pre><code>Job Status: COMPLETED\nProgress: 100%\nCurrent Step: Processing complete\nDocuments Processed: 2\nTotal Chunks Created: 15\n</code></pre></p>"},{"location":"userGuide/documentscli/#pdf-preprocessing","title":"PDF Preprocessing","text":"<p>Split large PDFs into smaller chunks for better processing:</p> <pre><code>compileo documents split-pdf large_document.pdf --pages-per-split 200 --overlap-pages 1\n</code></pre> <p>Parameters: - <code>pdf_path</code>: Path to PDF file (required) - <code>--pages-per-split</code>: Pages per split (default: 200) - <code>--overlap-pages</code>: Overlapping pages between splits (default: 1)</p>"},{"location":"userGuide/documentscli/#single-file-processing","title":"Single File Processing","text":"<p>Process individual files directly without pre-uploading:</p> <pre><code>compileo process document.pdf --project-id 1 --parser gemini --chunk-strategy character --character-chunk-size 500\n</code></pre> <p>Note: This command combines upload, parsing, and chunking into a single operation.</p>"},{"location":"userGuide/documentscli/#batch-processing-examples","title":"Batch Processing Examples","text":""},{"location":"userGuide/documentscli/#automated-document-pipeline","title":"Automated Document Pipeline","text":"<pre><code>#!/bin/bash\n# Complete document processing pipeline\n\nPROJECT_ID=1\nINPUT_DIR=\"./documents\"\n\necho \"\ud83d\ude80 Starting automated document processing...\"\n\n# Upload all documents\necho \"\ud83d\udce4 Uploading documents...\"\nJOB_ID=$(compileo documents upload --project-id $PROJECT_ID $INPUT_DIR/*.* | grep \"Job ID:\" | cut -d' ' -f4)\n\n# Wait for upload completion\necho \"\u23f3 Waiting for upload...\"\nwhile true; do\n    STATUS=$(compileo documents status --job-id $JOB_ID --type upload | grep \"Status:\" | cut -d' ' -f3)\n    if [ \"$STATUS\" = \"COMPLETED\" ]; then\n        break\n    fi\n    sleep 2\ndone\n\n# Get document IDs\necho \"\ud83d\udccb Getting document IDs...\"\nDOC_IDS=$(compileo documents list --project-id $PROJECT_ID --format json | jq -r '.documents[].id' | tr '\\n' ',' | sed 's/,$//')\n\nif [ -z \"$DOC_IDS\" ]; then\n    echo \"\u274c No documents found\"\n    exit 1\nfi\n\necho \"\ud83d\udcc4 Processing documents: $DOC_IDS\"\n\n# Process with semantic chunking\ncompileo documents process \\\n    --project-id $PROJECT_ID \\\n    --document-ids $DOC_IDS \\\n    --parser gemini \\\n    --chunk-strategy semantic \\\n    --semantic-prompt \"Split this document at natural section boundaries, ensuring each chunk contains complete information.\"\n\necho \"\u2705 Pipeline completed successfully!\"\n</code></pre>"},{"location":"userGuide/documentscli/#quality-assurance-workflow","title":"Quality Assurance Workflow","text":"<pre><code>#!/bin/bash\n# Quality assurance document processing\n\nPROJECT_ID=2\n\n# Process with different strategies for comparison\necho \"\ud83d\udd2c Running quality assurance tests...\"\n\n# Strategy 1: Character-based\ncompileo documents process \\\n    --project-id $PROJECT_ID \\\n    --document-ids 201,202 \\\n    --chunk-strategy character \\\n    --character-chunk-size 500 \\\n    --character-overlap 50\n\n# Strategy 2: Semantic-based\ncompileo documents process \\\n    --project-id $PROJECT_ID \\\n    --document-ids 203,204 \\\n    --chunk-strategy semantic \\\n    --semantic-prompt \"Split at meaningful topic boundaries.\"\n\n# Compare results\necho \"\ud83d\udcca Comparison complete. Check results in the GUI.\"\n</code></pre>"},{"location":"userGuide/documentscli/#performance-optimization","title":"Performance Optimization","text":""},{"location":"userGuide/documentscli/#large-document-sets","title":"Large Document Sets","text":"<pre><code># Use character chunking for speed\ncompileo documents process \\\n    --project-id 1 \\\n    --document-ids 101,102,103,104,105 \\\n    --chunk-strategy character \\\n    --character-chunk-size 1000 \\\n    --character-overlap 100\n\n# Process in smaller batches\ncompileo documents process --project-id 1 --document-ids 101,102 --chunk-strategy semantic --semantic-prompt \"...\"\ncompileo documents process --project-id 1 --document-ids 103,104 --chunk-strategy semantic --semantic-prompt \"...\"\n</code></pre>"},{"location":"userGuide/documentscli/#memory-management","title":"Memory Management","text":"<pre><code># Smaller chunks for memory-constrained environments\ncompileo documents process \\\n    --project-id 1 \\\n    --document-ids 101 \\\n    --chunk-strategy character \\\n    --character-chunk-size 256 \\\n    --character-overlap 25\n\n# Use efficient parsers\ncompileo documents process \\\n    --project-id 1 \\\n    --document-ids 101 \\\n    --parser pypdf \\\n    --chunk-strategy character \\\n    --character-chunk-size 500\n</code></pre>"},{"location":"userGuide/documentscli/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/documentscli/#common-issues","title":"Common Issues","text":"<p>Invalid Document IDs: <pre><code># Error: Document IDs not found\ncompileo documents process --project-id 1 --document-ids 999\n# Solution: Check available documents\ncompileo documents list --project-id 1\n</code></pre></p> <p>Unsupported File Types: <pre><code># Error: Unsupported file type\ncompileo documents upload --project-id 1 document.exe\n# Solution: Use supported formats (PDF, DOCX, TXT, MD, CSV, JSON, XML)\n</code></pre></p> <p>API Key Issues: <pre><code># Error: AI service authentication failed\ncompileo documents process --project-id 1 --document-ids 101 --parser gemini\n# Solution: Ensure API keys are configured in settings\n</code></pre></p> <p>Large File Processing: <pre><code># For very large PDFs, split first\ncompileo documents split-pdf large_document.pdf --pages-per-split 100\n# Then process individual splits\n</code></pre></p>"},{"location":"userGuide/documentscli/#integration-with-scripts","title":"Integration with Scripts","text":""},{"location":"userGuide/documentscli/#python-automation","title":"Python Automation","text":"<pre><code>import subprocess\nimport json\nimport time\n\ndef process_documents_batch(project_id, file_paths, chunk_strategy=\"character\", **kwargs):\n    \"\"\"Process a batch of documents with error handling.\"\"\"\n\n    # Upload files\n    upload_cmd = [\"compileo\", \"documents\", \"upload\", \"--project-id\", str(project_id)] + file_paths\n    upload_result = subprocess.run(upload_cmd, capture_output=True, text=True)\n\n    if upload_result.returncode != 0:\n        raise Exception(f\"Upload failed: {upload_result.stderr}\")\n\n    # Extract job ID from output\n    job_id = None\n    for line in upload_result.stdout.split('\\n'):\n        if \"Job ID:\" in line:\n            job_id = line.split(\"Job ID:\")[1].strip()\n            break\n\n    if not job_id:\n        raise Exception(\"Could not extract job ID from upload output\")\n\n    # Wait for upload completion\n    while True:\n        status_cmd = [\"compileo\", \"documents\", \"status\", \"--job-id\", job_id, \"--type\", \"upload\"]\n        status_result = subprocess.run(status_cmd, capture_output=True, text=True)\n\n        if \"COMPLETED\" in status_result.stdout:\n            break\n        elif \"FAILED\" in status_result.stdout:\n            raise Exception(\"Upload job failed\")\n\n        time.sleep(2)\n\n    # Get document IDs\n    list_cmd = [\"compileo\", \"documents\", \"list\", \"--project-id\", str(project_id), \"--format\", \"json\"]\n    list_result = subprocess.run(list_cmd, capture_output=True, text=True)\n\n    documents = json.loads(list_result.stdout)[\"documents\"]\n    doc_ids = [str(doc[\"id\"]) for doc in documents]\n\n    # Process documents\n    process_cmd = [\n        \"compileo\", \"documents\", \"process\",\n        \"--project-id\", str(project_id),\n        \"--document-ids\", \",\".join(doc_ids),\n        \"--chunk-strategy\", chunk_strategy\n    ]\n\n    # Add strategy-specific parameters\n    if chunk_strategy == \"character\":\n        process_cmd.extend([\n            \"--character-chunk-size\", str(kwargs.get(\"chunk_size\", 500)),\n            \"--character-overlap\", str(kwargs.get(\"overlap\", 50))\n        ])\n    elif chunk_strategy == \"semantic\":\n        process_cmd.extend([\n            \"--semantic-prompt\", kwargs.get(\"prompt\", \"Split at natural boundaries.\")\n        ])\n\n    process_result = subprocess.run(process_cmd, capture_output=True, text=True)\n\n    if process_result.returncode != 0:\n        raise Exception(f\"Processing failed: {process_result.stderr}\")\n\n    return process_result.stdout\n\n# Usage\ntry:\n    result = process_documents_batch(\n        project_id=1,\n        file_paths=[\"doc1.pdf\", \"doc2.pdf\"],\n        chunk_strategy=\"semantic\",\n        prompt=\"Split at chapter boundaries.\"\n    )\n    print(\"Processing completed successfully!\")\n    print(result)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"userGuide/documentscli/#monitoring-and-logging","title":"Monitoring and Logging","text":"<pre><code>import logging\nimport subprocess\nimport time\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef monitor_job_completion(job_id, job_type=\"process\", timeout=300):\n    \"\"\"Monitor job completion with logging.\"\"\"\n\n    start_time = time.time()\n    last_progress = 0\n\n    while time.time() - start_time &lt; timeout:\n        try:\n            cmd = [\"compileo\", \"documents\", \"status\", \"--job-id\", job_id, \"--type\", job_type]\n            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)\n\n            if result.returncode != 0:\n                logger.error(f\"Status check failed: {result.stderr}\")\n                continue\n\n            output = result.stdout\n\n            # Extract progress\n            progress = 0\n            if \"Progress:\" in output:\n                progress_line = [line for line in output.split('\\n') if \"Progress:\" in line]\n                if progress_line:\n                    progress = int(progress_line[0].split(\"Progress:\")[1].split(\"%\")[0].strip())\n\n            # Log progress changes\n            if progress != last_progress:\n                logger.info(f\"Job {job_id}: {progress}% complete\")\n                last_progress = progress\n\n            # Check completion\n            if \"COMPLETED\" in output:\n                logger.info(f\"Job {job_id} completed successfully!\")\n                return True\n            elif \"FAILED\" in output:\n                logger.error(f\"Job {job_id} failed!\")\n                return False\n\n        except subprocess.TimeoutExpired:\n            logger.warning(f\"Status check timed out for job {job_id}\")\n        except Exception as e:\n            logger.error(f\"Error monitoring job {job_id}: {e}\")\n\n        time.sleep(5)\n\n    logger.warning(f\"Job {job_id} monitoring timed out after {timeout} seconds\")\n    return False\n\n# Usage\njob_id = \"process_job_12345\"\nif monitor_job_completion(job_id):\n    print(\"Job completed successfully!\")\nelse:\n    print(\"Job failed or timed out\")\n</code></pre> <p>This CLI provides comprehensive document processing capabilities with support for multiple parsing options, chunking strategies, and batch operations suitable for both interactive use and automated workflows.</p>"},{"location":"userGuide/documentsgui/","title":"Documents Module GUI Usage Guide","text":"<p>The Compileo Documents GUI provides an intuitive web interface for document management, including upload, parsing, chunking, and content viewing. This guide covers all GUI features with step-by-step instructions.</p>"},{"location":"userGuide/documentsgui/#accessing-document-processing","title":"Accessing Document Processing","text":"<p>Navigate to the \"\ud83d\udcc4 Document Processing\" page from the main menu. The interface is organized into two main tabs:</p> <ol> <li>\ud83d\udcc4 Parse Documents - Upload and parse documents</li> <li>\u2702\ufe0f Configure &amp; Chunk Documents - Configure chunking and process parsed documents</li> </ol>"},{"location":"userGuide/documentsgui/#project-selection","title":"Project Selection","text":"<p>Before working with documents, select a project from the dropdown at the top of the page:</p> <ul> <li>Project Selection: Choose from available projects</li> <li>Auto-save: Your selection is remembered across sessions</li> <li>Validation: System prevents operations without a valid project</li> </ul>"},{"location":"userGuide/documentsgui/#tab-1-parse-documents","title":"Tab 1: Parse Documents","text":""},{"location":"userGuide/documentsgui/#document-upload","title":"Document Upload","text":"<ol> <li>File Upload Area:</li> <li>Click \"Browse files\" or drag-and-drop files</li> <li>Supported formats: PDF, DOCX, TXT, MD, CSV, JSON, XML</li> <li> <p>Multiple files: Upload several documents at once</p> </li> <li> <p>PDF Splitting Configuration (appears after file selection):</p> <ul> <li>Pages per Split: Number of pages per PDF chunk (default: 5, recommended: 5-10)</li> <li>Overlap Pages: Overlapping pages between chunks for context continuity (default: 0)</li> <li>Purpose: Automatically splits large PDFs to optimize AI parsing performance</li> <li>Automatic: PDFs are split when <code>total_pages &gt; pages_per_split</code></li> </ul> </li> </ol>"},{"location":"userGuide/documentsgui/#parser-selection","title":"Parser Selection","text":"<p>Choose from available AI parsers: - gemini: Google's Gemini models - grok: xAI's Grok models - ollama: Local Ollama models (configurable parameters) - pypdf: Python PDF parser (fast, no API required) - unstructured: Unstructured.io parser - huggingface: Hugging Face models - novlm: No-VLM models</p> <p>Ollama Parser Configuration: When using Ollama parsers, you can fine-tune AI behavior by configuring parameters in Settings \u2192 AI Model Configuration. Available parameters include temperature, repeat penalty, top-p, top-k, and num_predict for optimal parsing results.</p>"},{"location":"userGuide/documentsgui/#document-selection","title":"Document Selection","text":"<p>Existing Documents: - View all documents in the selected project - Status indicators:   - \u2705 Parsed: Document successfully converted to markdown   - \ud83d\udcc4 Uploaded: Document uploaded but not yet parsed - Selection: Check boxes to select documents for parsing - Delete: Click \ud83d\uddd1\ufe0f to remove documents</p> <p>Upload + Parse: - Upload new files AND select existing documents simultaneously - System processes both in a single operation</p>"},{"location":"userGuide/documentsgui/#parsing-action","title":"Parsing Action","text":"<ol> <li>Parse Button: Shows count of documents to be processed</li> <li>Progress: Real-time status updates during parsing</li> <li>Results: Success/failure notifications with job IDs</li> <li>Background Processing: Long operations run asynchronously</li> </ol>"},{"location":"userGuide/documentsgui/#tab-2-configure-chunk-documents","title":"Tab 2: Configure &amp; Chunk Documents","text":""},{"location":"userGuide/documentsgui/#chunking-strategy-selection","title":"Chunking Strategy Selection","text":"<p>Choose from five chunking strategies:</p>"},{"location":"userGuide/documentsgui/#1-character-based-chunking","title":"1. Character-Based Chunking","text":"<ul> <li>Best for: Speed and predictability</li> <li>Parameters:</li> <li>Chunk Size: Characters per chunk (100-4000)</li> <li>Overlap: Overlapping characters (0-500)</li> <li>Use cases: Large document sets, batch processing</li> </ul>"},{"location":"userGuide/documentsgui/#2-token-based-chunking","title":"2. Token-Based Chunking","text":"<ul> <li>Best for: LLM compatibility</li> <li>Parameters:</li> <li>Chunk Size: Tokens per chunk (100-2000)</li> <li>Overlap: Overlapping tokens (0-200)</li> <li>Use cases: Preparing data for language models</li> </ul>"},{"location":"userGuide/documentsgui/#3-semantic-chunking","title":"3. Semantic Chunking","text":"<ul> <li>Best for: Meaningful content boundaries</li> <li>Parameters:</li> <li>Similarity Threshold: Boundary detection sensitivity (0.1-0.9)</li> <li>Min Chunk Size: Minimum characters per chunk (50-500)</li> <li>Custom Prompt: AI instructions for boundary detection</li> <li>Use cases: Research papers, technical documentation</li> </ul>"},{"location":"userGuide/documentsgui/#4-delimiter-based-chunking","title":"4. Delimiter-Based Chunking","text":"<ul> <li>Best for: Structured documents with known separators</li> <li>Parameters:</li> <li>Delimiters: Select from predefined patterns</li> <li>Custom delimiters: <code>\\n\\n</code>, <code>\\n</code>, <code>.</code>, <code>!</code>, <code>?</code></li> <li>Use cases: Markdown files, structured text</li> </ul>"},{"location":"userGuide/documentsgui/#5-schema-based-chunking","title":"5. Schema-Based Chunking","text":"<ul> <li>Best for: Complex splitting rules</li> <li>Parameters:</li> <li>Schema JSON: Custom rules combining patterns and delimiters</li> <li>Use cases: Proprietary formats, complex document structures</li> </ul>"},{"location":"userGuide/documentsgui/#ai-model-selection","title":"AI Model Selection","text":"<p>Choose the AI model for intelligent chunking: - gemini: Google's Gemini (recommended) - grok: xAI's Grok - ollama: Local models</p>"},{"location":"userGuide/documentsgui/#configuration-mode","title":"Configuration Mode","text":""},{"location":"userGuide/documentsgui/#manual-configuration","title":"Manual Configuration","text":"<ul> <li>Set all parameters manually</li> <li>Full control over chunking behavior</li> <li>Suitable for experienced users</li> </ul>"},{"location":"userGuide/documentsgui/#ai-assisted-configuration","title":"AI-Assisted Configuration","text":"<ul> <li>Smart Recommendations: AI analyzes your documents and suggests optimal settings</li> <li>User Instructions: Describe your chunking goals</li> <li>Examples: Provide sample text from your documents</li> <li>Automatic Optimization: System recommends strategy, size, and overlap</li> </ul>"},{"location":"userGuide/documentsgui/#document-selection-for-chunking","title":"Document Selection for Chunking","text":"<p>Available Documents: Only parsed documents (status: \u2705) can be chunked</p> <p>Multi-file Selection: - Check boxes to select multiple documents - Grid layout for efficient selection - Real-time count of selected documents</p>"},{"location":"userGuide/documentsgui/#advanced-features-multi-part-documents","title":"Advanced Features: Multi-part Documents","text":"<p>Split Document Handling: - System detects documents split into multiple parts - File Selection Dropdown: Choose which part to chunk - Metadata Display: Page ranges, overlap information - Content Preview: View content before chunking</p> <p>Manifest Support: - Automatic detection of document manifests - Page range information for split files - Overlap visualization between chunks</p>"},{"location":"userGuide/documentsgui/#content-preview","title":"Content Preview","text":"<p>Before Chunking: - Preview parsed content for selected files - File-Specific Viewing: Select individual parsed file chunks from multi-part documents - Full Content Access: View complete content of selected parsed files (up to 10,000+ characters) - Pagination Support: Navigate through large content with 10,000-character pages - Copy-to-clipboard functionality for examples - Verify content integrity before processing</p>"},{"location":"userGuide/documentsgui/#chunking-execution","title":"Chunking Execution","text":"<ol> <li>Chunk Button: Initiates chunking process</li> <li>Progress Monitoring: Real-time status updates</li> <li>Background Processing: Asynchronous execution for large jobs</li> <li>Results Display: Success metrics and chunk counts</li> </ol>"},{"location":"userGuide/documentsgui/#job-monitoring","title":"Job Monitoring","text":""},{"location":"userGuide/documentsgui/#processing-status-display","title":"Processing Status Display","text":"<p>Active Jobs: - Real-time progress bars - Current operation status - Estimated completion time - Error notifications</p> <p>Job History: - Previous processing jobs - Success/failure status - Performance metrics - Detailed logs</p>"},{"location":"userGuide/documentsgui/#status-indicators","title":"Status Indicators","text":"<ul> <li>\u23f3 Pending: Job queued for processing</li> <li>\ud83d\udd04 Running: Currently being processed</li> <li>\u2705 Completed: Successfully finished</li> <li>\u274c Failed: Processing errors occurred</li> </ul>"},{"location":"userGuide/documentsgui/#document-management","title":"Document Management","text":""},{"location":"userGuide/documentsgui/#viewing-document-list","title":"Viewing Document List","text":"<p>Project Documents: - All documents in selected project - Status indicators for each document - File sizes and upload dates - Quick actions (delete, view content)</p>"},{"location":"userGuide/documentsgui/#document-content-viewer","title":"Document Content Viewer","text":"<p>Parsed Content Access: - Click document name to view parsed markdown - Pagination support for large documents - Search functionality within content - Export options (copy, download)</p>"},{"location":"userGuide/documentsgui/#document-deletion","title":"Document Deletion","text":"<p>Safe Deletion: - Confirmation prompts prevent accidents - Associated chunks automatically removed - File system cleanup included - Database record removal</p>"},{"location":"userGuide/documentsgui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/documentsgui/#1-document-preparation","title":"1. Document Preparation","text":"<p>File Organization: - Use consistent naming conventions - Group related documents in same project - Check file sizes before upload (large PDFs may need splitting)</p> <p>Format Selection: - PDFs: Use for scanned documents or complex layouts - DOCX/TXT: Use for text-heavy content - MD: Use for already structured content</p>"},{"location":"userGuide/documentsgui/#2-parsing-strategy","title":"2. Parsing Strategy","text":"<p>Parser Selection: - pypdf: Fast, no API costs, good for simple PDFs - gemini/grok: Best quality, handles complex layouts - ollama: Local processing, privacy-focused</p> <p>Batch Processing: - Parse multiple documents together for efficiency - Monitor job status for large batches - Use pagination settings for oversized documents</p>"},{"location":"userGuide/documentsgui/#3-chunking-optimization","title":"3. Chunking Optimization","text":"<p>Strategy Selection: - Character: Fast processing, predictable results - Semantic: Quality chunks, slower processing - Schema: Precise control, requires expertise</p> <p>Parameter Tuning: - Start with defaults, adjust based on results - Use AI-assisted configuration for optimization - Test on sample documents before full processing</p>"},{"location":"userGuide/documentsgui/#4-quality-assurance","title":"4. Quality Assurance","text":"<p>Content Verification: - Always preview parsed content before chunking - Check for parsing errors or missing content - Validate chunk boundaries make sense</p> <p>Performance Monitoring: - Track processing times for different strategies - Monitor API usage and costs - Log successful configurations for reuse</p>"},{"location":"userGuide/documentsgui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/documentsgui/#common-issues","title":"Common Issues","text":"<p>Upload Failures: - Check file size limits - Verify supported formats - Ensure write permissions on storage directory</p> <p>Parsing Errors: - Verify API keys are configured - Check document isn't corrupted - Try different parser if one fails</p> <p>Chunking Problems: - Ensure documents are parsed first - Check chunking parameters are valid - Verify AI model is available</p> <p>Performance Issues: - Use smaller batch sizes for large documents - Switch to faster parsers (pypdf) for bulk processing - Monitor system resources during processing</p>"},{"location":"userGuide/documentsgui/#advanced-features","title":"Advanced Features","text":""},{"location":"userGuide/documentsgui/#split-pdf-management","title":"Split PDF Management","text":"<p>Large Document Handling: - Automatic splitting for oversized PDFs - Configurable page ranges and overlaps - Manifest tracking for split relationships - Selective chunking of individual splits</p>"},{"location":"userGuide/documentsgui/#custom-schema-chunking","title":"Custom Schema Chunking","text":"<p>Rule Definition: <pre><code>{\n  \"rules\": [\n    {\"type\": \"pattern\", \"value\": \"^## \"},\n    {\"type\": \"delimiter\", \"value\": \"\\n\\n\"}\n  ],\n  \"combine\": \"any\"\n}\n</code></pre></p> <p>Advanced Patterns: - Regex patterns for headers and sections - Multiple delimiter combinations - Hierarchical rule processing</p>"},{"location":"userGuide/documentsgui/#api-integration","title":"API Integration","text":"<p>Programmatic Access: - All GUI features available via REST API - Batch processing scripts - Integration with external workflows - Automated document pipelines</p> <p>This GUI provides a complete document processing workflow from upload to chunking, with both manual control and AI-assisted optimization for maximum efficiency and quality.</p>"},{"location":"userGuide/extractionapi/","title":"Extraction API in Compileo","text":""},{"location":"userGuide/extractionapi/#overview","title":"Overview","text":"<p>The Compileo Extraction API provides endpoints for performing content extraction jobs on processed documents using taxonomy-based classification. This API enables asynchronous extraction of structured information from documents based on predefined taxonomies with support for both Named Entity Recognition (NER) and Whole Text Extraction modes.</p>"},{"location":"userGuide/extractionapi/#base-url-apiv1","title":"Base URL: <code>/api/v1</code>","text":""},{"location":"userGuide/extractionapi/#key-features","title":"Key Features","text":"<ul> <li>Dual Extraction Modes: </li> <li>NER: Extract specific entities (names, terms, concepts) from text chunks</li> <li>Whole Text: Extract complete relevant text portions classified into taxonomy categories</li> <li>Selective Taxonomy-Based Extraction: Extract content only from user-selected taxonomy categories</li> <li>Multi-Model AI Support: Choose between Grok, Gemini, Ollama, and OpenAI AI models for extraction</li> <li>Contextual Extraction: Only extracts from child categories when parent context is present in the text</li> <li>Document-Wide Extraction: Processes all chunks for selected categories without contextual filtering</li> <li>Relationship Inference: Automatically discover relationships between co-occurring entities</li> <li>High-Precision Validation: Strict subtractive validation stage that programmatically filters out hallucinations.</li> <li>Snippet Deduplication: Programmatic deduplication of extracted segments to ensure unique results.</li> <li>Progress Tracking: Real-time progress monitoring with detailed step updates</li> <li>Result Organization: Results organized by chunk with entity/text details and confidence scores. Optimized JSON schema for downstream processing.</li> <li>Job Management: Full lifecycle management including cancellation and restart</li> </ul>"},{"location":"userGuide/extractionapi/#contextual-extraction-behavior","title":"Contextual Extraction Behavior","text":"<p>The extraction system implements intelligent contextual filtering to ensure accuracy and prevent false positives:</p>"},{"location":"userGuide/extractionapi/#how-contextual-extraction-works","title":"How Contextual Extraction Works","text":"<ol> <li> <p>Parent-Child Relationship Analysis: The system analyzes taxonomy hierarchies to understand parent-child relationships between categories.</p> </li> <li> <p>Context Relevance Check: For each text chunk, the system determines if the parent categories of selected child categories are relevant to the content.</p> </li> <li> <p>Selective Extraction: Child categories are only processed for extraction if their parent context is present in the text (explicitly or implied).</p> </li> <li> <p>Empty Results for Irrelevant Content: If a parent category is not relevant to a chunk, all its child categories return empty results rather than extracting unrelated content.</p> </li> </ol>"},{"location":"userGuide/extractionapi/#example-behavior","title":"Example Behavior","text":"<p>Selected Categories: \"Associated Conditions and Prevention\", \"Diagnosis and Pathophysiology\"</p> <p>Text Chunk: \"The patient presented with chest pain and shortness of breath. ECG showed ST elevation.\"</p> <ul> <li>Analysis: The text discusses cardiac symptoms but does not mention \"Metabolic Syndrome\" (parent of \"Associated Conditions\") or \"Mitral Regurgitation\" (parent of \"Diagnosis\")</li> <li>Result: Both selected categories return empty results, ensuring no false extractions of cardiac content into metabolic syndrome categories</li> </ul> <p>Text Chunk: \"Metabolic syndrome patients often develop associated conditions like hypertension and diabetes.\"</p> <ul> <li>Analysis: The text explicitly discusses \"Metabolic Syndrome\" and its associated conditions</li> <li>Result: \"Associated Conditions and Prevention\" extracts relevant content; \"Diagnosis and Pathophysiology\" returns empty (no diagnosis content present)</li> </ul>"},{"location":"userGuide/extractionapi/#extraction-modes","title":"Extraction Modes","text":"<p>The API supports two extraction modes that control how content is processed:</p>"},{"location":"userGuide/extractionapi/#1-contextual-extraction-default","title":"1. Contextual Extraction (Default)","text":"<ul> <li>Behavior: Only extracts from child categories when parent context is present in the text</li> <li>Purpose: Prevents false positives by ensuring child categories are only extracted when parent context is relevant</li> <li>Use Case: Recommended for most scenarios requiring high precision</li> <li>Trade-off: May miss valid extractions in edge cases where relevant content doesn't explicitly mention parent topics</li> </ul>"},{"location":"userGuide/extractionapi/#2-document-wide-extraction","title":"2. Document-Wide Extraction","text":"<ul> <li>Behavior: Processes ALL chunks in the document regardless of content relevance to parent categories</li> <li>Purpose: Maximizes extraction coverage by attempting extraction on every chunk</li> <li>Use Case: When you want maximum extraction coverage and are willing to review more results</li> <li>Trade-off: Higher risk of false positives but potentially more comprehensive results</li> </ul>"},{"location":"userGuide/extractionapi/#choosing-an-extraction-mode","title":"Choosing an Extraction Mode","text":"<ul> <li>Use Contextual Mode when:</li> <li>You need high-precision results with minimal false positives</li> <li>You're working with well-structured taxonomies</li> <li> <p>You want to avoid reviewing irrelevant extractions</p> </li> <li> <p>Use Document-Wide Mode when:</p> </li> <li>You want maximum extraction coverage</li> <li>You're willing to manually review and filter results</li> <li>The taxonomy structure may not perfectly match content organization</li> <li>You're doing exploratory extraction to discover all possible content</li> </ul>"},{"location":"userGuide/extractionapi/#api-endpoints","title":"API Endpoints","text":""},{"location":"userGuide/extractionapi/#1-create-extraction-job","title":"1. Create Extraction Job","text":"<p>Submits a new selective extraction job for processing.</p> <ul> <li>Endpoint: <code>POST /extraction/</code></li> <li> <p>Description: Creates and queues a new extraction job with specified taxonomy and categories.</p> </li> <li> <p>Request Body: <pre><code>{\n  \"taxonomy_id\": 123,\n  \"selected_categories\": [\"category_id_1\", \"category_id_2\"],\n  \"parameters\": {\n    \"extraction_depth\": 3,\n    \"confidence_threshold\": 0.5,\n    \"batch_size\": 10,\n    \"max_chunks\": 1000\n  },\n  \"initial_classifier\": \"grok\",\n  \"enable_validation_stage\": false,\n  \"validation_classifier\": null,\n  \"only_validated\": false,\n  \"extraction_type\": \"ner\",\n  \"extraction_mode\": \"contextual\"\n}\n</code></pre></p> </li> </ul>"},{"location":"userGuide/extractionapi/#2-delete-extraction-job","title":"2. Delete Extraction Job","text":"<p>Permanently deletes an extraction job and all associated results from the filesystem and database.</p> <ul> <li>Endpoint: <code>DELETE /extraction/{job_id}/delete</code></li> <li> <p>Description: Removes the specified extraction job, its results, and cleans up all associated files and database entries.</p> </li> <li> <p>Parameters:</p> </li> <li> <p><code>job_id</code> (path): The unique identifier of the extraction job to delete</p> </li> <li> <p>Response:</p> </li> <li>200 OK: Job successfully deleted</li> <li>404 Not Found: Job not found</li> <li> <p>500 Internal Server Error: Deletion failed</p> </li> <li> <p>Example: <pre><code>curl -X DELETE \"http://localhost:8000/api/v1/extraction/123/delete\"\n</code></pre></p> </li> </ul>"},{"location":"userGuide/extractioncli/","title":"Extraction CLI Commands","text":"<p>This guide covers the command-line interface for creating and managing extraction jobs in Compileo.</p>"},{"location":"userGuide/extractioncli/#overview","title":"Overview","text":"<p>The extraction CLI provides complete parity with the extraction API, allowing users to create extraction jobs through command-line interface with identical functionality and parameters.</p>"},{"location":"userGuide/extractioncli/#commands","title":"Commands","text":""},{"location":"userGuide/extractioncli/#extraction-create","title":"<code>extraction create</code>","text":"<p>Creates a new extraction job using a taxonomy to extract content from processed documents.</p>"},{"location":"userGuide/extractioncli/#syntax","title":"Syntax","text":"<pre><code>compileo extraction create --project-id &lt;project_id&gt; --taxonomy-id &lt;taxonomy_id&gt; --selected-categories \"category1,category2\" --effective-classifier &lt;model&gt; [options]\n</code></pre>"},{"location":"userGuide/extractioncli/#required-parameters","title":"Required Parameters","text":"<ul> <li><code>--project-id</code> (integer): The ID of the project containing the data</li> <li><code>--taxonomy-id</code> (integer): The ID of the taxonomy to use for extraction</li> <li><code>--selected-categories</code> (string): Comma-separated list of category names to extract</li> <li><code>--initial-classifier</code> (string): The AI model for the initial classification stage (e.g., <code>grok</code>, <code>gemini</code>, <code>ollama</code>, <code>openai</code>).</li> <li><code>--extraction-type</code> (string, default: \"ner\"): Type of extraction: <code>'ner'</code> for named entities, <code>'whole_text'</code> for complete text portions.</li> </ul>"},{"location":"userGuide/extractioncli/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>--extraction-mode</code> (string, default: \"contextual\"): Extraction mode: <code>'contextual'</code> or <code>'document-wide'</code>.</li> <li><code>--enable-validation-stage</code>: Enable validation stage for improved accuracy</li> <li><code>--validation-classifier</code> (string): Separate classifier for validation phase</li> <li><code>--confidence-threshold</code> (float, default: 0.5): Minimum confidence score (0.0-1.0)</li> <li><code>--max-chunks</code> (integer): Maximum number of chunks to process</li> </ul>"},{"location":"userGuide/extractioncli/#available-ai-models","title":"Available AI Models","text":"<ul> <li>Gemini: <code>gemini/gemini-1.5-flash</code>, <code>gemini/gemini-1.5-pro</code></li> <li>Ollama: <code>ollama/llama3.1</code>, <code>ollama/llama3.2</code>, <code>ollama/mistral</code></li> <li>Grok: <code>grok/grok-1</code>, <code>grok/grok-beta</code></li> <li>OpenAI: <code>openai/gpt-4o</code>, <code>openai/gpt-4-turbo</code>, <code>openai/gpt-3.5-turbo</code></li> </ul>"},{"location":"userGuide/extractioncli/#examples","title":"Examples","text":"<p>Basic extraction with Gemini: <pre><code>compileo extraction create \\\n  --project-id 1 \\\n  --taxonomy-id 5 \\\n  --selected-categories \"diagnosis,treatment\" \\\n  --effective-classifier \"gemini/gemini-1.5-flash\"\n</code></pre></p> <p>Advanced extraction with validation: <pre><code>compileo extraction create \\\n  --project-id 1 \\\n  --taxonomy-id 5 \\\n  --selected-categories \"symptoms,causes,effects\" \\\n  --effective-classifier \"ollama/llama3.1\" \\\n  --enable-validation-stage \\\n  --validation-classifier \"gemini/gemini-1.5-pro\" \\\n  --confidence-threshold 0.7 \\\n  --max-chunks 1000\n</code></pre></p> <p>Limited processing for testing: <pre><code>compileo extraction create \\\n  --project-id 1 \\\n  --taxonomy-id 5 \\\n  --selected-categories \"all\" \\\n  --effective-classifier \"gemini/gemini-1.5-flash\" \\\n  --max-chunks 100\n</code></pre></p> <p>Document-wide extraction for maximum coverage: <pre><code>compileo extraction create \\\n  --project-id 1 \\\n  --taxonomy-id 5 \\\n  --selected-categories \"diagnosis,treatment\" \\\n  --effective-classifier \"ollama/llama3.1\" \\\n  --extraction-mode document-wide \\\n  --confidence-threshold 0.3\n</code></pre></p>"},{"location":"userGuide/extractioncli/#output","title":"Output","text":"<p>Success Response: <pre><code>\ud83d\ude80 Creating extraction job for project 1\n\ud83d\udccb Taxonomy ID: 5\n\ud83c\udff7\ufe0f Categories: diagnosis, treatment\n\ud83e\udd16 Classifier: gemini/gemini-1.5-flash\n\n\u2705 Extraction job created successfully!\n\ud83d\udccb Job ID: 12345\n\ud83d\udd0d Monitor progress: compileo jobs poll 12345\n\ud83d\udcca Check status: compileo jobs status 12345\n</code></pre></p>"},{"location":"userGuide/extractioncli/#job-monitoring","title":"Job Monitoring","text":"<p>After creating an extraction job, you can monitor its progress using the job management commands:</p> <pre><code># Check job status\ncompileo jobs status 12345\n\n# Poll for completion (long-running)\ncompileo jobs poll 12345\n\n# Stream real-time updates\ncompileo jobs stream 12345\n</code></pre>"},{"location":"userGuide/extractioncli/#error-handling","title":"Error Handling","text":"<p>The CLI provides clear error messages for common issues:</p> <ul> <li>Invalid project ID: \"Project with ID X not found\"</li> <li>Invalid taxonomy ID: \"Taxonomy with ID Y not found\"</li> <li>Invalid categories: \"Category 'invalid' not found in taxonomy\"</li> <li>Model unavailable: \"AI model 'invalid/model' is not available\"</li> <li>Connection errors: \"Failed to connect to API server\"</li> </ul>"},{"location":"userGuide/extractioncli/#integration-with-other-commands","title":"Integration with Other Commands","text":""},{"location":"userGuide/extractioncli/#complete-workflow-example","title":"Complete Workflow Example","text":"<pre><code># 1. Create project\ncompileo projects create --name \"Medical Study\" --description \"Clinical trial data\"\n\n# 2. Upload documents\ncompileo documents upload --project-id 1 --files \"study1.pdf,study2.pdf\"\n\n# 3. Parse documents\ncompileo documents parse --project-id 1 --document-ids \"1,2\"\n\n# 4. Chunk documents\ncompileo documents chunk --project-id 1 --document-ids \"1,2\" --chunker \"gemini\"\n\n# 5. Generate taxonomy\ncompileo taxonomy generate --project-id 1 --name \"Medical Conditions\" --documents \"1,2\"\n\n# 6. Create extraction job\ncompileo extraction create \\\n  --project-id 1 \\\n  --taxonomy-id 1 \\\n  --selected-categories \"diagnosis,treatment,outcome\" \\\n  --effective-classifier \"gemini/gemini-1.5-flash\"\n\n# 7. Monitor and retrieve results\ncompileo jobs status &lt;job_id&gt;\ncompileo extraction results &lt;job_id&gt;\n</code></pre>"},{"location":"userGuide/extractioncli/#configuration","title":"Configuration","text":"<p>The CLI uses the same configuration as the API and GUI:</p> <ul> <li>API Keys: Retrieved from GUI settings database</li> <li>Model Selection: Based on available AI providers</li> <li>Validation: Same parameter validation as API endpoints</li> </ul>"},{"location":"userGuide/extractioncli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/extractioncli/#common-issues","title":"Common Issues","text":"<p>\"Model not available\" - Check that the AI model is properly configured in settings - Verify API keys are set for the selected provider - Try a different model from the same provider</p> <p>\"Taxonomy not found\" - Verify the taxonomy ID exists for the project - Check that taxonomy generation completed successfully - Use <code>compileo taxonomy list --project-id X</code> to see available taxonomies</p> <p>\"No chunks found\" - Ensure documents have been parsed and chunked - Check that chunking completed successfully - Verify project contains processed documents</p>"},{"location":"userGuide/extractioncli/#getting-help","title":"Getting Help","text":"<pre><code># Show command help\ncompileo extraction create --help\n\n# List all extraction commands\ncompileo extraction --help\n\n# General CLI help\ncompileo --help\n</code></pre>"},{"location":"userGuide/extractioncli/#api-parity","title":"API Parity","text":"<p>The CLI provides complete feature parity with the <code>/api/v1/extraction/</code> endpoint:</p> <ul> <li>Same parameters: All API parameters are supported</li> <li>Same validation: Identical input validation rules</li> <li>Same processing: Uses the same backend extraction pipeline</li> <li>Same results: Produces identical extraction results</li> </ul> <p>This ensures consistent behavior whether using the API, GUI, or CLI interfaces.</p>"},{"location":"userGuide/extractiongui/","title":"Advanced Entity Extraction &amp; Dataset Generation in Compileo GUI","text":""},{"location":"userGuide/extractiongui/#overview","title":"Overview","text":"<p>The Compileo GUI provides a sophisticated, unified interface for advanced entity extraction, relationship inference, and automated Q&amp;A dataset generation. The extraction system transforms unstructured text into structured datasets through AI-powered entity recognition and intelligent relationship discovery.</p>"},{"location":"userGuide/extractiongui/#key-capabilities","title":"Key Capabilities","text":""},{"location":"userGuide/extractiongui/#entity-extraction","title":"Entity Extraction","text":"<ul> <li>Dual Extraction Modes: Supports both Named Entity Recognition (NER) and Whole Text Extraction.</li> <li>NER: Extracts specific entities (names, terms, concepts) from text chunks.</li> <li>Whole Text: Extracts complete relevant text portions classified into taxonomy categories.</li> <li>High-Precision Validation: Strict subtractive validation stage that programmatically filters out hallucinations and discovery errors.</li> <li>Snippet Deduplication: Programmatic deduplication of extracted segments to ensure unique and clean results.</li> <li>Flexible Extraction Modes: Choose between Contextual and Document-Wide processing.</li> <li>Contextual Extraction: Only extracts from child categories when parent context is present in the text, preventing false positives.</li> <li>Document-Wide Extraction: Processes all chunks for selected categories regardless of contextual relevance, maximizing coverage.</li> <li>AI-Powered Recognition: Extract specific entities from text using Grok, Gemini, or Ollama models</li> <li>Taxonomy Integration: Work with hierarchical taxonomies for precise categorization</li> <li>Multi-Category Support: Extract entities across multiple taxonomy categories simultaneously</li> <li>Confidence Scoring: Quality assessment for all extracted entities</li> </ul>"},{"location":"userGuide/extractiongui/#relationship-inference","title":"Relationship Inference","text":"<ul> <li>Automatic Discovery: Identify relationships between co-occurring entities</li> <li>Domain Agnostic: Works across medical, business, legal, and technical domains</li> <li>Confidence Weighting: Quality scoring for relationship strength</li> </ul>"},{"location":"userGuide/extractiongui/#qa-dataset-generation","title":"Q&amp;A Dataset Generation","text":"<ul> <li>Template-Based Creation: Generate question-answer pairs from entity relationships</li> <li>Multiple Formats: Export in JSONL, JSON, and CSV formats</li> <li>Context Preservation: Maintain source relationships and metadata</li> </ul>"},{"location":"userGuide/extractiongui/#accessing-extraction","title":"Accessing Extraction","text":"<ol> <li>Navigate to the Application: Open Compileo in your web browser</li> <li>Select Extraction: Click on \"\ud83d\udd0d Extraction\" in the sidebar</li> <li>Choose Operation: Use the three-tab interface for different workflows</li> </ol>"},{"location":"userGuide/extractiongui/#interface-components","title":"Interface Components","text":""},{"location":"userGuide/extractiongui/#three-tab-structure","title":"Three-Tab Structure","text":"<p>The extraction interface is organized into three main tabs:</p>"},{"location":"userGuide/extractiongui/#run-extraction-tab","title":"\ud83c\udfc3 Run Extraction Tab","text":""},{"location":"userGuide/extractiongui/#project-taxonomy-selection","title":"Project &amp; Taxonomy Selection","text":"<ul> <li>Project Selector: Choose from available projects in \"Project Name (ID: 123)\" format</li> <li>Taxonomy Selector: Pick the taxonomy to use for entity categorization</li> </ul>"},{"location":"userGuide/extractiongui/#ai-model-configuration","title":"AI Model Configuration","text":"<ul> <li>Extraction Type: Select the type of extraction: <code>Named Entity Recognition (NER)</code> or <code>Whole Text Extraction</code>.</li> <li>Extraction Mode: Choose the extraction mode: <code>Contextual Extraction</code> (default) or <code>Document-Wide Extraction</code>.</li> <li>Primary Classifier: Select the AI model (Grok, Gemini, Ollama) for the initial classification stage.</li> <li>Validation Stage: Optionally enable a second AI model for result validation. This stage is subtractive, meaning it verifies existing findings and removes errors but is strictly forbidden from discovering new categories.</li> <li>Validation Classifier: Choose a different AI model for the validation stage if enabled.</li> </ul>"},{"location":"userGuide/extractiongui/#taxonomy-tree-selection","title":"Taxonomy Tree Selection","text":"<ul> <li>Interactive Tree: Expandable taxonomy hierarchy with checkboxes</li> <li>Search Functionality: Filter categories by name or description</li> <li>Batch Controls: Expand/collapse all, select/deselect all visible categories</li> <li>Selection Summary: Shows count of selected categories for extraction</li> </ul>"},{"location":"userGuide/extractiongui/#extraction-parameters","title":"Extraction Parameters","text":"<ul> <li>Depth Control: How deep in taxonomy hierarchy to extract (1-5 levels)</li> <li>Confidence Threshold: Minimum confidence score for results (0.0-1.0)</li> <li>Batch Size: Number of chunks to process per batch (1-100)</li> <li>Max Chunks: Maximum chunks to process (1-10,000)</li> </ul>"},{"location":"userGuide/extractiongui/#job-initiation","title":"Job Initiation","text":"<ul> <li>Start Extraction: Launch extraction job with configured parameters</li> <li>Progress Feedback: Real-time status updates and success notifications</li> </ul>"},{"location":"userGuide/extractiongui/#monitor-jobs-tab","title":"\ud83d\udcca Monitor Jobs Tab","text":""},{"location":"userGuide/extractiongui/#project-selection","title":"Project Selection","text":"<ul> <li>Project Filter: View jobs for specific projects</li> </ul>"},{"location":"userGuide/extractiongui/#job-dashboard","title":"Job Dashboard","text":"<ul> <li>Status Overview: Summary metrics for total, running, completed, and failed jobs</li> <li>Job Cards: Individual cards for each extraction job with:</li> <li>Status Indicators: Color-coded status badges and progress bars</li> <li>Timing Information: Created, started, completed timestamps</li> <li>Expandable Details: Parameters, error messages, and timing breakdowns</li> </ul>"},{"location":"userGuide/extractiongui/#job-management-actions","title":"Job Management Actions","text":"<ul> <li>Restart Failed Jobs: Green \"Restart\" button for failed/cancelled jobs</li> <li>Cancel Running Jobs: Red \"Cancel\" button for pending/running jobs</li> <li>View Completed Results: Blue \"View Results\" button for completed jobs</li> </ul>"},{"location":"userGuide/extractiongui/#browse-manage-extractions-tab","title":"\ud83d\udccb Browse &amp; Manage Extractions Tab","text":""},{"location":"userGuide/extractiongui/#enhanced-management-interface","title":"Enhanced Management Interface","text":"<ul> <li>Search &amp; Filter Controls: Find specific extraction jobs by name, status, or date range</li> <li>Bulk Selection: Select multiple jobs for batch operations</li> <li>Advanced Filtering: Filter by project, taxonomy, AI model, or extraction type</li> <li>Sorting Options: Sort by creation date, completion time, or status</li> </ul>"},{"location":"userGuide/extractiongui/#job-management-actions_1","title":"Job Management Actions","text":"<ul> <li>View Results: Access detailed extraction results for completed jobs</li> <li>Delete Jobs: Permanently remove extraction jobs and associated data</li> <li>Restart Failed Jobs: Retry failed extractions with the same parameters</li> <li>Cancel Running Jobs: Stop active extraction processes</li> </ul>"},{"location":"userGuide/extractiongui/#results-organization","title":"Results Organization","text":"<ul> <li>Optimized Data Structure: Results are deduplicated and optimized for downstream processing, focusing on unique text snippets and entities.</li> <li>Category-Based Display: Results grouped by taxonomy categories</li> <li>Expandable Sections: Each category shows extracted entities with:</li> <li>Entity Frequency: How many chunks contain each entity</li> <li>Confidence Scores: Quality indicators for extractions</li> <li>Source References: Links to original text chunks</li> </ul>"},{"location":"userGuide/extractiongui/#relationship-analysis","title":"Relationship Analysis","text":"<ul> <li>Relationship Summary: Overview of discovered entity relationships</li> <li>Type Distribution: Breakdown by relationship types (associative, causal, etc.)</li> <li>Confidence Metrics: Quality assessment of relationship inferences</li> </ul>"},{"location":"userGuide/extractiongui/#qa-dataset-generation_1","title":"Q&amp;A Dataset Generation","text":"<ul> <li>One-Click Generation: Automatic Q&amp;A pair creation from relationships</li> <li>Sample Preview: View generated questions and answers before export</li> <li>Export Options: Download in JSONL, JSON, or CSV formats</li> <li>Statistics Display: Generation metrics and quality indicators</li> </ul>"},{"location":"userGuide/extractiongui/#job-status-types","title":"Job Status Types","text":""},{"location":"userGuide/extractiongui/#pending","title":"Pending","text":"<ul> <li>Appearance: Status shows \"Pending\", progress at 0%</li> <li>Actions Available: Cancel</li> <li>Description: Job is queued and waiting for processing resources</li> </ul>"},{"location":"userGuide/extractiongui/#running","title":"Running","text":"<ul> <li>Appearance: Status shows \"Running\", progress bar updates in real-time</li> <li>Actions Available: Cancel</li> <li>Description: Job is actively being processed with live progress updates</li> </ul>"},{"location":"userGuide/extractiongui/#completed","title":"Completed","text":"<ul> <li>Appearance: Status shows \"Completed\", progress at 100%</li> <li>Actions Available: View Results</li> <li>Description: Job finished successfully with results available</li> </ul>"},{"location":"userGuide/extractiongui/#failed","title":"Failed","text":"<ul> <li>Appearance: Status shows \"Failed\", progress bar shows last completed percentage</li> <li>Actions Available: Restart</li> <li>Description: Job encountered an error and stopped</li> </ul>"},{"location":"userGuide/extractiongui/#cancelled","title":"Cancelled","text":"<ul> <li>Appearance: Status shows \"Cancelled\", progress shows completion at cancellation time</li> <li>Actions Available: Restart</li> <li>Description: Job was manually stopped by user</li> </ul>"},{"location":"userGuide/extractiongui/#error-handling","title":"Error Handling","text":"<p>The GUI provides intelligent error handling with user-friendly messages:</p>"},{"location":"userGuide/extractiongui/#common-error-types","title":"Common Error Types","text":"<ul> <li>\ud83d\udd0c Connection Error: Network issues preventing communication with the server</li> <li>\u23f1\ufe0f Timeout Error: Operations taking longer than expected</li> <li>\ud83d\udd10 Authentication Error: Session expired, requiring re-login</li> <li>\u274c Not Found: Requested job or resource doesn't exist</li> <li>\ud83d\udc0c Rate Limit Exceeded: Too many requests, need to wait</li> </ul>"},{"location":"userGuide/extractiongui/#specific-extraction-errors","title":"Specific Extraction Errors","text":"<ul> <li>\ud83d\udcc2 Taxonomy not found: Selected taxonomy doesn't exist</li> <li>\ud83e\udd16 AI model unavailable: Selected AI service issues</li> <li>\ud83d\uddc4\ufe0f Database connection failed: Backend database problems</li> <li>\ud83d\udc0c API rate limit exceeded: External AI service limits reached</li> <li>\ud83d\udd0d Entity extraction failed: AI parsing or response issues</li> </ul>"},{"location":"userGuide/extractiongui/#workflow-examples","title":"Workflow Examples","text":""},{"location":"userGuide/extractiongui/#complete-entity-extraction-workflow","title":"Complete Entity Extraction Workflow","text":"<ol> <li>Setup Project &amp; Taxonomy:</li> <li>Select project containing your documents</li> <li> <p>Choose appropriate taxonomy for entity categorization</p> </li> <li> <p>Configure AI Models:</p> </li> <li>Select primary AI model (Grok recommended for accuracy)</li> <li>Optionally enable validation with different AI model</li> <li> <p>Adjust confidence threshold based on use case</p> </li> <li> <p>Select Categories:</p> </li> <li>Use taxonomy tree to select relevant categories</li> <li>Search and filter categories as needed</li> <li> <p>Review selection summary before proceeding</p> </li> <li> <p>Run Extraction:</p> </li> <li>Set extraction parameters (depth, batch size, max chunks)</li> <li>Click \"Start Extraction\" to launch job</li> <li> <p>Monitor progress in Monitor Jobs tab</p> </li> <li> <p>Review Results:</p> </li> <li>Switch to Browse &amp; Manage Extractions tab</li> <li>Explore extracted entities by category</li> <li>Review relationship discoveries</li> <li>Generate Q&amp;A datasets if needed</li> </ol>"},{"location":"userGuide/extractiongui/#advanced-dataset-generation","title":"Advanced Dataset Generation","text":"<ol> <li>Complete Entity Extraction: Ensure extraction job finishes successfully</li> <li>Review Relationships: Check discovered entity associations</li> <li>Generate Q&amp;A Pairs: Use one-click generation from relationships</li> <li>Preview &amp; Export: Review samples and download in preferred format</li> <li>Integration: Use exported datasets in ML training pipelines</li> </ol>"},{"location":"userGuide/extractiongui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/extractiongui/#entity-extraction-optimization","title":"Entity Extraction Optimization","text":"<ul> <li>AI Model Selection: Use Grok for highest accuracy, Gemini for speed</li> <li>Confidence Thresholds: Start with 0.5, adjust based on domain requirements</li> <li>Category Selection: Be specific - fewer, well-chosen categories yield better results</li> <li>Batch Size Tuning: Larger batches for homogeneous content, smaller for diverse content</li> </ul>"},{"location":"userGuide/extractiongui/#relationship-inference_1","title":"Relationship Inference","text":"<ul> <li>Domain Understanding: Ensure taxonomy reflects real-world relationships</li> <li>Quality Validation: Review relationship confidence scores</li> <li>Iterative Refinement: Adjust taxonomy based on extraction results</li> </ul>"},{"location":"userGuide/extractiongui/#qa-dataset-generation_2","title":"Q&amp;A Dataset Generation","text":"<ul> <li>Template Customization: Review generated Q&amp;A pairs for quality</li> <li>Format Selection: Choose JSONL for most ML frameworks</li> <li>Context Preservation: Include relationship metadata when possible</li> <li>Quality Assurance: Manually review samples before large-scale generation</li> </ul>"},{"location":"userGuide/extractiongui/#integration-with-other-features","title":"Integration with Other Features","text":""},{"location":"userGuide/extractiongui/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<ol> <li>Upload Documents: Use Documents section to ingest and chunk files</li> <li>Create Taxonomy: Build domain-specific taxonomies for categorization</li> <li>Run Extraction: Use unified extraction interface for entity discovery</li> <li>Generate Datasets: Create training data from extracted entities and relationships</li> <li>Quality Analysis: Review extraction metrics and relationship quality</li> </ol>"},{"location":"userGuide/extractiongui/#advanced-analytics","title":"Advanced Analytics","text":"<ol> <li>Monitor Performance: Track extraction accuracy across different domains</li> <li>Relationship Mining: Discover patterns in entity associations</li> <li>Dataset Quality: Assess generated Q&amp;A pair quality and diversity</li> <li>Model Improvement: Use extraction results to refine AI prompts and taxonomies</li> </ol>"},{"location":"userGuide/extractiongui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/extractiongui/#no-entities-found","title":"No Entities Found","text":"<ul> <li>Check AI Model: Ensure selected AI model has valid API keys</li> <li>Verify Taxonomy: Confirm taxonomy categories are relevant to content</li> <li>Adjust Confidence: Lower confidence threshold if too restrictive</li> <li>Review Content: Ensure documents contain extractable entities</li> </ul>"},{"location":"userGuide/extractiongui/#poor-relationship-quality","title":"Poor Relationship Quality","text":"<ul> <li>Taxonomy Refinement: Improve category definitions and relationships</li> <li>Content Quality: Ensure documents have clear entity associations</li> <li>AI Model Selection: Try different models for better relationship inference</li> </ul>"},{"location":"userGuide/extractiongui/#qa-generation-issues","title":"Q&amp;A Generation Issues","text":"<ul> <li>Relationship Quality: Ensure high-confidence relationships exist</li> <li>Template Relevance: Customize templates for your domain</li> <li>Content Coverage: Verify sufficient entity pairs for generation</li> </ul>"},{"location":"userGuide/extractiongui/#advanced-features","title":"Advanced Features","text":""},{"location":"userGuide/extractiongui/#multi-model-validation","title":"Multi-Model Validation","text":"<ul> <li>Cross-Model Agreement: Use different AI models for validation</li> <li>Confidence Boosting: Higher confidence for agreed-upon extractions</li> <li>Error Detection: Identify inconsistent AI responses</li> </ul>"},{"location":"userGuide/extractiongui/#batch-processing","title":"Batch Processing","text":"<ul> <li>Large-Scale Operations: Process thousands of documents efficiently</li> <li>Progress Monitoring: Real-time updates for long-running jobs</li> <li>Resource Management: Automatic load balancing across available resources</li> </ul>"},{"location":"userGuide/extractiongui/#custom-templates","title":"Custom Templates","text":"<ul> <li>Domain-Specific Q&amp;A: Create templates for specialized domains</li> <li>Question Variety: Generate multiple question types from same relationships</li> <li>Context Enhancement: Include domain-specific context in generated pairs</li> </ul> <p>This comprehensive GUI transforms complex entity extraction and relationship inference into an accessible, powerful tool for creating high-quality structured datasets from unstructured text. The unified interface eliminates the need for technical expertise while providing advanced capabilities for expert users.</p>"},{"location":"userGuide/extractiongui/#job-status-types_1","title":"Job Status Types","text":""},{"location":"userGuide/extractiongui/#pending_1","title":"Pending","text":"<ul> <li>Appearance: Status shows \"Pending\", progress at 0%</li> <li>Actions Available: Cancel</li> <li>Description: Job is queued and waiting for processing resources</li> </ul>"},{"location":"userGuide/extractiongui/#running_1","title":"Running","text":"<ul> <li>Appearance: Status shows \"Running\", progress bar updates in real-time</li> <li>Actions Available: Cancel</li> <li>Description: Job is actively being processed with live progress updates</li> </ul>"},{"location":"userGuide/extractiongui/#completed_1","title":"Completed","text":"<ul> <li>Appearance: Status shows \"Completed\", progress at 100%</li> <li>Actions Available: View Results</li> <li>Description: Job finished successfully with results available</li> </ul>"},{"location":"userGuide/extractiongui/#failed_1","title":"Failed","text":"<ul> <li>Appearance: Status shows \"Failed\", progress bar shows last completed percentage</li> <li>Actions Available: Restart</li> <li>Description: Job encountered an error and stopped</li> </ul>"},{"location":"userGuide/extractiongui/#cancelled_1","title":"Cancelled","text":"<ul> <li>Appearance: Status shows \"Cancelled\", progress shows completion at cancellation time</li> <li>Actions Available: Restart</li> <li>Description: Job was manually stopped by user</li> </ul>"},{"location":"userGuide/extractiongui/#error-handling_1","title":"Error Handling","text":"<p>The GUI provides intelligent error handling with user-friendly messages:</p>"},{"location":"userGuide/extractiongui/#common-error-types_1","title":"Common Error Types","text":"<ul> <li>\ud83d\udd0c Connection Error: Network issues preventing communication with the server</li> <li>\u23f1\ufe0f Timeout Error: Operations taking longer than expected</li> <li>\ud83d\udd10 Authentication Error: Session expired, requiring re-login</li> <li>\u274c Not Found: Requested job or resource doesn't exist</li> <li>\ud83d\udc0c Rate Limit Exceeded: Too many requests, need to wait</li> </ul>"},{"location":"userGuide/extractiongui/#specific-extraction-errors_1","title":"Specific Extraction Errors","text":"<ul> <li>\ud83d\udcc2 Taxonomy not found: Selected taxonomy doesn't exist</li> <li>\ud83e\udd16 Classification service unavailable: AI service issues</li> <li>\ud83d\uddc4\ufe0f Database connection failed: Backend database problems</li> <li>\ud83d\udc0c API rate limit exceeded: External service limits reached</li> </ul>"},{"location":"userGuide/extractiongui/#workflow-examples_1","title":"Workflow Examples","text":""},{"location":"userGuide/extractiongui/#monitoring-active-extractions","title":"Monitoring Active Extractions","text":"<ol> <li>Select Project: Choose the project with active extraction jobs</li> <li>View Dashboard: See all jobs with their current status and progress</li> <li>Monitor Progress: Watch progress bars update in real-time for running jobs</li> <li>Check Details: Expand job cards to see parameters and timing information</li> </ol>"},{"location":"userGuide/extractiongui/#handling-failed-jobs","title":"Handling Failed Jobs","text":"<ol> <li>Identify Failed Jobs: Look for jobs with \"Failed\" status</li> <li>Review Error Details: Expand the job card to see specific error messages</li> <li>Assess Feasibility: Determine if the error is recoverable</li> <li>Restart if Appropriate: Click \"Restart\" button for transient failures</li> <li>Monitor Restart: Watch the restarted job progress through the dashboard</li> </ol>"},{"location":"userGuide/extractiongui/#managing-long-running-jobs","title":"Managing Long-Running Jobs","text":"<ol> <li>Identify Long Jobs: Look for jobs running for extended periods</li> <li>Check Progress: Monitor if progress is still advancing</li> <li>Cancel if Stuck: Use \"Cancel\" button for jobs that appear stuck</li> <li>Restart After Review: Restart cancelled jobs after investigating issues</li> </ol>"},{"location":"userGuide/extractiongui/#best-practices_1","title":"Best Practices","text":""},{"location":"userGuide/extractiongui/#job-monitoring","title":"Job Monitoring","text":"<ul> <li>Regular Checks: Periodically refresh the page to see latest job status</li> <li>Progress Tracking: Use progress bars to estimate completion time</li> <li>Error Review: Always check error details before restarting failed jobs</li> <li>Resource Management: Cancel unnecessary jobs to free up system resources</li> </ul>"},{"location":"userGuide/extractiongui/#error-recovery","title":"Error Recovery","text":"<ul> <li>Understand Errors: Read error messages carefully to identify root causes</li> <li>Retry Strategically: Only restart jobs where the error appears transient</li> <li>Contact Support: For persistent errors, gather error details before contacting support</li> <li>Prevent Recurrence: Note patterns in failures to avoid similar issues</li> </ul>"},{"location":"userGuide/extractiongui/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Batch Monitoring: Use the dashboard to monitor multiple jobs simultaneously</li> <li>Priority Management: Cancel lower-priority jobs if high-priority work is blocked</li> <li>Timing Awareness: Note typical completion times for different job types</li> <li>Load Balancing: Distribute large extraction workloads across multiple projects</li> </ul>"},{"location":"userGuide/extractiongui/#integration-with-other-features_1","title":"Integration with Other Features","text":""},{"location":"userGuide/extractiongui/#document-processing-workflow","title":"Document Processing Workflow","text":"<ol> <li>Upload Documents: Use the Documents section to upload and process files</li> <li>Create Taxonomy: Build or generate taxonomies for content categorization</li> <li>Start Extraction: Initiate extraction jobs (typically through API or other interfaces)</li> <li>Monitor Progress: Use the Extraction GUI to track job completion</li> <li>View Results: Access extraction results through the completed job interface</li> </ol>"},{"location":"userGuide/extractiongui/#dataset-generation-follow-up","title":"Dataset Generation Follow-up","text":"<ol> <li>Complete Extractions: Ensure extraction jobs finish successfully</li> <li>Generate Datasets: Use the Dataset section to create training data</li> <li>Quality Analysis: Review extraction quality metrics</li> <li>Iterate: Restart failed extractions or adjust parameters as needed</li> </ol>"},{"location":"userGuide/extractiongui/#troubleshooting_1","title":"Troubleshooting","text":""},{"location":"userGuide/extractiongui/#jobs-not-appearing","title":"Jobs Not Appearing","text":"<ul> <li>Check Project Selection: Ensure correct project is selected</li> <li>Refresh Page: Use browser refresh to reload job list</li> <li>Verify Permissions: Ensure you have access to view jobs in the selected project</li> </ul>"},{"location":"userGuide/extractiongui/#progress-not-updating","title":"Progress Not Updating","text":"<ul> <li>Check Connection: Verify internet connectivity</li> <li>Refresh Browser: Hard refresh (Ctrl+F5) to clear cache</li> <li>Contact Support: If progress consistently doesn't update</li> </ul>"},{"location":"userGuide/extractiongui/#action-buttons-not-working","title":"Action Buttons Not Working","text":"<ul> <li>Check Job Status: Buttons only appear for valid state transitions</li> <li>Verify Permissions: Ensure you have permissions to modify jobs</li> <li>Network Issues: Check connection and retry</li> </ul>"},{"location":"userGuide/extractiongui/#advanced-features_1","title":"Advanced Features","text":""},{"location":"userGuide/extractiongui/#real-time-updates","title":"Real-time Updates","text":"<p>The GUI automatically refreshes job status, but for the most current information:</p> <ul> <li>Use browser refresh for immediate updates</li> <li>Monitor progress bars for live completion tracking</li> <li>Check timestamps to verify data freshness</li> </ul>"},{"location":"userGuide/extractiongui/#bulk-operations","title":"Bulk Operations","text":"<p>While individual job control is available, consider:</p> <ul> <li>Project Organization: Group related jobs in dedicated projects</li> <li>Batch Processing: Use API or CLI for bulk job operations</li> <li>Automation: Set up monitoring scripts for large-scale operations</li> </ul>"},{"location":"userGuide/extractiongui/#performance-metrics","title":"Performance Metrics","text":"<p>Track job performance over time:</p> <ul> <li>Completion Times: Note typical duration for different job types</li> <li>Success Rates: Monitor failure frequency and types</li> <li>Resource Usage: Observe system impact during peak processing</li> </ul> <p>This GUI provides a user-friendly way to manage the complex asynchronous nature of extraction jobs while maintaining full visibility into the processing pipeline.</p>"},{"location":"userGuide/gui_user_guide/","title":"Compileo GUI User Guide","text":""},{"location":"userGuide/gui_user_guide/#overview","title":"Overview","text":"<p>The Compileo GUI provides a user-friendly web interface for document processing, taxonomy management, and dataset generation. This guide covers the main features and workflows available through the GUI.</p>"},{"location":"userGuide/gui_user_guide/#getting-started","title":"Getting Started","text":""},{"location":"userGuide/gui_user_guide/#prerequisites","title":"Prerequisites","text":"<ul> <li>Running Compileo API server (<code>python -m src.compileo.api.main</code>)</li> <li>API keys configured for AI services (Gemini, Grok, etc.)</li> <li>At least one project created</li> </ul>"},{"location":"userGuide/gui_user_guide/#launching-the-gui","title":"Launching the GUI","text":"<pre><code>streamlit run src/compileo/features/gui/main.py\n</code></pre> <p>The GUI will be available at <code>http://localhost:8501</code> by default.</p>"},{"location":"userGuide/gui_user_guide/#main-interface","title":"Main Interface","text":""},{"location":"userGuide/gui_user_guide/#navigation-layout","title":"Navigation Layout","text":"<p>The GUI features a modern header and grouped sidebar navigation:</p> <p>Header: - \ud83d\udd2c Compileo Dataset Creator: Application branding</p> <p>Sidebar Navigation (Grouped): - \ud83c\udfe0 Home - \u2699\ufe0f Settings - \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \ud83e\uddd9 Wizard   - Dataset Generation Wizard - \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \u2699\ufe0f Workflow   - Projects   - Document Processing   - Taxonomy   - Extraction   - Extraction Results   - Dataset Generation - \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \ud83d\udcca Analysis   - Quality Metrics   - Benchmarking - \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 - \u2699\ufe0f Job Management   - Job Queue (Real-time monitoring of active and pending jobs)   - Job History (View all past jobs with filters)</p>"},{"location":"userGuide/gui_user_guide/#document-processing-tabs","title":"\ud83d\udcc4 Document Processing Tabs","text":""},{"location":"userGuide/gui_user_guide/#parse-documents-tab","title":"Parse Documents Tab","text":""},{"location":"userGuide/gui_user_guide/#purpose","title":"Purpose","text":"<p>Upload new documents and parse existing ones into a clean markdown format.</p>"},{"location":"userGuide/gui_user_guide/#workflow","title":"Workflow","text":"<ol> <li>Select Project: Choose the target project.</li> <li>Upload Files: Upload new documents (PDF, DOCX, TXT, etc.).</li> <li>Select Parser: Choose the parsing engine (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>).</li> <li>Select Documents: Check the boxes next to the documents you want to parse.</li> <li>Parse: Click the \"Parse Documents\" button. A job will be submitted to the background queue, and you can monitor its progress in the Job Queue sidebar or the dedicated Job Management page.</li> </ol>"},{"location":"userGuide/gui_user_guide/#pre-parsing-pdf-splitter","title":"Pre-Parsing PDF Splitter","text":""},{"location":"userGuide/gui_user_guide/#purpose_1","title":"Purpose","text":"<p>For very large PDF documents (e.g., thousands of pages), direct parsing by AI models can lead to token limit issues or summarization. The Pre-Parsing PDF Splitter automatically divides these large PDFs into smaller, manageable chunks (individual PDF files) before they are sent to any parsing model. This ensures that each segment of the document can be processed completely and accurately.</p>"},{"location":"userGuide/gui_user_guide/#automatic-splitting","title":"Automatic Splitting","text":"<ul> <li>If a PDF document has more than 200 pages, it will be automatically split into multiple smaller PDF files.</li> <li>Each split file will contain approximately 200 pages.</li> <li>An overlap of 1 page is included between consecutive split files. This overlap helps maintain content continuity, allowing downstream chunking and parsing processes to handle information that spans across the split boundaries effectively.</li> <li>The split files are named sequentially (e.g., <code>original_document_name_1.pdf</code>, <code>original_document_name_2.pdf</code>).</li> </ul>"},{"location":"userGuide/gui_user_guide/#how-it-works","title":"How it Works","text":"<p>When you upload a large PDF or initiate a parsing job for one, the system first checks its page count. If it exceeds the 200-page threshold, the splitter automatically creates the smaller PDF files. These smaller files are then processed sequentially by the chosen parsing engine. From the user's perspective, this process is largely transparent, ensuring reliable parsing of even the largest documents.</p>"},{"location":"userGuide/gui_user_guide/#configure-chunk-documents-tab","title":"Configure &amp; Chunk Documents Tab","text":""},{"location":"userGuide/gui_user_guide/#purpose_2","title":"Purpose","text":"<p>Configure chunking strategies and apply them to your parsed documents using either manual configuration or AI-assisted recommendations.</p>"},{"location":"userGuide/gui_user_guide/#configuration-modes","title":"Configuration Modes","text":"<p>Manual Configuration (Default): Direct parameter setting for experienced users.</p> <p>AI-Assisted Configuration: Intelligent recommendations based on your document structure and goals.</p>"},{"location":"userGuide/gui_user_guide/#manual-configuration-workflow","title":"Manual Configuration Workflow","text":"<ol> <li>Select Strategy: Choose chunking method (<code>character</code>, <code>token</code>, <code>semantic</code>, <code>delimiter</code>, <code>schema</code>)</li> <li>Configure Parameters: Set strategy-specific parameters manually</li> <li>Select Documents: Choose parsed documents to process</li> <li>Process: Apply chunking with your chosen settings. A job will be submitted to the background queue, and you can monitor its progress in the Job Queue sidebar or the dedicated Job Management page.</li> </ol>"},{"location":"userGuide/gui_user_guide/#ai-assisted-configuration-workflow","title":"AI-Assisted Configuration Workflow","text":"<ol> <li>Describe Goal: Provide chunking objective (required field)</li> <li>Select Document: Choose representative document for analysis</li> <li>Preview Content: Browse document with pagination (10K chars per page)</li> <li>Extract Examples: Select text directly in the preview area to gather examples</li> <li>Get Recommendations: AI analyzes goal, content, and examples</li> <li>Apply Settings: Use AI-recommended parameters or make adjustments</li> <li>Process Documents: Apply configuration to selected documents</li> </ol>"},{"location":"userGuide/gui_user_guide/#ai-assisted-features","title":"AI-Assisted Features","text":"<ul> <li>Goal Description: Required field describing chunking objectives</li> <li>Document Preview: Paginated content viewing with header highlighting</li> <li>Text Selection: Click and drag to select any portion of document content</li> <li>Real-time Feedback: Selected text appears in a dedicated field</li> <li>Flexible Examples: Any text portion can be added to the AI example pool</li> <li>AI Recommendations: Intelligent strategy and parameter suggestions</li> <li>JSON Schema Auto-Fix: Automatic correction of backslash escaping issues when copying AI recommendations</li> </ul>"},{"location":"userGuide/gui_user_guide/#ai-assisted-features_1","title":"AI-Assisted Features","text":"<ul> <li>Goal Description: Required field describing chunking objectives</li> <li>Document Preview: Paginated content viewing with header highlighting</li> <li>Text Selection: Click and drag to select any portion of document content</li> <li>Real-time Feedback: Selected text appears in a dedicated field</li> <li>Flexible Examples: Any text portion can be added to the AI example pool</li> <li>AI Recommendations: Intelligent strategy and parameter suggestions</li> </ul>"},{"location":"userGuide/gui_user_guide/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>gemini</code> parser for complex document layouts</li> <li>Set chunk size to 1000 for balanced processing</li> <li>Use 10-20% overlap for context continuity</li> <li>Process multiple related documents together</li> <li>For semantic: Use the placeholder example as a template for custom prompts</li> <li>For schema: Start with simple regex patterns and build to complex rules</li> <li>Schema Include Pattern: Use <code>include_pattern: true</code> when you want chunks to START with matched patterns (e.g., disease names), <code>false</code> when patterns should be excluded from chunks</li> </ul>"},{"location":"userGuide/gui_user_guide/#taxonomy-tab","title":"\ud83c\udff7\ufe0f Taxonomy Tab","text":""},{"location":"userGuide/gui_user_guide/#three-main-sub-tabs","title":"Three Main Sub-tabs","text":""},{"location":"userGuide/gui_user_guide/#1-build-taxonomy","title":"1. \ud83c\udfd7\ufe0f Build Taxonomy","text":"<p>Create new taxonomies using AI generation or manual construction with hybrid capabilities.</p> <p>AI Generation Mode: - Select project and enter taxonomy name - Choose AI generator (<code>gemini</code>, <code>grok</code>, <code>ollama</code>) - Set domain and specificity level - Select documents to analyze - Configure category limits per hierarchy level - Generate taxonomy automatically</p> <p>Hybrid Mode: - Manually define basic category structure - Use AI to extend and refine the taxonomy - Add subcategories automatically - Load existing taxonomies as starting points</p> <p>Manual Structure Building: - Add top-level categories with descriptions - Build hierarchical subcategories - Set confidence thresholds per category - AI enhancement for existing manual structures</p>"},{"location":"userGuide/gui_user_guide/#2-classification-extraction","title":"2. \ud83d\udd0d Classification &amp; Extraction","text":"<p>Apply taxonomies to content for selective categorization and information extraction.</p> <p>Selective Category Selection: - Interactive taxonomy tree selector - Check/uncheck specific categories for extraction - Preview selection with statistics - Hierarchical category navigation</p> <p>Extraction Parameters: - Extraction Depth: Maximum taxonomy hierarchy levels to traverse (1-5) - Confidence Threshold: Minimum confidence score for results (0.0-1.0) - Skip Fine Classification: Enable for faster processing (coarse only) - Advanced Settings: Max chunks, batch size, processing controls</p> <p>Extraction Workflow: 1. Select project and taxonomy 2. Choose specific categories using tree selector 3. Configure extraction parameters 4. Run selective extraction job 5. Monitor progress and view results 6. Export results as JSON or CSV</p> <p>Results Viewer: - Organized by selected categories - Confidence score filtering - Paginated results with metadata - Export options for analysis</p>"},{"location":"userGuide/gui_user_guide/#3-browse-manage-taxonomies","title":"3. \ud83d\udccb Browse &amp; Manage Taxonomies","text":"<ul> <li>Search and filter existing taxonomies</li> <li>View taxonomies in tree or list format</li> <li>Edit taxonomy metadata</li> <li>Export taxonomies as JSON or CSV</li> <li>Bulk operations and management</li> <li>Delete taxonomies (with confirmation)</li> </ul>"},{"location":"userGuide/gui_user_guide/#best-practices_1","title":"Best Practices","text":"<ul> <li>Start with AI generation for initial taxonomy creation</li> <li>Use hybrid mode for iterative refinement</li> <li>Choose domain-specific settings for better categorization</li> <li>Select specific categories for focused extraction</li> <li>Regularly update taxonomies as content evolves</li> <li>Use extraction results to improve taxonomy accuracy</li> </ul>"},{"location":"userGuide/gui_user_guide/#extraction-tab","title":"\ud83d\udd0d Extraction Tab","text":""},{"location":"userGuide/gui_user_guide/#purpose_3","title":"Purpose","text":"<p>Advanced entity extraction, relationship inference, and Q&amp;A dataset generation from unstructured text documents using AI-powered analysis.</p>"},{"location":"userGuide/gui_user_guide/#three-tab-unified-interface","title":"Three-Tab Unified Interface","text":""},{"location":"userGuide/gui_user_guide/#run-extraction-tab","title":"\ud83c\udfc3 Run Extraction Tab","text":"<p>Extraction Type &amp; Mode Selection: - Extraction Type: Select between <code>Named Entity Recognition (NER)</code> (extracts specific entities) or <code>Whole Text Extraction</code> (extracts complete text portions). - Extraction Mode: Choose <code>Contextual Extraction</code> (filters by parent context for precision) or <code>Document-Wide Extraction</code> (processes all chunks for maximum coverage).</p> <p>AI Model Selection: - Choose from Grok, Gemini, or Ollama AI models as the <code>Primary Classifier</code>. - Optionally enable a <code>Validation Stage</code> with a different AI model for quality assurance.</p> <p>Taxonomy Integration: - Select project and taxonomy for entity categorization. - Interactive taxonomy tree for category selection. - Search and filter categories for precise targeting.</p> <p>Extraction Parameters: - Depth Control: Maximum taxonomy hierarchy levels (1-5). - Confidence Threshold: Minimum quality score (0.0-1.0). - Batch Processing: Chunk size and processing limits. - Advanced Controls: Performance tuning options.</p> <p>Workflow: 1. Select project and taxonomy. 2. Choose <code>Extraction Type</code> and <code>Extraction Mode</code>. 3. Choose AI model(s) for extraction. 4. Configure extraction parameters. 5. Select specific categories using tree interface. 6. Start extraction job and monitor progress.</p>"},{"location":"userGuide/gui_user_guide/#monitor-jobs-tab","title":"\ud83d\udcca Monitor Jobs Tab","text":"<p>Real-time Job Tracking: - Live progress updates for all extraction jobs - Status indicators (Pending, Running, Completed, Failed) - Detailed job parameters and timing information - Action buttons for job management (restart, cancel, view results)</p> <p>Job Management: - Filter jobs by project and status - View comprehensive job metadata - Monitor resource usage and performance - Handle failed jobs with restart capabilities</p>"},{"location":"userGuide/gui_user_guide/#view-results-tab","title":"\ud83d\udccb View Results Tab","text":"<p>Entity Results Display: - Extracted entities organized by taxonomy categories - Confidence scores and source chunk references - Frequency analysis across document collections - Advanced filtering and search capabilities</p> <p>Relationship Analysis: - Automatic discovery of entity relationships - Relationship type distribution and quality metrics - Interactive relationship visualization - Confidence-weighted association analysis</p> <p>Q&amp;A Dataset Generation: - One-click generation of question-answer pairs - Template-based customization for different domains - Multiple export formats (JSONL, JSON, CSV) - Quality preview and statistics</p>"},{"location":"userGuide/gui_user_guide/#advanced-features","title":"Advanced Features","text":""},{"location":"userGuide/gui_user_guide/#multi-model-validation","title":"Multi-Model Validation","text":"<ul> <li>Cross-model agreement checking</li> <li>Enhanced confidence through AI consensus</li> <li>Error detection and quality assurance</li> </ul>"},{"location":"userGuide/gui_user_guide/#scalable-processing","title":"Scalable Processing","text":"<ul> <li>Large document collection handling</li> <li>Batch processing optimization</li> <li>Memory-efficient streaming operations</li> <li>Parallel AI model utilization</li> </ul>"},{"location":"userGuide/gui_user_guide/#best-practices_2","title":"Best Practices","text":"<ul> <li>AI Model Selection: Use Grok for accuracy, Gemini for speed</li> <li>Category Targeting: Select specific categories for focused extraction</li> <li>Confidence Tuning: Adjust thresholds based on domain requirements</li> <li>Quality Validation: Enable multi-model validation for critical applications</li> <li>Resource Monitoring: Track performance for large-scale operations</li> </ul>"},{"location":"userGuide/gui_user_guide/#extraction-results-tab","title":"\ud83d\udcca Extraction Results Tab","text":""},{"location":"userGuide/gui_user_guide/#purpose_4","title":"Purpose","text":"<p>View, analyze, and export results from completed extraction jobs.</p>"},{"location":"userGuide/gui_user_guide/#features","title":"Features","text":"<ul> <li>Job Management: View all extraction jobs with status</li> <li>Results Organization: Results organized by selected categories</li> <li>Filtering &amp; Search: Filter by confidence, category, or content</li> <li>Export Options: Export as JSON or CSV for analysis</li> <li>Pagination: Navigate through large result sets</li> <li>Metadata Display: View extraction metadata and statistics</li> </ul>"},{"location":"userGuide/gui_user_guide/#workflow_1","title":"Workflow","text":"<ol> <li>Select Job: Choose from completed extraction jobs</li> <li>Browse Results: Navigate through categorized results</li> <li>Apply Filters: Filter by confidence score or categories</li> <li>Export Data: Download results for further analysis</li> <li>Review Statistics: Analyze extraction performance metrics</li> </ol>"},{"location":"userGuide/gui_user_guide/#best-practices_3","title":"Best Practices","text":"<ul> <li>Review high-confidence results first</li> <li>Use category filtering for focused analysis</li> <li>Export results regularly for backup</li> <li>Monitor extraction quality metrics</li> </ul>"},{"location":"userGuide/gui_user_guide/#dataset-generation-tab","title":"\ud83d\udd27 Dataset Generation Tab","text":""},{"location":"userGuide/gui_user_guide/#purpose_5","title":"Purpose","text":"<p>Generate high-quality datasets from processed document chunks and extraction results using advanced controls. Note: Dataset generation now follows an extraction-first approach - perform taxonomy-based extraction before generating datasets to ensure structured, categorized content is used as input.</p>"},{"location":"userGuide/gui_user_guide/#configuration-sections","title":"Configuration Sections","text":""},{"location":"userGuide/gui_user_guide/#basic-settings","title":"Basic Settings","text":"<ul> <li>Project Selection: Choose source project</li> <li>Generation Mode: <code>default</code>, <code>question</code>, <code>answer</code>, <code>summarization</code></li> <li>Output Format: <code>jsonl</code> or <code>parquet</code></li> <li>Concurrent Workers: Number of parallel processing threads (1-10)</li> </ul>"},{"location":"userGuide/gui_user_guide/#quality-control","title":"Quality Control","text":"<ul> <li>Analyze Quality: Enable/disable quality analysis</li> <li>Quality Threshold: Minimum acceptable quality score (0.0-1.0)</li> </ul>"},{"location":"userGuide/gui_user_guide/#advanced-options","title":"Advanced Options","text":"<ul> <li>Include Evaluation Sets: Generate train/validation/test splits</li> <li>Enable Versioning: Create versioned dataset snapshots</li> <li>Data Source: Choose data source for generation (Chunks Only, Taxonomy, Extract)</li> <li>Taxonomy Selection: Choose taxonomy for content filtering (when using Taxonomy mode)</li> </ul>"},{"location":"userGuide/gui_user_guide/#high-level-prompts","title":"High-Level Prompts","text":"<p>Define the target audience and purpose for more relevant content: - Custom Audience: \"medical residents\", \"data scientists\", etc. - Custom Purpose: Specific use case description - Complexity Level: <code>beginner</code> to <code>expert</code> - Domain: Knowledge area (e.g., \"cardiology\", \"machine learning\")</p>"},{"location":"userGuide/gui_user_guide/#dataset-size-control","title":"Dataset Size Control","text":"<ul> <li>Datasets per Chunk: Number of entries to generate per document chunk (1-10)</li> </ul>"},{"location":"userGuide/gui_user_guide/#data-source-modes","title":"Data Source Modes","text":"<p>Chunks Only - Uses raw text chunks directly from processed documents - No taxonomy or extraction filtering required - Best for basic dataset generation from any content</p> <p>Taxonomy - Applies taxonomy definitions to enhance generation prompts - Works with all chunks in the project (no extraction dependency) - Adds domain-specific context and terminology</p> <p>Extract - Uses extracted entities as the primary content source - Generates datasets focused on specific concepts/entities - Creates educational content about extracted terms</p>"},{"location":"userGuide/gui_user_guide/#model-selection","title":"Model Selection","text":"<ul> <li>Parsing Model: Document parsing AI</li> <li>Chunking Model: Text chunking AI</li> <li>Classification Model: Content classification AI</li> </ul>"},{"location":"userGuide/gui_user_guide/#workflow_2","title":"Workflow","text":"<ol> <li>Configure all parameters according to your needs</li> <li>Click \ud83d\ude80 Generate Dataset. A job will be submitted to the background queue, and you can monitor its progress in the Job Queue sidebar or the dedicated Job Management page.</li> <li>Monitor progress in the status section in real-time.</li> <li>Review results and download generated datasets.</li> </ol>"},{"location":"userGuide/gui_user_guide/#best-practices_4","title":"Best Practices","text":"<ul> <li>Start with small datasets (2-3 per chunk) for testing</li> <li>Use high-level prompts for domain-specific content</li> <li>Enable quality analysis for production datasets</li> <li>Use taxonomy filtering for focused content generation</li> </ul>"},{"location":"userGuide/gui_user_guide/#dataset-creation-wizard","title":"\ud83e\uddd9 Dataset Creation Wizard","text":""},{"location":"userGuide/gui_user_guide/#purpose_6","title":"Purpose","text":"<p>Comprehensive guided workflow for dataset generation with flexible navigation, automatic processing, and complete AI model selection.</p>"},{"location":"userGuide/gui_user_guide/#key-features","title":"Key Features","text":"<ul> <li>5-Step Guided Process: From project selection to review &amp; generate</li> <li>Flexible Navigation: Click any step tab to navigate non-linearly. Most steps are \"resume-ready\" and retrieve state directly from the database.</li> <li>Automatic File Upload: Drag-and-drop with immediate processing</li> <li>Complete AI Model Selection: 4-model configuration (parsing, chunking, classification, generation)</li> <li>Full Chunking Strategy Parity: All Document Processing tab strategies available</li> <li>Database-Mediated Workflow: UI state is synchronized with the database, allowing progress to survive session resets.</li> <li>Smart Data Source Selection: Automatic taxonomy/chunks fallback</li> <li>Document Management: Upload and delete capabilities with error correction</li> <li>Real-time Progress Monitoring: Live job tracking with detailed status updates</li> </ul>"},{"location":"userGuide/gui_user_guide/#steps","title":"Steps","text":"<ol> <li>Project Selection: Choose or create project with statistics display</li> <li>Parse &amp; Chunk &amp; Taxonomy: Automated end-to-end processing. Upload documents, select models and chunking strategy, then initiate the full pipeline from parsing to automatic taxonomy generation.</li> <li>Edit Taxonomy: Reactive simplified editor for picking and refining taxonomy structures. Supports renaming and real-time category management.</li> <li>Generation Parameters: Configure generation mode, output format, quality settings, and high-level prompt parameters (Audience, Purpose, Complexity, Domain).</li> <li>Review &amp; Generate: Comprehensive configuration summary and background job execution with progress monitoring.</li> </ol>"},{"location":"userGuide/gui_user_guide/#navigation-features","title":"Navigation Features","text":"<ul> <li>Clickable Step Tabs: Navigate to any completed step or future steps</li> <li>Prerequisite Validation: Clear error messages when required steps are missing</li> <li>Progress Tracking: Visual progress indicators and completion status</li> <li>State Persistence: Configuration saved across navigation</li> </ul>"},{"location":"userGuide/gui_user_guide/#benefits","title":"Benefits","text":"<ul> <li>Beginner-Friendly: Step-by-step guidance with clear instructions</li> <li>Expert Control: Full access to advanced configuration options</li> <li>Error Prevention: Validation prevents invalid configurations</li> <li>Workflow Flexibility: Non-linear navigation for iterative refinement</li> <li>Quality Assurance: Built-in validation and progress monitoring</li> </ul>"},{"location":"userGuide/gui_user_guide/#quality-metrics-tab","title":"\ud83d\udcca Quality Metrics Tab","text":""},{"location":"userGuide/gui_user_guide/#features_1","title":"Features","text":"<ul> <li>Analyze existing datasets for quality issues</li> <li>View detailed quality reports</li> <li>Compare dataset versions</li> <li>Identify areas for improvement</li> </ul>"},{"location":"userGuide/gui_user_guide/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>Diversity: Content variety and coverage</li> <li>Consistency: Internal coherence</li> <li>Difficulty: Appropriate complexity levels</li> <li>Bias Detection: Identify potential biases</li> <li>Relevance: Alignment with intended purpose</li> </ul>"},{"location":"userGuide/gui_user_guide/#benchmarking-tab","title":"\ud83d\udcc8 Benchmarking Tab","text":""},{"location":"userGuide/gui_user_guide/#purpose_7","title":"Purpose","text":"<p>Evaluate AI models on generated datasets.</p>"},{"location":"userGuide/gui_user_guide/#supported-benchmarks","title":"Supported Benchmarks","text":"<ul> <li>GLUE: General Language Understanding</li> <li>SuperGLUE: Advanced language tasks</li> <li>MMLU: Massive Multitask Language Understanding</li> <li>Medical Benchmarks: Domain-specific evaluation</li> </ul>"},{"location":"userGuide/gui_user_guide/#workflow_3","title":"Workflow","text":"<ol> <li>Select dataset and benchmark suite</li> <li>Configure evaluation parameters</li> <li>Run benchmark tests</li> <li>Review performance results</li> <li>Compare model performance</li> </ol>"},{"location":"userGuide/gui_user_guide/#settings-tab","title":"\u2699\ufe0f Settings Tab","text":""},{"location":"userGuide/gui_user_guide/#job-handling-configuration","title":"Job Handling Configuration","text":"<p>Configure global and per-user limits for concurrent jobs. These settings help manage system resources and ensure fair usage.</p> <ul> <li>Max Concurrent Jobs (Global): The maximum number of jobs that can run simultaneously across all users.</li> <li>Max Concurrent Jobs Per User: The maximum number of jobs a single user can run concurrently.</li> </ul>"},{"location":"userGuide/gui_user_guide/#api-key-configuration","title":"API Key Configuration","text":""},{"location":"userGuide/gui_user_guide/#api-key-configuration_1","title":"API Key Configuration","text":"<p>Configure API keys for AI services: - Gemini API Key: Google AI services - Grok API Key: xAI services - HuggingFace API Key: HuggingFace model access - Ollama: Local AI models (no key required)</p>"},{"location":"userGuide/gui_user_guide/#system-settings","title":"System Settings","text":"<ul> <li>Default Models: Set preferred AI models</li> <li>Quality Thresholds: Default quality settings</li> <li>Output Directories: Configure storage locations</li> </ul>"},{"location":"userGuide/gui_user_guide/#plugin-management","title":"Plugin Management","text":"<ul> <li>Plugins Tab: Manage extensions to Compileo's functionality (upload, list, uninstall).</li> </ul>"},{"location":"userGuide/gui_user_guide/#common-workflows","title":"Common Workflows","text":""},{"location":"userGuide/gui_user_guide/#complete-entity-extraction-dataset-generation-pipeline","title":"Complete Entity Extraction &amp; Dataset Generation Pipeline","text":"<ol> <li>Create Project (Projects tab)</li> <li>Process Documents (Document Processing tab)</li> <li>Upload medical PDFs</li> <li>Use Gemini parser for document processing</li> <li> <p>Configure chunking with appropriate size and overlap</p> </li> <li> <p>Generate Taxonomy (Taxonomy \u2192 Build Taxonomy tab)</p> </li> <li>AI generation mode for medical domain</li> <li>Analyze processed documents for category discovery</li> <li> <p>Create hierarchical taxonomy structure</p> </li> <li> <p>Run Advanced Entity Extraction (Extraction \u2192 Run Extraction tab)</p> </li> <li>Select Grok AI model for high accuracy</li> <li>Choose taxonomy with medical categories</li> <li>Configure extraction parameters (depth, confidence, batch size)</li> <li>Select specific categories (symptoms, diagnoses, medications)</li> <li> <p>Start extraction job and monitor real-time progress</p> </li> <li> <p>Monitor Extraction Jobs (Extraction \u2192 Monitor Jobs tab)</p> </li> <li>Track job status and progress updates</li> <li>View detailed job parameters and timing</li> <li> <p>Handle any failed jobs with restart functionality</p> </li> <li> <p>Analyze Extraction Results (Extraction \u2192 View Results tab)</p> </li> <li>Review extracted entities by category</li> <li>Examine relationship discoveries between entities</li> <li>Filter results by confidence scores</li> <li> <p>Analyze entity frequency and distribution</p> </li> <li> <p>Generate Q&amp;A Dataset (Extraction \u2192 View Results tab)</p> </li> <li>Use one-click Q&amp;A generation from relationships</li> <li>Preview generated question-answer pairs</li> <li>Customize templates for medical education</li> <li> <p>Export in JSONL format for ML training</p> </li> <li> <p>Quality Assurance (Quality Metrics tab)</p> </li> <li>Analyze generated Q&amp;A dataset quality</li> <li>Review diversity, consistency, and relevance metrics</li> <li> <p>Validate medical accuracy of generated content</p> </li> <li> <p>Advanced Analysis (Benchmarking tab)</p> </li> <li>Test AI models on generated medical datasets</li> <li>Compare performance across different benchmarks</li> <li>Validate dataset effectiveness for training</li> </ol>"},{"location":"userGuide/gui_user_guide/#quick-dataset-generation","title":"Quick Dataset Generation","text":"<p>For users with existing processed content:</p> <ol> <li>Select project with processed documents</li> <li>Go to Core Dataset Generation</li> <li>Set basic parameters (mode, format, workers)</li> <li>Configure high-level prompts</li> <li>Generate dataset</li> <li>Review results</li> </ol>"},{"location":"userGuide/gui_user_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/gui_user_guide/#common-issues","title":"Common Issues","text":""},{"location":"userGuide/gui_user_guide/#gui-is-frozen-during-processing","title":"\"GUI is frozen during processing\"","text":"<ul> <li>This issue has been resolved with the new asynchronous job queuing system. All long-running operations now run in the background, keeping the GUI responsive. You can monitor job progress in the Job Queue sidebar or the Job Management page.</li> </ul>"},{"location":"userGuide/gui_user_guide/#job-stuck-in-pendingrunning","title":"\"Job stuck in pending/running\"","text":"<ul> <li>Check the Job Queue sidebar or Job Management page for detailed status.</li> <li>Verify that worker processes are running and connected to Redis.</li> <li>Check server logs for errors related to job execution or resource limits.</li> </ul>"},{"location":"userGuide/gui_user_guide/#job-failed-unexpectedly","title":"\"Job failed unexpectedly\"","text":"<ul> <li>Review the job details in the Job Management page for error messages.</li> <li>Check server logs for detailed traceback information.</li> <li>Ensure all required API keys are configured in the Settings tab.</li> <li>Restart the job if it's a transient error.</li> </ul>"},{"location":"userGuide/gui_user_guide/#too-many-concurrent-jobs","title":"\"Too many concurrent jobs\"","text":"<ul> <li>Adjust the \"Max Concurrent Jobs (Global)\" or \"Max Concurrent Jobs Per User\" settings in the Settings tab.</li> <li>Consider scaling up your worker processes if you have available resources.</li> </ul>"},{"location":"userGuide/gui_user_guide/#performance-tips","title":"Performance Tips","text":""},{"location":"userGuide/gui_user_guide/#no-projects-available","title":"\"No projects available\"","text":"<ul> <li>Create a project first in the Projects tab</li> <li>Check API server is running</li> </ul>"},{"location":"userGuide/gui_user_guide/#api-key-not-configured","title":"\"API key not configured\"","text":"<ul> <li>Go to Settings tab</li> <li>Add required API keys</li> <li>Restart GUI if necessary</li> </ul>"},{"location":"userGuide/gui_user_guide/#no-chunks-found","title":"\"No chunks found\"","text":"<ul> <li>Process documents first in Document Processing tab</li> <li>Check document formats are supported</li> <li>Verify processing completed successfully</li> </ul>"},{"location":"userGuide/gui_user_guide/#taxonomy-generation-failed","title":"\"Taxonomy generation failed\"","text":"<ul> <li>Check document content quality</li> <li>Try different domain settings</li> <li>Reduce sample size if needed</li> </ul>"},{"location":"userGuide/gui_user_guide/#dataset-generation-timeout","title":"\"Dataset generation timeout\"","text":"<ul> <li>Reduce concurrent workers</li> <li>Decrease datasets per chunk</li> <li>Process in smaller batches</li> </ul>"},{"location":"userGuide/gui_user_guide/#invalid-escape-error-when-using-schema-chunking","title":"\"Invalid \\escape error when using schema chunking\"","text":"<ul> <li>This occurs when copying AI-recommended JSON schemas into the GUI text area</li> <li>The system automatically detects and fixes this issue - look for the \"\ud83d\udd27 Auto-fixed JSON schema backslash escaping issues\" message</li> <li>If the error persists, try re-pasting the JSON from the AI recommendations dialog</li> <li>The GUI includes automatic validation and correction for common JSON formatting issues</li> </ul>"},{"location":"userGuide/gui_user_guide/#performance-tips_1","title":"Performance Tips","text":"<ul> <li>Memory Usage: Reduce concurrent workers on low-memory systems</li> <li>Processing Speed: Use appropriate chunk sizes (smaller = faster processing)</li> <li>Quality vs Speed: Disable quality analysis for faster generation</li> <li>Batch Processing: Process multiple documents together when possible</li> </ul>"},{"location":"userGuide/gui_user_guide/#api-integration","title":"API Integration","text":"<p>The GUI uses REST API endpoints for all operations. You can also use these endpoints directly:</p> <pre><code>import requests\n\n# Example: Generate dataset\ndata = {\n    \"project_id\": 123,\n    \"generation_mode\": \"default\",\n    \"custom_audience\": \"medical residents\",\n    \"datasets_per_chunk\": 3\n}\n\nresponse = requests.post(\"http://localhost:8000/api/v1/datasets/generate\", json=data)\n</code></pre>"},{"location":"userGuide/gui_user_guide/#support-and-resources","title":"Support and Resources","text":"<ul> <li>Documentation: Check <code>docs/</code> folder for detailed guides</li> <li>Logging System: See Logging System Guide for details on log levels and configuration.</li> <li>CLI Reference: See <code>docs/parametersTree.md</code> for command-line options</li> <li>API Documentation: Available at <code>http://localhost:8000/docs</code> when API server is running</li> <li>Logs: Check terminal output for detailed error messages</li> </ul>"},{"location":"userGuide/gui_user_guide/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>AI Model Selection: Choose Grok for accuracy, Gemini for speed, based on your quality vs. performance needs</li> <li>Taxonomy Design: Create domain-specific taxonomies that reflect real-world entity relationships</li> <li>Category Targeting: Select specific categories rather than extracting everything for better quality</li> <li>Confidence Tuning: Adjust confidence thresholds based on domain requirements and use case sensitivity</li> <li>Multi-Model Validation: Enable validation with different AI models for critical applications</li> <li>Relationship Analysis: Review discovered relationships to improve taxonomy and extraction quality</li> <li>Q&amp;A Customization: Use domain-specific templates and customize prompts for your target audience</li> <li>Quality Assurance: Always validate extraction results and generated datasets before production use</li> <li>Scalable Processing: Monitor resource usage and adjust batch sizes for optimal performance</li> <li>Iterative Improvement: Use extraction results to refine taxonomies and improve future extractions</li> </ol>"},{"location":"userGuide/ingestionapi/","title":"Ingestion Module API Usage Guide","text":"<p>The Compileo Ingestion API provides programmatic access to document parsing and processing capabilities. This guide covers all ingestion-related API endpoints with examples and best practices.</p>"},{"location":"userGuide/ingestionapi/#base-url-apiv1","title":"Base URL: <code>/api/v1</code>","text":"<p>All ingestion operations are performed through the main API endpoints. This guide focuses on the ingestion-specific functionality.</p>"},{"location":"userGuide/ingestionapi/#document-parsing-api","title":"Document Parsing API","text":""},{"location":"userGuide/ingestionapi/#parse-documents","title":"Parse Documents","text":"<p>Endpoint: <code>POST /api/v1/documents/parse</code></p> <p>Initiates asynchronous document parsing with automatic PDF splitting support.</p> <p>Request Body: <pre><code>{\n  \"project_id\": 1,\n  \"document_ids\": [1, 2, 3],\n  \"parser\": \"gemini\",\n  \"pages_per_split\": 5,\n  \"overlap_pages\": 0\n}\n</code></pre></p> <p>Parameters: - <code>project_id</code> (integer, required): Target project ID - <code>document_ids</code> (array, required): List of document IDs to parse - <code>parser</code> (string, optional): AI parser (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>pypdf</code>, <code>unstructured</code>, <code>huggingface</code>, <code>novlm</code>) - <code>pages_per_split</code> (integer, optional): Pages per PDF chunk (default: 5) - <code>overlap_pages</code> (integer, optional): Overlap pages between chunks (default: 0)</p> <p>Response: <pre><code>{\n  \"job_id\": \"parse-job-uuid-123\",\n  \"message\": \"Parsing job submitted successfully\",\n  \"document_count\": 3\n}\n</code></pre></p> <p>PDF Splitting &amp; Analysis Architecture: - PDFs are automatically split when <code>total_pages &gt; pages_per_split</code>. - Two-Pass VLM Strategy: For VLM parsers (<code>grok</code>, <code>gemini</code>, <code>openai</code>, <code>ollama</code>, <code>huggingface</code>), a structure analysis is performed before parsing content.     - Structure Skim: The system analyzes the middle chunk (or full file if not split) to detect the visual hierarchy (fonts, sizes for H1, H2, etc.) at 300 DPI.     - Style Guide: A resulting \"Style Guide\" is generated and injected into the prompt for every page parsed. - Each chunk is parsed individually with the selected AI model, using the global Style Guide for consistent heading hierarchy. - Results are stored as separate JSON files per chunk. - Manifest file tracks split relationships and metadata.</p>"},{"location":"userGuide/ingestionapi/#document-upload-api","title":"Document Upload API","text":""},{"location":"userGuide/ingestionapi/#upload-documents","title":"Upload Documents","text":"<p>Endpoint: <code>POST /api/v1/documents/upload</code></p> <p>Uploads documents to a project with automatic ingestion processing.</p> <p>Request Body (multipart/form-data): - <code>files</code>: Document files (PDF, DOCX, TXT, etc.) - <code>project_id</code>: Target project ID - <code>auto_parse</code>: Automatically start parsing after upload (default: false) - <code>parser</code>: Parser to use if auto_parse is enabled - <code>pages_per_split</code>: PDF splitting configuration</p> <p>Response: <pre><code>{\n  \"job_id\": \"upload-job-uuid-456\",\n  \"message\": \"Documents uploaded successfully\",\n  \"uploaded_count\": 2,\n  \"document_ids\": [4, 5]\n}\n</code></pre></p>"},{"location":"userGuide/ingestionapi/#parser-configuration","title":"Parser Configuration","text":""},{"location":"userGuide/ingestionapi/#available-parsers","title":"Available Parsers","text":"Parser Description API Key Required GPU Support <code>gemini</code> Google Gemini with file upload Yes Yes <code>grok</code> xAI Grok with preprocessing Yes No <code>ollama</code> Local Ollama models No No <code>pypdf</code> Direct PDF text extraction No No <code>unstructured</code> Office document parsing No No <code>huggingface</code> GPU-accelerated OCR Yes Yes <code>novlm</code> Intelligent parser selection No No"},{"location":"userGuide/ingestionapi/#parser-specific-settings","title":"Parser-Specific Settings","text":"<p>Ollama Parser: <pre><code>{\n  \"parser\": \"ollama\",\n  \"model\": \"llama2:7b\",\n  \"temperature\": 0.1,\n  \"num_predict\": 512\n}\n</code></pre></p> <p>Gemini Parser: <pre><code>{\n  \"parser\": \"gemini\",\n  \"model\": \"gemini-2.5-flash\"\n}\n</code></pre></p>"},{"location":"userGuide/ingestionapi/#pdf-splitting-details","title":"PDF Splitting Details","text":""},{"location":"userGuide/ingestionapi/#automatic-splitting-logic","title":"Automatic Splitting Logic","text":"<p>PDFs are automatically split when: - <code>total_pages &gt; pages_per_split</code> (default: 5) - Document exceeds AI model token limits - Improves parsing quality for large documents</p>"},{"location":"userGuide/ingestionapi/#split-file-structure","title":"Split File Structure","text":"<pre><code>storage/uploads/{project_id}/\n\u251c\u2500\u2500 {doc_id}_{filename}_manifest.json    # Split metadata\n\u251c\u2500\u2500 {doc_id}_{filename}_chunk_001.pdf   # Pages 1-5\n\u251c\u2500\u2500 {doc_id}_{filename}_chunk_002.pdf   # Pages 6-10\n\u2514\u2500\u2500 {doc_id}_{filename}_chunk_003.pdf   # Pages 11-15\n\nstorage/parsed/{project_id}/\n\u251c\u2500\u2500 {doc_id}_1.json    # Parsed chunk 1\n\u251c\u2500\u2500 {doc_id}_2.json    # Parsed chunk 2\n\u2514\u2500\u2500 {doc_id}_3.json    # Parsed chunk 3\n</code></pre>"},{"location":"userGuide/ingestionapi/#manifest-file-format","title":"Manifest File Format","text":"<pre><code>{\n  \"original_file\": \"document.pdf\",\n  \"total_pages\": 15,\n  \"pages_per_split\": 5,\n  \"overlap_pages\": 0,\n  \"splits\": [\n    {\n      \"chunk_id\": 1,\n      \"start_page\": 1,\n      \"end_page\": 5,\n      \"filename\": \"chunk_001.pdf\"\n    },\n    {\n      \"chunk_id\": 2,\n      \"start_page\": 6,\n      \"end_page\": 10,\n      \"filename\": \"chunk_002.pdf\"\n    },\n    {\n      \"chunk_id\": 3,\n      \"start_page\": 11,\n      \"end_page\": 15,\n      \"filename\": \"chunk_003.pdf\"\n    }\n  ]\n}\n</code></pre>"},{"location":"userGuide/ingestionapi/#job-monitoring","title":"Job Monitoring","text":""},{"location":"userGuide/ingestionapi/#check-parse-job-status","title":"Check Parse Job Status","text":"<p>Endpoint: <code>GET /api/v1/jobs/status/{job_id}</code></p> <p>Response: <pre><code>{\n  \"job_id\": \"parse-job-uuid-123\",\n  \"status\": \"completed\",\n  \"progress\": 1.0,\n  \"created_at\": \"2025-11-11T10:25:53Z\",\n  \"completed_at\": \"2025-11-11T10:27:23Z\",\n  \"result\": {\n    \"processed_documents\": 3,\n    \"total_chunks_created\": 9,\n    \"parser\": \"gemini\",\n    \"pages_per_split\": 5\n  }\n}\n</code></pre></p>"},{"location":"userGuide/ingestionapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/ingestionapi/#1-parser-selection","title":"1. Parser Selection","text":"<p>For Speed: - Use <code>pypdf</code> for simple text extraction - Use <code>unstructured</code> for Office documents</p> <p>For Quality: - Use <code>gemini</code> or <code>grok</code> for complex layouts - Use <code>ollama</code> for local processing</p>"},{"location":"userGuide/ingestionapi/#2-pdf-splitting","title":"2. PDF Splitting","text":"<p>Small Documents (&lt; 50 pages): - Set <code>pages_per_split: 10</code> to avoid unnecessary splitting</p> <p>Large Documents (&gt; 200 pages): - Set <code>pages_per_split: 5</code> for optimal parsing - Use <code>overlap_pages: 1</code> for context continuity</p>"},{"location":"userGuide/ingestionapi/#3-batch-processing","title":"3. Batch Processing","text":"<p>Optimal Batch Sizes: - 5-10 documents per job for AI parsers - 20-50 documents per job for fast parsers - Monitor job queue status to avoid overload</p>"},{"location":"userGuide/ingestionapi/#4-error-handling","title":"4. Error Handling","text":"<p>Common Issues: - API key validation before job submission - File format verification - Storage space monitoring - HuggingFace Network Timeouts: The <code>huggingface</code> parser downloads a large (~6GB) model. In Docker, this can hang due to network/SSL issues. Pre-populating the <code>compileo_hf_models</code> volume with model weights is recommended if parsing jobs remain stuck in the \"running\" state without progress.</p> <p>Retry Logic: - Failed jobs can be restarted - Different parsers can be tried - Chunk size adjustment for parsing failures</p>"},{"location":"userGuide/ingestionapi/#api-examples","title":"API Examples","text":""},{"location":"userGuide/ingestionapi/#python-client-example","title":"Python Client Example","text":"<pre><code>import requests\n\n# Parse documents with PDF splitting\nresponse = requests.post(\n    \"http://localhost:8000/api/v1/documents/parse\",\n    json={\n        \"project_id\": 1,\n        \"document_ids\": [1, 2, 3],\n        \"parser\": \"gemini\",\n        \"pages_per_split\": 5,\n        \"overlap_pages\": 0\n    }\n)\n\njob_id = response.json()[\"job_id\"]\n\n# Monitor job progress\nwhile True:\n    status = requests.get(f\"http://localhost:8000/api/v1/jobs/status/{job_id}\")\n    if status.json()[\"status\"] == \"completed\":\n        break\n    time.sleep(5)\n</code></pre>"},{"location":"userGuide/ingestionapi/#curl-example","title":"cURL Example","text":"<pre><code># Parse documents\ncurl -X POST http://localhost:8000/api/v1/documents/parse \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": 1,\n    \"document_ids\": [1, 2],\n    \"parser\": \"gemini\",\n    \"pages_per_split\": 5\n  }'\n</code></pre> <p>This API provides complete programmatic access to Compileo's document ingestion and parsing capabilities, with automatic PDF splitting for optimal AI model performance.</p>"},{"location":"userGuide/ingestiongui/","title":"Ingestion Module GUI Usage Guide","text":"<p>The Compileo Ingestion GUI provides an intuitive web interface for document parsing and processing. This guide covers the document upload, parsing, and PDF splitting features available in the GUI.</p>"},{"location":"userGuide/ingestiongui/#accessing-document-processing","title":"Accessing Document Processing","text":"<p>Navigate to the \"\ud83d\udcc4 Document Processing\" tab from the main menu. The interface provides comprehensive document management capabilities.</p>"},{"location":"userGuide/ingestiongui/#project-selection","title":"Project Selection","text":"<p>Before working with documents, select a project from the dropdown at the top of the page:</p> <ul> <li>Project Selection: Choose from available projects in your workspace</li> <li>Auto-save: Your selection is remembered across browser sessions</li> <li>Validation: System prevents operations without a valid project selection</li> </ul>"},{"location":"userGuide/ingestiongui/#document-upload-parsing","title":"Document Upload &amp; Parsing","text":""},{"location":"userGuide/ingestiongui/#file-upload-area","title":"File Upload Area","text":"<ol> <li>Upload Interface:</li> <li>Click \"Browse files\" or drag-and-drop documents onto the upload area</li> <li>Supported Formats: PDF, DOCX, DOC, TXT, MD, CSV, JSON, XML, Images (PNG, JPEG, WEBP)</li> <li>Multiple Files: Upload several documents simultaneously</li> <li> <p>File Size Limits: Maximum 200MB per file</p> </li> <li> <p>Website Scraping (if plugin installed):</p> </li> <li>If the Scrapy-Playwright plugin is installed, a toggle will appear to select between \"Upload\" and \"Website\" mode.</li> <li>URL Input: Enter the full URL (e.g., <code>https://example.com</code>) to scrape.</li> <li>Depth: Select crawling depth (default 1 for single page).</li> <li> <p>The system will scrape the website content and process it as a document.</p> </li> <li> <p>PDF Splitting Configuration (appears after file selection):</p> </li> <li>Pages per Split: Number of pages per PDF chunk (default: 5, recommended: 5-10)</li> <li>Overlap Pages: Overlapping pages between chunks for context continuity (default: 0)</li> <li>Purpose: Automatically splits large PDFs to prevent AI token limit issues</li> </ol>"},{"location":"userGuide/ingestiongui/#parser-selection","title":"Parser Selection","text":"<p>The GUI features Reactive Parser Filtering. When you upload files, the system automatically detects their types and restricts the available parsers to only those capable of processing your specific documents.</p> Parser Best For Supported Formats API Key gemini Complex layouts, Vision PDF, TXT, MD, CSV, Images Yes grok Technical PDFs PDF Yes ollama Local processing PDF, TXT, MD No pypdf Simple extraction PDF No unstructured Office, MD, Data ALL (DOCX, CSV, XML, etc.) No huggingface Advanced OCR PDF Yes novlm Smart routing ALL (Auto-selects best engine) No <p>Ollama Configuration: When using Ollama parsers, fine-tune AI behavior in Settings \u2192 AI Model Configuration with parameters like temperature, repeat penalty, and token limits.</p>"},{"location":"userGuide/ingestiongui/#document-selection-management","title":"Document Selection &amp; Management","text":"<p>Existing Documents: - View all documents in the selected project - Status Indicators:   - \u2705 Parsed: Successfully converted to structured markdown   - \ud83d\udcc4 Uploaded: Document uploaded but not yet parsed   - \ud83d\udd04 Processing: Currently being parsed   - \u274c Failed: Parsing encountered errors - Bulk Selection: Check boxes to select multiple documents - Delete Function: Click \ud83d\uddd1\ufe0f to remove documents with confirmation</p> <p>Combined Operations: - Upload new files AND select existing documents simultaneously - System processes both uploaded and selected documents in a single operation</p>"},{"location":"userGuide/ingestiongui/#pdf-splitting-integration","title":"PDF Splitting Integration","text":""},{"location":"userGuide/ingestiongui/#automatic-pdf-processing","title":"Automatic PDF Processing","text":"<p>When you upload or parse PDFs, the system automatically:</p> <ol> <li>Page Count Analysis: Checks total pages in each PDF</li> <li>Splitting Decision: Splits PDFs when <code>total_pages &gt; pages_per_split</code></li> <li>Chunk Creation: Creates individual PDF files for each page range</li> <li>Manifest Generation: Creates metadata file tracking all splits</li> <li>Structure Analysis (VLM): For VLM parsers, the system analyzes the middle chunk (or full file) to generate a \"Style Guide\" based on the document's visual hierarchy (fonts, sizes).</li> <li>Context-Aware Parsing: Each chunk is parsed separately with your chosen AI model, using the generated Style Guide to ensure consistent headings (#, ##, ###) and clean output (no icons).</li> </ol>"},{"location":"userGuide/ingestiongui/#split-file-organization","title":"Split File Organization","text":"<p>Upload Directory Structure: <pre><code>storage/uploads/{project_id}/\n\u251c\u2500\u2500 {document_id}_{original_name}_manifest.json\n\u251c\u2500\u2500 {document_id}_{original_name}_chunk_001.pdf  # Pages 1-5\n\u251c\u2500\u2500 {document_id}_{original_name}_chunk_002.pdf  # Pages 6-10\n\u2514\u2500\u2500 {document_id}_{original_name}_chunk_003.pdf  # Pages 11-15\n</code></pre></p> <p>Parsed Results: <pre><code>storage/parsed/{project_id}/\n\u251c\u2500\u2500 {document_id}_1.json    # Parsed content from chunk 1\n\u251c\u2500\u2500 {document_id}_2.json    # Parsed content from chunk 2\n\u2514\u2500\u2500 {document_id}_3.json    # Parsed content from chunk 3\n</code></pre></p>"},{"location":"userGuide/ingestiongui/#manifest-file-details","title":"Manifest File Details","text":"<p>The manifest file contains complete splitting metadata:</p> <pre><code>{\n  \"original_file\": \"medical_textbook.pdf\",\n  \"total_pages\": 150,\n  \"pages_per_split\": 5,\n  \"overlap_pages\": 0,\n  \"splits\": [\n    {\n      \"chunk_id\": 1,\n      \"start_page\": 1,\n      \"end_page\": 5,\n      \"filename\": \"chunk_001.pdf\"\n    },\n    {\n      \"chunk_id\": 2,\n      \"start_page\": 6,\n      \"end_page\": 10,\n      \"filename\": \"chunk_002.pdf\"\n    }\n  ]\n}\n</code></pre>"},{"location":"userGuide/ingestiongui/#parsing-execution","title":"Parsing Execution","text":""},{"location":"userGuide/ingestiongui/#start-parsing-process","title":"Start Parsing Process","text":"<ol> <li>Parse Button: Shows count of documents to be processed</li> <li>Progress Monitoring: Real-time status updates with progress bars</li> <li>Background Processing: Large parsing jobs run asynchronously</li> <li>Job Tracking: Each parsing operation gets a unique job ID</li> </ol>"},{"location":"userGuide/ingestiongui/#job-status-monitoring","title":"Job Status Monitoring","text":"<p>Active Jobs Display: - Real-time progress bars for current operations - Estimated completion times - Current processing phase (uploading, splitting, parsing) - Error notifications with retry options</p> <p>Job History: - Previous parsing jobs with success/failure status - Performance metrics (processing time, pages parsed) - Detailed error logs for troubleshooting</p>"},{"location":"userGuide/ingestiongui/#content-preview-validation","title":"Content Preview &amp; Validation","text":""},{"location":"userGuide/ingestiongui/#parsed-content-viewer","title":"Parsed Content Viewer","text":"<p>Access Parsed Results: - Click document names to view parsed markdown content - Pagination Support: Navigate through large documents (10,000+ characters per page) - Search Functionality: Find specific content within parsed documents - Export Options: Copy content or download as files</p>"},{"location":"userGuide/ingestiongui/#multi-part-document-handling","title":"Multi-Part Document Handling","text":"<p>Split Document Management: - System automatically detects documents split into multiple parts - File Selection: Choose which chunk to view from dropdown - Metadata Display: Shows page ranges and overlap information - Content Preview: Verify parsing quality before further processing</p>"},{"location":"userGuide/ingestiongui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/ingestiongui/#1-document-preparation","title":"1. Document Preparation","text":"<p>File Organization: - Use consistent naming conventions for related documents - Group documents by project or content type - Check file sizes before upload (large PDFs will be automatically split)</p> <p>Format Selection: - PDFs: Best for scanned documents, complex layouts, or images - DOCX/TXT: Ideal for text-heavy content and structured documents - MD: Use for already structured or pre-processed content</p>"},{"location":"userGuide/ingestiongui/#2-parser-selection-strategy","title":"2. Parser Selection Strategy","text":"<p>For Speed &amp; Cost Efficiency: - Use <code>pypdf</code> for simple text extraction (no API costs) - Use <code>unstructured</code> for Office documents and structured content - Use <code>ollama</code> for local processing without internet dependency</p> <p>For Quality &amp; Complex Content (VLM Parsers): These parsers use a Two-Pass Strategy (Skim + Parse) for superior structure detection: - Use <code>gemini</code> for documents with images, tables, or complex layouts. - Use <code>grok</code> for technical documentation and research papers. - Use <code>huggingface</code> for scanned documents requiring OCR. - Use <code>openai</code> or <code>ollama</code> for versatile, vision-based parsing.</p>"},{"location":"userGuide/ingestiongui/#3-pdf-splitting-optimization","title":"3. PDF Splitting Optimization","text":"<p>Small Documents (&lt; 50 pages): - Default <code>pages_per_split: 5</code> works well - Consider increasing to 10 if parsing speed is priority</p> <p>Large Documents (&gt; 200 pages): - Keep <code>pages_per_split: 5</code> for optimal AI model performance - Consider <code>overlap_pages: 1</code> for documents where context spans page boundaries</p> <p>Special Cases: - Image-heavy PDFs: Smaller chunks (3-5 pages) for better OCR accuracy - Text-dense PDFs: Larger chunks (8-10 pages) for better context preservation</p>"},{"location":"userGuide/ingestiongui/#4-batch-processing-guidelines","title":"4. Batch Processing Guidelines","text":"<p>Optimal Batch Sizes: - AI Parsers (gemini, grok): 3-5 documents per job - Fast Parsers (pypdf, unstructured): 10-20 documents per job - Mixed Batches: Group similar document types together</p> <p>Monitoring &amp; Scaling: - Monitor job queue status to avoid system overload - Use job history to track processing times and success rates - Scale batch sizes based on observed performance</p>"},{"location":"userGuide/ingestiongui/#5-quality-assurance","title":"5. Quality Assurance","text":"<p>Content Verification: - Always preview parsed content before chunking or dataset generation - Check for parsing artifacts, missing content, or formatting issues - Validate that PDF splitting preserved document structure</p> <p>Error Handling: - Failed jobs can be restarted with different parser settings - Check API key validity for cloud-based parsers - Monitor system resources during large batch operations</p>"},{"location":"userGuide/ingestiongui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/ingestiongui/#common-upload-issues","title":"Common Upload Issues","text":"<p>File Size Limits: - Maximum 100MB per file - Large PDFs are automatically split, but consider pre-splitting very large files - Check available disk space before large uploads</p> <p>Unsupported Formats: - Verify file extensions match supported formats - Some file types may require specific parser selection</p>"},{"location":"userGuide/ingestiongui/#parsing-problems","title":"Parsing Problems","text":"<p>API Key Issues: - Verify API keys are configured in Settings for cloud parsers - Check API key validity and rate limits - Consider switching to local parsers (ollama, pypdf) if API issues persist</p> <p>Document Corruption: - Ensure PDF files are not corrupted or password-protected - Try different parsers for problematic documents - Check file encoding for text-based documents</p> <p>HuggingFace Model Download Issues: - The Nanonets-OCR2-3B model is large (~6GB) and can hang during download in certain Docker network environments. - Symptoms: Parsing jobs \"hang\" at the model loading stage without errors, or SSL connection errors appear in logs. - Solution: Pre-populate the <code>compileo_hf_models</code> Docker volume by manually copying model weights if download issues persist. - GPU Driver Mismatch: If HuggingFace defaults to CPU, ensure your host NVIDIA driver supports CUDA 13.0.</p> <p>Memory Issues: - Large documents may require more system memory - Consider splitting very large PDFs into smaller chunks - Monitor system resources during processing</p>"},{"location":"userGuide/ingestiongui/#pdf-splitting-issues","title":"PDF Splitting Issues","text":"<p>Unexpected Splitting: - PDFs are automatically split when <code>total_pages &gt; pages_per_split</code> - Adjust <code>pages_per_split</code> setting if splitting is too aggressive - Some documents may benefit from manual pre-splitting</p> <p>Split Quality Problems: - Page boundaries may split content inappropriately - Use <code>overlap_pages: 1</code> to maintain context across splits - Review manifest files to verify split quality</p>"},{"location":"userGuide/ingestiongui/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"userGuide/ingestiongui/#custom-parser-settings","title":"Custom Parser Settings","text":"<p>Ollama Advanced Configuration: <pre><code>{\n  \"temperature\": 0.1,\n  \"repeat_penalty\": 1.1,\n  \"top_p\": 0.9,\n  \"top_k\": 40,\n  \"num_predict\": 512,\n  \"seed\": 42\n}\n</code></pre></p> <p>HuggingFace Optimization: - Automatic GPU detection and utilization - Model caching for improved performance - Batch processing for multiple images</p>"},{"location":"userGuide/ingestiongui/#api-integration","title":"API Integration","text":"<p>Programmatic Access: - All GUI features available via REST API - Batch processing scripts using API endpoints - Integration with external document processing workflows - Automated document pipelines</p> <p>This GUI provides a complete document ingestion workflow with intelligent PDF splitting, multiple parser options, and comprehensive monitoring for efficient and high-quality document processing.</p>"},{"location":"userGuide/jobhandlingapi/","title":"Job Handling Module API Usage Guide","text":"<p>The Compileo Job Handling API provides comprehensive endpoints for monitoring, managing, and controlling background jobs using an enhanced Redis-based queue system with RQ (Redis Queue).</p>"},{"location":"userGuide/jobhandlingapi/#base-url-apiv1jobs","title":"Base URL: <code>/api/v1/jobs</code>","text":""},{"location":"userGuide/jobhandlingapi/#overview","title":"Overview","text":"<p>The job handling system uses Redis Queue (RQ) for reliable background job processing with the following enhancements:</p> <ul> <li>Datetime Compatibility: Proper handling of timezone-aware timestamps</li> <li>Worker Health Monitoring: Automatic cleanup of stale worker processes</li> <li>Comprehensive Job Cleanup: Multi-level cleanup of jobs, registries, and locks</li> <li>Real-time Statistics: Accurate job counts and system monitoring</li> <li>Duplicate Prevention: Atomic status updates prevent job re-execution</li> <li>Resource Limit Enforcement: Strict checking of global and per-user concurrency limits (jobs queue instead of failing)</li> </ul>"},{"location":"userGuide/jobhandlingapi/#get-job-status","title":"Get Job Status","text":""},{"location":"userGuide/jobhandlingapi/#get-statusjob_id","title":"GET <code>/status/{job_id}</code>","text":"<p>Retrieve the current status and details of a specific job.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/jobs/status/550e8400-e29b-41d4-a716-446655440000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"job_type\": \"extraction\",\n  \"status\": \"running\",\n  \"progress\": 0.75,\n  \"created_at\": \"2025-11-12T15:30:00Z\",\n  \"started_at\": \"2025-11-12T15:30:05Z\",\n  \"completed_at\": null,\n  \"error\": null,\n  \"user_id\": \"default_user\",\n  \"metrics\": {\n    \"execution_time_seconds\": 45.2,\n    \"items_processed\": 75,\n    \"total_items\": 100\n  }\n}\n</code></pre></p> <p>Status Values: - <code>pending</code>: Job queued, waiting for worker - <code>running</code>: Job currently being processed - <code>completed</code>: Job finished successfully - <code>failed</code>: Job failed with error - <code>cancelled</code>: Job manually cancelled - <code>scheduled</code>: Job scheduled for future execution</p>"},{"location":"userGuide/jobhandlingapi/#poll-job-status-for-changes","title":"Poll Job Status for Changes","text":""},{"location":"userGuide/jobhandlingapi/#get-statusjob_idpoll","title":"GET <code>/status/{job_id}/poll</code>","text":"<p>Long-polling endpoint that waits for status changes or timeout.</p> <p>Parameters: - <code>current_status</code> (optional): Current known status - returns immediately if different - <code>timeout</code> (optional, default: 30): Maximum seconds to wait</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/jobs/status/550e8400-e29b-41d4-a716-446655440000/poll?timeout=60\"\n</code></pre></p>"},{"location":"userGuide/jobhandlingapi/#stream-job-status-server-sent-events","title":"Stream Job Status (Server-Sent Events)","text":""},{"location":"userGuide/jobhandlingapi/#get-statusjob_idstream","title":"GET <code>/status/{job_id}/stream</code>","text":"<p>Real-time job status updates via Server-Sent Events.</p> <p>Response (text/event-stream): <pre><code>event: job_update\ndata: {\"job_id\": \"550e8400-e29b-41d4-a716-446655440000\", \"status\": \"running\", \"progress\": 0.1}\n\nevent: job_update\ndata: {\"job_id\": \"550e8400-e29b-41d4-a716-446655440000\", \"status\": \"running\", \"progress\": 0.5}\n\nevent: job_complete\ndata: {\"job_id\": \"550e8400-e29b-41d4-a716-446655440000\", \"status\": \"completed\", \"result\": {...}}\n</code></pre></p>"},{"location":"userGuide/jobhandlingapi/#get-queue-statistics","title":"Get Queue Statistics","text":""},{"location":"userGuide/jobhandlingapi/#get-queuestats","title":"GET <code>/queue/stats</code>","text":"<p>Comprehensive queue statistics and system health metrics.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/jobs/queue/stats\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"pending_jobs\": 3,\n  \"running_jobs\": 2,\n  \"scheduled_jobs\": 1,\n  \"completed_jobs\": 150,\n  \"failed_jobs\": 2,\n  \"cancelled_jobs\": 1,\n  \"total_jobs\": 159,\n  \"queue_type\": \"redis\",\n  \"cache_size\": 100,\n  \"active_workers\": 2,\n  \"cpu_usage_percent\": 45.2,\n  \"memory_usage_mb\": 850,\n  \"global_max_concurrent_jobs\": 10,\n  \"per_user_max_concurrent_jobs\": 3,\n  \"worker_health_status\": \"healthy\",\n  \"last_cleanup\": \"2025-11-12T15:25:00Z\"\n}\n</code></pre></p>"},{"location":"userGuide/jobhandlingapi/#cancel-job","title":"Cancel Job","text":""},{"location":"userGuide/jobhandlingapi/#post-canceljob_id","title":"POST <code>/cancel/{job_id}</code>","text":"<p>Cancel a pending or running job.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/jobs/cancel/550e8400-e29b-41d4-a716-446655440000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"cancelled\",\n  \"message\": \"Job cancelled successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/jobhandlingapi/#restart-failed-job","title":"Restart Failed Job","text":""},{"location":"userGuide/jobhandlingapi/#post-restartjob_id","title":"POST <code>/restart/{job_id}</code>","text":"<p>Restart a failed or cancelled job with fresh parameters.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/jobs/restart/550e8400-e29b-41d4-a716-446655440000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"pending\",\n  \"message\": \"Job restarted and re-queued\"\n}\n</code></pre></p>"},{"location":"userGuide/jobhandlingapi/#rq-system-features","title":"RQ System Features","text":""},{"location":"userGuide/jobhandlingapi/#enhanced-reliability-features","title":"Enhanced Reliability Features","text":"<p>1. Datetime Compatibility - Automatic handling of timezone-aware vs naive timestamps - Prevents \"can't subtract offset-naive and offset-aware datetimes\" errors - Compatible with RQ's internal timestamp handling</p> <p>2. Worker Health Monitoring - Continuous monitoring of worker processes - Automatic cleanup of stale workers (&gt;5 minutes old) - Prevents accumulation of dead worker registrations</p> <p>3. Comprehensive Job Cleanup - Multi-level cleanup every 10 minutes:   - RQ failed jobs registry (immediate cleanup)   - RQ finished jobs registry (24-hour retention)   - Custom job storage (2-hour retention)   - Orphaned processing locks (10-minute cleanup) - Startup cleanup removes jobs older than 1 hour</p> <p>4. Duplicate Execution Prevention - Atomic status updates in Redis prevent race conditions - Early status validation before job execution - UUID-based job IDs ensure uniqueness - Processing locks prevent concurrent execution</p>"},{"location":"userGuide/jobhandlingapi/#job-types-supported","title":"Job Types Supported","text":"<ul> <li>extraction: Taxonomy-based content extraction</li> <li>document_processing: Parse and chunk documents</li> <li>taxonomy_processing: Generate taxonomies from chunks</li> <li>dataset_generation: Create datasets from extractions</li> </ul>"},{"location":"userGuide/jobhandlingapi/#error-handling","title":"Error Handling","text":"<p>Common Error Responses: <pre><code>{\n  \"error\": \"JobNotFoundError\",\n  \"message\": \"Job 550e8400-e29b-41d4-a716-446655440000 not found\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\"\n}\n</code></pre></p> <pre><code>{\n  \"error\": \"JobAlreadyCompletedError\",\n  \"message\": \"Cannot cancel job that is already completed\",\n  \"job_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"current_status\": \"completed\"\n}\n</code></pre>"},{"location":"userGuide/jobhandlingapi/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Job Submission: &lt; 100ms average latency</li> <li>Status Queries: &lt; 50ms average response time</li> <li>Queue Throughput: 100+ jobs/minute sustained</li> <li>Memory Usage: &lt; 2GB per worker process</li> <li>Cleanup Frequency: Every 10 minutes (comprehensive)</li> <li>Worker Monitoring: Every 5 minutes (health checks)</li> </ul>"},{"location":"userGuide/jobhandlingapi/#best-practices","title":"Best Practices","text":"<ol> <li>Use Streaming for Real-time Updates: Prefer <code>/stream</code> endpoint for live job monitoring</li> <li>Implement Proper Error Handling: Always check response status and handle errors gracefully</li> <li>Monitor Queue Statistics: Use <code>/queue/stats</code> for system health monitoring</li> <li>Cancel Unnecessary Jobs: Clean up jobs that are no longer needed</li> <li>Handle Timeouts: Implement client-side timeouts for long-running operations</li> </ol>"},{"location":"userGuide/jobhandlingapi/#troubleshooting","title":"Troubleshooting","text":"<p>Job Stuck in Pending: - Check worker health: <code>GET /queue/stats</code> - verify active workers &gt; 0 - Check Redis connectivity - Review worker logs for errors</p> <p>Job Shows Incorrect Status: - Wait for cache refresh (30 seconds) or force refresh - Check if cleanup recently ran (may have updated statistics) - Verify job ID format (should be UUID)</p> <p>High Memory Usage: - Check for worker accumulation: stale workers not cleaned up - Monitor job cleanup frequency - Review large job result storage</p>"},{"location":"userGuide/jobhandlingcli/","title":"Job Handling Module CLI Usage Guide","text":"<p>The Compileo Job Handling CLI provides tools for monitoring and managing background jobs.</p>"},{"location":"userGuide/jobhandlingcli/#command-overview","title":"Command Overview","text":""},{"location":"userGuide/jobhandlingcli/#get-job-status","title":"Get Job Status","text":""},{"location":"userGuide/jobhandlingcli/#compileo-jobs-status-job_id","title":"<code>compileo jobs status &lt;job_id&gt;</code>","text":"<p>Retrieve the current status and details of a specific job.</p> <p>Usage: <pre><code>compileo jobs status b3013a51-4b10-4fca-ac09-239a2b886b7e\n</code></pre></p> <p>Example Output: <pre><code>\ud83d\udcca Job Status:\n  ID: b3013a51-4b10-4fca-ac09-239a2b886b7e\n  Status: completed\n  Progress: 100%\n  Created: 2024-01-21 12:00:00\n  Started: 2024-01-21 12:00:05\n  Completed: 2024-01-21 12:01:00\n</code></pre></p>"},{"location":"userGuide/jobhandlingcli/#cancel-a-job","title":"Cancel a Job","text":""},{"location":"userGuide/jobhandlingcli/#compileo-jobs-cancel-job_id","title":"<code>compileo jobs cancel &lt;job_id&gt;</code>","text":"<p>Cancel a running or pending job.</p> <p>Usage: <pre><code>compileo jobs cancel b3013a51-4b10-4fca-ac09-239a2b886b7e\n</code></pre></p> <p>Example Output: <pre><code>\u2705 Job b3013a51-4b10-4fca-ac09-239a2b886b7e cancelled successfully.\n</code></pre></p>"},{"location":"userGuide/jobhandlingcli/#duplicate-job-execution-prevention","title":"Duplicate Job Execution Prevention","text":"<p>The job handling system includes a critical fix to prevent the same job from being executed multiple times. This is achieved by: 1.  Early Status Check: The system immediately checks if a job's status is already <code>COMPLETED</code>, <code>FAILED</code>, or <code>CANCELLED</code> before execution. If so, the worker exits immediately. 2.  Atomic Status Updates: Job statuses are updated atomically in Redis to prevent race conditions where a job might be picked up by another worker before its status is updated.</p> <p>This ensures that each job is processed exactly once, preventing redundant operations and inconsistent outputs.</p>"},{"location":"userGuide/jobhandlinggui/","title":"Job Handling Module GUI Usage Guide","text":"<p>The Compileo Job Handling GUI provides a comprehensive web interface for monitoring, managing, and controlling background jobs using an enhanced Redis-based queue system.</p>"},{"location":"userGuide/jobhandlinggui/#accessing-the-job-dashboard","title":"Accessing the Job Dashboard","text":"<p>Navigate to the \"\ud83d\udcca Job Dashboard\" page from the main menu to access the job monitoring interface.</p>"},{"location":"userGuide/jobhandlinggui/#job-queue-overview","title":"Job Queue Overview","text":"<p>The dashboard displays real-time job queue statistics and system health:</p>"},{"location":"userGuide/jobhandlinggui/#queue-statistics-panel","title":"Queue Statistics Panel","text":"<ul> <li>Pending Jobs: Jobs waiting in queue</li> <li>Running Jobs: Currently executing jobs</li> <li>Scheduled Jobs: Jobs scheduled for future execution</li> <li>Total Jobs: All jobs in system (with automatic cleanup)</li> <li>Active Workers: Number of healthy worker processes</li> <li>System Health: CPU and memory usage metrics</li> </ul>"},{"location":"userGuide/jobhandlinggui/#enhanced-features","title":"Enhanced Features","text":"<ul> <li>Real-time Updates: Statistics refresh automatically every 30 seconds</li> <li>Worker Health Monitoring: Automatic detection and cleanup of stale workers</li> <li>Job Cleanup Status: Last cleanup time and items removed</li> <li>System Alerts: Warnings for high resource usage or failed jobs</li> </ul>"},{"location":"userGuide/jobhandlinggui/#job-monitoring","title":"Job Monitoring","text":""},{"location":"userGuide/jobhandlinggui/#job-list-view","title":"Job List View","text":"<p>The main job table provides comprehensive job information:</p> <ol> <li> <p>Job Information:</p> <ul> <li>Job ID: Unique UUID identifier</li> <li>Job Type: extraction, document_processing, taxonomy_processing, dataset_generation</li> <li>Status: pending, running, completed, failed, cancelled, scheduled</li> <li>Progress: Percentage complete (0-100%)</li> <li>User: User who submitted the job</li> </ul> </li> <li> <p>Timestamps:</p> <ul> <li>Created: When job was submitted</li> <li>Started: When processing began</li> <li>Completed: When job finished (or failed/cancelled)</li> </ul> </li> <li> <p>Performance Metrics:</p> <ul> <li>Execution Time: Total processing duration</li> <li>Items Processed: Number of items handled</li> <li>Error Messages: Failure details (if applicable)</li> </ul> </li> </ol>"},{"location":"userGuide/jobhandlinggui/#filtering-and-sorting","title":"Filtering and Sorting","text":"<ul> <li>Status Filter: View jobs by status (All, Pending, Running, Completed, Failed, Cancelled)</li> <li>Type Filter: Filter by job type</li> <li>Time Range: Filter jobs by creation date</li> <li>Search: Find jobs by ID or user</li> <li>Sorting: Sort by creation time, status, or progress</li> </ul>"},{"location":"userGuide/jobhandlinggui/#job-management","title":"Job Management","text":""},{"location":"userGuide/jobhandlinggui/#view-job-details","title":"View Job Details","text":"<ol> <li>Select Job: Click on any job row in the table</li> <li>Detailed View: Expand to see complete job information including:<ul> <li>Full parameters and configuration</li> <li>Progress history and milestones</li> <li>Result summaries (for completed jobs)</li> <li>Error details (for failed jobs)</li> <li>Performance metrics and timing</li> </ul> </li> </ol>"},{"location":"userGuide/jobhandlinggui/#cancel-a-job","title":"Cancel a Job","text":"<ol> <li>Locate Target Job: Find the running or pending job in the list</li> <li>Cancel Action: Click the \"\u274c Cancel\" button in the Actions column</li> <li>Confirmation: Confirm cancellation in the dialog prompt</li> <li>Status Update: Job status changes to \"cancelled\" immediately</li> </ol>"},{"location":"userGuide/jobhandlinggui/#restart-failed-jobs","title":"Restart Failed Jobs","text":"<ol> <li>Identify Failed Job: Find job with \"failed\" status</li> <li>Restart Action: Click the \"\ud83d\udd04 Restart\" button</li> <li>Confirmation: Confirm restart with same parameters</li> <li>Re-queue: Job returns to \"pending\" status with fresh execution</li> </ol>"},{"location":"userGuide/jobhandlinggui/#real-time-monitoring","title":"Real-time Monitoring","text":""},{"location":"userGuide/jobhandlinggui/#live-progress-updates","title":"Live Progress Updates","text":"<ul> <li>Progress Bars: Visual progress indicators for running jobs</li> <li>Status Changes: Automatic status updates without page refresh</li> <li>Performance Metrics: Live execution time and throughput updates</li> <li>Worker Status: Real-time worker health and activity monitoring</li> </ul>"},{"location":"userGuide/jobhandlinggui/#server-sent-events-sse","title":"Server-Sent Events (SSE)","text":"<p>The GUI uses Server-Sent Events for real-time updates: - Instant Notifications: Immediate status change alerts - Progress Streaming: Live progress updates during execution - Error Alerts: Real-time failure notifications - Completion Alerts: Success notifications with result summaries</p>"},{"location":"userGuide/jobhandlinggui/#rq-system-features","title":"RQ System Features","text":""},{"location":"userGuide/jobhandlinggui/#enhanced-reliability-features","title":"Enhanced Reliability Features","text":"<p>1. Datetime Compatibility - Automatic handling of timezone-aware timestamps from RQ - Prevents datetime comparison errors during worker monitoring - Compatible with Redis timestamp storage</p> <p>2. Worker Health Management - Continuous monitoring of RQ worker processes - Automatic cleanup of workers unresponsive for &gt;5 minutes - Prevention of dead worker accumulation in Redis</p> <p>3. Comprehensive Job Cleanup - Multi-level Cleanup: Runs every 10 minutes   - RQ failed jobs: Immediate cleanup   - RQ finished jobs: 24-hour retention   - Custom jobs: 2-hour retention   - Processing locks: 10-minute cleanup - Startup Cleanup: Removes jobs &gt;1 hour old on system start - Registry Management: Proper RQ registry maintenance</p> <p>4. Duplicate Execution Prevention - Atomic Redis operations prevent race conditions - Early status validation before job execution - UUID-based job identification - Processing locks for concurrent execution safety</p>"},{"location":"userGuide/jobhandlinggui/#job-types-and-operations","title":"Job Types and Operations","text":""},{"location":"userGuide/jobhandlinggui/#document-processing-jobs","title":"Document Processing Jobs","text":"<ul> <li>Parse Documents: Convert PDFs/text to markdown chunks</li> <li>Chunk Documents: Apply chunking strategies (token, character, semantic, schema)</li> <li>Progress Tracking: Real-time parsing and chunking progress</li> </ul>"},{"location":"userGuide/jobhandlinggui/#taxonomy-processing-jobs","title":"Taxonomy Processing Jobs","text":"<ul> <li>Generate Taxonomy: AI-powered taxonomy creation from document chunks</li> <li>Category Analysis: Hierarchical category structure generation</li> <li>Validation: Taxonomy completeness and consistency checks</li> </ul>"},{"location":"userGuide/jobhandlinggui/#extraction-jobs","title":"Extraction Jobs","text":"<ul> <li>Selective Extraction: Taxonomy-based content extraction</li> <li>Multi-stage Classification: Primary + validation classifiers</li> <li>Confidence Scoring: Result confidence and category matching</li> </ul>"},{"location":"userGuide/jobhandlinggui/#dataset-generation-jobs","title":"Dataset Generation Jobs","text":"<ul> <li>Automated Dataset Creation: Convert extractions to training datasets</li> <li>Format Conversion: JSON, CSV, and other dataset formats</li> <li>Quality Validation: Dataset completeness and consistency checks</li> </ul>"},{"location":"userGuide/jobhandlinggui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/jobhandlinggui/#common-issues","title":"Common Issues","text":"<p>Jobs Stuck in Pending: - Check worker health in queue statistics - Verify Redis connectivity - Review worker logs for startup errors - Check resource limits (CPU/memory)</p> <p>Incorrect Job Counts: - Wait for automatic refresh (30 seconds) - Check last cleanup time in statistics - Verify job status accuracy</p> <p>High Memory Usage: - Monitor worker accumulation - Check for failed job cleanup - Review large result storage</p> <p>Datetime Errors: - System automatically handles timezone issues - Check server logs for RQ compatibility - Verify system clock synchronization</p>"},{"location":"userGuide/jobhandlinggui/#performance-optimization","title":"Performance Optimization","text":""},{"location":"userGuide/jobhandlinggui/#best-practices","title":"Best Practices","text":"<ol> <li>Monitor Queue Health: Use statistics panel for system monitoring</li> <li>Cancel Unnecessary Jobs: Clean up jobs no longer needed</li> <li>Use Appropriate Timeouts: Set reasonable job timeouts</li> <li>Monitor Resource Usage: Watch CPU/memory trends</li> <li>Regular Cleanup: System performs automatic maintenance</li> </ol>"},{"location":"userGuide/jobhandlinggui/#system-limits","title":"System Limits","text":"<ul> <li>Global Max Jobs: 10 concurrent jobs (configurable)</li> <li>Per-User Max Jobs: 3 concurrent jobs (configurable)</li> <li>Job Timeout: 72 hours default (configurable)</li> <li>Result TTL: 500 seconds default (configurable)</li> <li>Cleanup Frequency: Every 10 minutes</li> </ul>"},{"location":"userGuide/jobhandlinggui/#advanced-features","title":"Advanced Features","text":""},{"location":"userGuide/jobhandlinggui/#job-dependencies-future","title":"Job Dependencies (Future)","text":"<ul> <li>Chain jobs with prerequisite relationships</li> <li>Automatic dependency resolution</li> <li>Failure cascade handling</li> </ul>"},{"location":"userGuide/jobhandlinggui/#scheduled-jobs","title":"Scheduled Jobs","text":"<ul> <li>Time-based job execution</li> <li>Cron-style scheduling</li> <li>Recurring job patterns</li> </ul>"},{"location":"userGuide/jobhandlinggui/#bulk-operations","title":"Bulk Operations","text":"<ul> <li>Multiple job cancellation</li> <li>Batch status updates</li> <li>Bulk cleanup operations</li> </ul> <p>This enhanced job handling system provides enterprise-grade reliability with comprehensive monitoring, automatic maintenance, and real-time user feedback for all background processing operations.</p>"},{"location":"userGuide/jobsapi/","title":"Jobs Module API Usage Guide","text":"<p>The Compileo Jobs API provides comprehensive REST endpoints for real-time job monitoring, status tracking, and job management. This API enables clients to monitor long-running background jobs such as document parsing, chunking, taxonomy generation, and dataset creation.</p>"},{"location":"userGuide/jobsapi/#base-url-apiv1jobs","title":"Base URL: <code>/api/v1/jobs</code>","text":""},{"location":"userGuide/jobsapi/#job-status-monitoring","title":"Job Status Monitoring","text":""},{"location":"userGuide/jobsapi/#get-statusjob_id","title":"GET <code>/status/{job_id}</code>","text":"<p>Get the current status of a specific job.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/jobs/status/123e4567-e89b-12d3-a456-426614174000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"job_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"status\": \"running\",\n  \"progress\": 65.5,\n  \"current_step\": \"Processing document 3 of 5\",\n  \"result\": null,\n  \"error\": null,\n  \"created_at\": \"2024-01-21T10:30:00Z\",\n  \"started_at\": \"2024-01-21T10:30:05Z\",\n  \"completed_at\": null,\n  \"updated_at\": \"2024-01-21T10:35:22Z\"\n}\n</code></pre></p> <p>Status Values: - <code>pending</code>: Job is queued and waiting to start - <code>running</code>: Job is currently executing - <code>completed</code>: Job finished successfully - <code>failed</code>: Job failed with an error - <code>cancelled</code>: Job was cancelled by user</p>"},{"location":"userGuide/jobsapi/#long-polling-for-status-changes","title":"Long Polling for Status Changes","text":""},{"location":"userGuide/jobsapi/#get-statusjob_idpoll","title":"GET <code>/status/{job_id}/poll</code>","text":"<p>Wait for job status changes with long polling. Returns immediately when status changes or after timeout.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/jobs/status/123e4567-e89b-12d3-a456-426614174000/poll?timeout=30&amp;current_status=running\"\n</code></pre></p> <p>Query Parameters: - <code>timeout</code>: Maximum wait time in seconds (default: 30, range: 1-300) - <code>current_status</code>: Current known status - returns immediately if status has changed</p> <p>Response: Returns the same format as <code>/status/{job_id}</code> when status changes or timeout occurs.</p>"},{"location":"userGuide/jobsapi/#real-time-status-streaming","title":"Real-Time Status Streaming","text":""},{"location":"userGuide/jobsapi/#get-statusjob_idstream","title":"GET <code>/status/{job_id}/stream</code>","text":"<p>Server-sent events endpoint for real-time job status updates.</p> <p>Request: <pre><code>curl -N \"http://localhost:8000/api/v1/jobs/status/123e4567-e89b-12d3-a456-426614174000/stream\"\n</code></pre></p> <p>Response: Server-sent events stream with JSON data: <pre><code>data: {\"job_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"status\": \"running\", \"progress\": 10.0, \"current_step\": \"Initializing\", \"timestamp\": \"2024-01-21T10:30:05Z\"}\n\ndata: {\"job_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"status\": \"running\", \"progress\": 25.0, \"current_step\": \"Processing document 1\", \"timestamp\": \"2024-01-21T10:31:15Z\"}\n\ndata: {\"job_id\": \"123e4567-e89b-12d3-a456-426614174000\", \"status\": \"completed\", \"progress\": 100.0, \"current_step\": \"Job completed successfully\", \"timestamp\": \"2024-01-21T10:35:22Z\"}\n</code></pre></p>"},{"location":"userGuide/jobsapi/#queue-statistics","title":"Queue Statistics","text":""},{"location":"userGuide/jobsapi/#get-queuestats","title":"GET <code>/queue/stats</code>","text":"<p>Get comprehensive statistics about the job queue system.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/jobs/queue/stats\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"pending_jobs\": 3,\n  \"running_jobs\": 2,\n  \"scheduled_jobs\": 1,\n  \"completed_jobs\": 45,\n  \"failed_jobs\": 2,\n  \"cancelled_jobs\": 1,\n  \"retrying_jobs\": 0,\n  \"total_jobs\": 54,\n  \"queue_type\": \"redis\",\n  \"cache_size\": 12,\n  \"cpu_usage_percent\": 45.2,\n  \"memory_usage_mb\": 234.8,\n  \"active_workers\": 3,\n  \"queue_health\": {\n    \"status\": \"healthy\",\n    \"resource_utilization\": \"optimal\",\n    \"limit_enforcement\": \"active\"\n  }\n}\n</code></pre></p>"},{"location":"userGuide/jobsapi/#job-management","title":"Job Management","text":""},{"location":"userGuide/jobsapi/#post-canceljob_id","title":"POST <code>/cancel/{job_id}</code>","text":"<p>Cancel a running or pending job.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/jobs/cancel/123e4567-e89b-12d3-a456-426614174000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Job 123e4567-e89b-12d3-a456-426614174000 cancelled successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/jobsapi/#post-restartjob_id","title":"POST <code>/restart/{job_id}</code>","text":"<p>Restart a failed or cancelled job.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/jobs/restart/123e4567-e89b-12d3-a456-426614174000\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Job 123e4567-e89b-12d3-a456-426614174000 restarted successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/jobsapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/jobsapi/#1-job-status-monitoring","title":"1. Job Status Monitoring","text":"<p>Polling Strategy: <pre><code>import requests\nimport time\n\ndef monitor_job_completion(job_id, max_attempts=60, poll_interval=5):\n    \"\"\"Monitor job until completion with exponential backoff.\"\"\"\n    attempt = 0\n\n    while attempt &lt; max_attempts:\n        try:\n            response = requests.get(f'http://localhost:8000/api/v1/jobs/status/{job_id}')\n            response.raise_for_status()\n\n            job_status = response.json()\n\n            print(f\"Job {job_id}: {job_status['status']} - {job_status['progress']:.1f}% - {job_status['current_step']}\")\n\n            if job_status['status'] in ['completed', 'failed', 'cancelled']:\n                return job_status\n\n            # Exponential backoff with jitter\n            sleep_time = min(poll_interval * (2 ** attempt), 30)  # Max 30 seconds\n            time.sleep(sleep_time)\n            attempt += 1\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Error polling job status: {e}\")\n            time.sleep(poll_interval)\n            attempt += 1\n\n    raise TimeoutError(f\"Job {job_id} did not complete within {max_attempts * poll_interval} seconds\")\n</code></pre></p> <p>Long Polling Implementation: <pre><code>def wait_for_job_change(job_id, current_status=None, timeout=30):\n    \"\"\"Wait for job status change using long polling.\"\"\"\n    params = {'timeout': timeout}\n    if current_status:\n        params['current_status'] = current_status\n\n    response = requests.get(\n        f'http://localhost:8000/api/v1/jobs/status/{job_id}/poll',\n        params=params\n    )\n    response.raise_for_status()\n\n    return response.json()\n</code></pre></p> <p>Real-Time Streaming: <pre><code>import json\nimport requests\n\ndef stream_job_updates(job_id):\n    \"\"\"Stream real-time job updates using server-sent events.\"\"\"\n    response = requests.get(\n        f'http://localhost:8000/api/v1/jobs/status/{job_id}/stream',\n        stream=True\n    )\n\n    for line in response.iter_lines():\n        if line.startswith(b'data: '):\n            data = line[6:].decode('utf-8')  # Remove 'data: ' prefix\n            try:\n                update = json.loads(data)\n                print(f\"Job Update: {update}\")\n\n                # Handle different update types\n                if 'error' in update:\n                    print(f\"Error: {update['error']}\")\n                    break\n                elif update.get('status') in ['completed', 'failed', 'cancelled']:\n                    print(\"Job finished\")\n                    break\n\n            except json.JSONDecodeError:\n                continue\n</code></pre></p>"},{"location":"userGuide/jobsapi/#2-error-handling-and-recovery","title":"2. Error Handling and Recovery","text":"<p>Robust Job Monitoring: <pre><code>def monitor_job_with_recovery(job_id):\n    \"\"\"Monitor job with automatic error recovery.\"\"\"\n    try:\n        # First, try to get initial status\n        status_response = requests.get(f'http://localhost:8000/api/v1/jobs/status/{job_id}')\n        status_response.raise_for_status()\n        job_status = status_response.json()\n\n        # If job failed, try to restart\n        if job_status['status'] == 'failed':\n            print(f\"Job {job_id} failed: {job_status.get('error', 'Unknown error')}\")\n            restart_response = requests.post(f'http://localhost:8000/api/v1/jobs/restart/{job_id}')\n            if restart_response.status_code == 200:\n                print(\"Job restarted successfully\")\n                return monitor_job_completion(job_id)\n            else:\n                print(\"Failed to restart job\")\n\n        # If job is cancelled, ask user if they want to restart\n        elif job_status['status'] == 'cancelled':\n            user_input = input(f\"Job {job_id} was cancelled. Restart? (y/n): \")\n            if user_input.lower() == 'y':\n                restart_response = requests.post(f'http://localhost:8000/api/v1/jobs/restart/{job_id}')\n                if restart_response.status_code == 200:\n                    return monitor_job_completion(job_id)\n\n        # For running or pending jobs, monitor normally\n        elif job_status['status'] in ['running', 'pending']:\n            return monitor_job_completion(job_id)\n\n        # For completed jobs, return status\n        elif job_status['status'] == 'completed':\n            return job_status\n\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 404:\n            print(f\"Job {job_id} not found\")\n        else:\n            print(f\"HTTP error: {e}\")\n    except requests.exceptions.ConnectionError:\n        print(\"Connection error - check if API server is running\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n    return None\n</code></pre></p>"},{"location":"userGuide/jobsapi/#3-batch-job-management","title":"3. Batch Job Management","text":"<p>Monitor Multiple Jobs: <pre><code>def monitor_multiple_jobs(job_ids):\n    \"\"\"Monitor multiple jobs concurrently.\"\"\"\n    import threading\n    import queue\n\n    results = {}\n    result_queue = queue.Queue()\n\n    def monitor_single_job(job_id):\n        try:\n            result = monitor_job_completion(job_id)\n            result_queue.put((job_id, result))\n        except Exception as e:\n            result_queue.put((job_id, {'error': str(e)}))\n\n    # Start monitoring threads\n    threads = []\n    for job_id in job_ids:\n        thread = threading.Thread(target=monitor_single_job, args=(job_id,))\n        thread.start()\n        threads.append(thread)\n\n    # Collect results\n    for _ in range(len(job_ids)):\n        job_id, result = result_queue.get()\n        results[job_id] = result\n\n    # Wait for all threads to complete\n    for thread in threads:\n        thread.join()\n\n    return results\n</code></pre></p> <p>Queue Health Monitoring: <pre><code>def check_queue_health():\n    \"\"\"Check overall queue health and performance.\"\"\"\n    try:\n        response = requests.get('http://localhost:8000/api/v1/jobs/queue/stats')\n        response.raise_for_status()\n        stats = response.json()\n\n        # Check for concerning metrics\n        alerts = []\n\n        if stats['failed_jobs'] &gt; stats['total_jobs'] * 0.1:  # &gt;10% failure rate\n            alerts.append(f\"High failure rate: {stats['failed_jobs']}/{stats['total_jobs']} jobs failed\")\n\n        if stats['cpu_usage_percent'] &gt; 90:\n            alerts.append(f\"High CPU usage: {stats['cpu_usage_percent']}%\")\n\n        if stats['memory_usage_mb'] &gt; 1000:  # &gt;1GB\n            alerts.append(f\"High memory usage: {stats['memory_usage_mb']}MB\")\n\n        if stats['pending_jobs'] &gt; 10:\n            alerts.append(f\"Large pending queue: {stats['pending_jobs']} jobs waiting\")\n\n        if not alerts:\n            print(\"Queue health: GOOD\")\n            print(f\"Active jobs: {stats['running_jobs']}\")\n            print(f\"Workers: {stats.get('active_workers', 'unknown')}\")\n        else:\n            print(\"Queue health: WARNING\")\n            for alert in alerts:\n                print(f\"  - {alert}\")\n\n        return stats\n\n    except Exception as e:\n        print(f\"Failed to check queue health: {e}\")\n        return None\n</code></pre></p>"},{"location":"userGuide/jobsapi/#4-job-lifecycle-management","title":"4. Job Lifecycle Management","text":"<p>Complete Job Workflow: <pre><code>def submit_and_monitor_job(job_type, parameters):\n    \"\"\"Complete workflow: submit job, monitor progress, handle results.\"\"\"\n    # This would typically be called after submitting a job via another API\n    # For example, after calling document processing API\n\n    # Assume job_id is obtained from job submission\n    job_id = \"example-job-id-from-submission\"\n\n    print(f\"Monitoring job {job_id}...\")\n\n    # Monitor job completion\n    final_status = monitor_job_completion(job_id)\n\n    if final_status['status'] == 'completed':\n        print(\"Job completed successfully!\")\n        if final_status.get('result'):\n            print(f\"Result: {final_status['result']}\")\n\n        # Process results based on job type\n        if job_type == 'document_processing':\n            print(\"Documents processed successfully\")\n        elif job_type == 'taxonomy_generation':\n            print(\"Taxonomy generated successfully\")\n        elif job_type == 'dataset_generation':\n            print(\"Dataset created successfully\")\n\n    elif final_status['status'] == 'failed':\n        print(f\"Job failed: {final_status.get('error', 'Unknown error')}\")\n        # Handle failure - retry, notify user, etc.\n\n    elif final_status['status'] == 'cancelled':\n        print(\"Job was cancelled\")\n        # Handle cancellation\n\n    return final_status\n</code></pre></p>"},{"location":"userGuide/jobsapi/#5-performance-optimization","title":"5. Performance Optimization","text":"<p>Efficient Polling Strategies: <pre><code>def adaptive_polling(job_id, initial_interval=1, max_interval=30):\n    \"\"\"Adapt polling frequency based on job progress.\"\"\"\n    import time\n\n    last_progress = 0\n    interval = initial_interval\n\n    while True:\n        response = requests.get(f'http://localhost:8000/api/v1/jobs/status/{job_id}')\n        job_status = response.json()\n\n        current_progress = job_status['progress']\n\n        # If progress changed significantly, reset to faster polling\n        if current_progress - last_progress &gt; 5:  # 5% progress change\n            interval = max(initial_interval, interval / 2)\n        else:\n            # Gradually slow down polling\n            interval = min(interval * 1.2, max_interval)\n\n        print(f\"Progress: {current_progress:.1f}% - Next check in {interval:.1f}s\")\n\n        if job_status['status'] in ['completed', 'failed', 'cancelled']:\n            return job_status\n\n        time.sleep(interval)\n        last_progress = current_progress\n</code></pre></p> <p>Connection Pooling for High-Frequency Monitoring: <pre><code>import requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\ndef create_resilient_session():\n    \"\"\"Create HTTP session with connection pooling and retries.\"\"\"\n    session = requests.Session()\n\n    # Configure retries\n    retry_strategy = Retry(\n        total=3,\n        backoff_factor=0.5,\n        status_forcelist=[429, 500, 502, 503, 504]\n    )\n\n    # Configure adapter with connection pooling\n    adapter = HTTPAdapter(\n        max_retries=retry_strategy,\n        pool_connections=10,\n        pool_maxsize=20\n    )\n\n    session.mount(\"http://\", adapter)\n    session.mount(\"https://\", adapter)\n\n    return session\n\n# Use resilient session for monitoring\nsession = create_resilient_session()\nresponse = session.get(f'http://localhost:8000/api/v1/jobs/status/{job_id}')\n</code></pre></p> <p>This API provides comprehensive job monitoring and management capabilities essential for tracking long-running background operations in the Compileo system.</p>"},{"location":"userGuide/logging/","title":"Logging System in Compileo","text":"<p>Compileo provides a project-wide, persistent logging system that allows you to control the verbosity of log output across the CLI, API, and GUI. This is particularly useful for developers troubleshooting complex AI processing workflows or for production environments where minimal output is desired.</p>"},{"location":"userGuide/logging/#log-levels","title":"Log Levels","text":"<p>The system supports three basic log levels:</p> <ol> <li>none: Disables all logging output. No logs will be printed to the console or captured by the middleware.</li> <li>error: Only logs critical errors and exceptions. This is the recommended setting for standard production usage.</li> <li>debug: Enables extensive log reporting for developers. This includes detailed internal process information, request/response headers, and JSON-structured debug data for AI model interactions.</li> </ol>"},{"location":"userGuide/logging/#controlling-the-log-level","title":"Controlling the Log Level","text":"<p>You can control the global log level through three primary interfaces:</p>"},{"location":"userGuide/logging/#1-web-gui-settings-page","title":"1. Web GUI (Settings Page)","text":"<p>The easiest way to change the log level is through the Compileo GUI: 1.  Open the Compileo GUI in your browser. 2.  Navigate to the Settings tab. 3.  In the General settings section, locate the Log Level dropdown. 4.  Select your desired level (none, error, or debug). 5.  Click Save Settings. The change will be persisted in the database and applied immediately to the API server and all background workers.</p>"},{"location":"userGuide/logging/#2-rest-api","title":"2. REST API","text":"<p>You can programmatically manage global settings, including the log level, via the settings API endpoint:</p> <ul> <li>GET <code>/api/v1/settings/</code>: Retrieves the current global settings, including the current log level.</li> <li>POST <code>/api/v1/settings/</code>: Updates global settings.<ul> <li>Payload Example:     <pre><code>{\n  \"log_level\": \"debug\"\n}\n</code></pre></li> </ul> </li> </ul>"},{"location":"userGuide/logging/#3-cli-parameters","title":"3. CLI Parameters","text":"<p>The Compileo CLI supports a global <code>--log-level</code> option that allows you to override or set the persistent log level for the duration of the command:</p> <pre><code># Set log level to debug for a specific command\ncompileo --log-level debug projects list\n\n# The CLI also updates the persistent database setting when this flag is used\n</code></pre>"},{"location":"userGuide/logging/#internal-implementation","title":"Internal Implementation","text":"<p>The logging system is built on top of the standard Python <code>logging</code> module but is centralized through a unified utility in <code>src/compileo/core/logging.py</code>. </p>"},{"location":"userGuide/logging/#synchronization","title":"Synchronization","text":"<p>Because Compileo uses multiple processes (API server, RQ workers, CLI), the log level is stored in a shared SQLite database (<code>gui_settings</code> table). All components call <code>setup_logging()</code> during their initialization lifecycle to ensure they inherit the current global setting.</p>"},{"location":"userGuide/logging/#structured-logging","title":"Structured Logging","text":"<p>For developers using the <code>debug</code> level, many internal components emit structured JSON debug messages. These are captured as <code>DEBUG</code> level log entries and contain valuable context such as timestamps, component names, and raw AI model inputs/outputs.</p>"},{"location":"userGuide/performanceapi/","title":"Performance Optimization API in Compileo","text":""},{"location":"userGuide/performanceapi/#overview","title":"Overview","text":"<p>The Compileo Performance API provides comprehensive optimization capabilities for extraction workflows, including caching, job queue management, lazy loading, cleanup operations, and performance monitoring.</p>"},{"location":"userGuide/performanceapi/#base-url-apiv1performance","title":"Base URL: <code>/api/v1/performance</code>","text":""},{"location":"userGuide/performanceapi/#1-cache-management","title":"1. Cache Management","text":""},{"location":"userGuide/performanceapi/#get-cache-statistics","title":"Get Cache Statistics","text":"<p>Endpoint: <code>GET /cache/stats</code></p> <p>Description: Retrieves comprehensive statistics for all cache systems.</p> <p>Success Response (200 OK): <pre><code>{\n  \"result_cache\": {\n    \"total_entries\": 150,\n    \"total_size_bytes\": 2048576,\n    \"hit_rate\": 0.85,\n    \"miss_rate\": 0.15,\n    \"avg_access_time_ms\": 2.3\n  },\n  \"search_cache\": {\n    \"total_entries\": 75,\n    \"total_size_bytes\": 1024000,\n    \"hit_rate\": 0.92,\n    \"miss_rate\": 0.08,\n    \"avg_access_time_ms\": 1.8\n  }\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#clear-all-caches","title":"Clear All Caches","text":"<p>Endpoint: <code>POST /cache/clear</code></p> <p>Description: Clears all cache systems to free memory and ensure fresh data.</p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"All caches cleared successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#2-job-queue-management","title":"2. Job Queue Management","text":""},{"location":"userGuide/performanceapi/#submit-job","title":"Submit Job","text":"<p>Endpoint: <code>POST /jobs/submit</code></p> <p>Description: Submits a new job to the enhanced job queue with priority support.</p> <p>Request Body: <pre><code>{\n  \"job_type\": \"extraction\",\n  \"parameters\": {\n    \"taxonomy_id\": 123,\n    \"selected_categories\": [\"category1\", \"category2\"]\n  },\n  \"priority\": \"high\"\n}\n</code></pre></p> <p>Parameters: - <code>job_type</code> (string): Type of job (extraction, analysis, etc.) - <code>parameters</code> (object): Job-specific parameters - <code>priority</code> (string, optional): Job priority (low, normal, high, urgent)</p> <p>Success Response (200 OK): <pre><code>{\n  \"job_id\": \"job_12345\",\n  \"status\": \"submitted\",\n  \"message\": \"Job job_12345 submitted successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#get-job-status","title":"Get Job Status","text":"<p>Endpoint: <code>GET /jobs/{job_id}</code></p> <p>Description: Retrieves current status and details of a specific job.</p> <p>Path Parameters: - <code>job_id</code> (string, required): Job identifier</p> <p>Success Response (200 OK): <pre><code>{\n  \"job_id\": \"job_12345\",\n  \"job_type\": \"extraction\",\n  \"status\": \"running\",\n  \"priority\": \"normal\",\n  \"progress\": 0.65,\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"started_at\": \"2024-01-15T10:31:00Z\",\n  \"parameters\": {...}\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#cancel-job","title":"Cancel Job","text":"<p>Endpoint: <code>DELETE /jobs/{job_id}</code></p> <p>Description: Cancels a pending or running job.</p> <p>Path Parameters: - <code>job_id</code> (string, required): Job identifier</p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"Job job_12345 cancelled successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#get-queue-statistics","title":"Get Queue Statistics","text":"<p>Endpoint: <code>GET /jobs/queue/stats</code></p> <p>Description: Retrieves comprehensive job queue statistics and performance metrics.</p> <p>Success Response (200 OK): <pre><code>{\n  \"pending_jobs\": 5,\n  \"running_jobs\": 3,\n  \"completed_jobs\": 150,\n  \"failed_jobs\": 2,\n  \"total_jobs\": 160,\n  \"avg_processing_time_seconds\": 45.2,\n  \"queue_depth\": 8,\n  \"worker_utilization\": 0.75\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#3-enhanced-search","title":"3. Enhanced Search","text":""},{"location":"userGuide/performanceapi/#paginated-search","title":"Paginated Search","text":"<p>Endpoint: <code>GET /search/paginated</code></p> <p>Description: Performs advanced search with caching and pagination support.</p> <p>Query Parameters: - <code>query</code> (string, optional): Search query string - <code>categories</code> (array, optional): Filter by taxonomy categories - <code>min_confidence</code> (float, optional): Minimum confidence threshold - <code>date_from</code> (datetime, optional): Start date filter - <code>date_to</code> (datetime, optional): End date filter - <code>page</code> (integer, optional, default: 0): Page number - <code>per_page</code> (integer, optional, default: 50): Results per page</p> <p>Success Response (200 OK): <pre><code>{\n  \"results\": [\n    {\n      \"chunk_id\": \"chunk_123\",\n      \"chunk_text\": \"...\",\n      \"categories_matched\": [\"category1\"],\n      \"confidence_score\": 0.89\n    }\n  ],\n  \"total_count\": 250,\n  \"metadata\": {\n    \"cache_hit\": true,\n    \"search_time_ms\": 45,\n    \"filtered_categories\": [\"category1\"]\n  }\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#get-search-count","title":"Get Search Count","text":"<p>Endpoint: <code>GET /search/lazy/count</code></p> <p>Description: Gets total count of search results without loading all data (lazy loading).</p> <p>Query Parameters: Same as paginated search</p> <p>Success Response (200 OK): <pre><code>{\n  \"total_count\": 1250\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#get-search-suggestions","title":"Get Search Suggestions","text":"<p>Endpoint: <code>GET /search/suggestions</code></p> <p>Description: Provides search suggestions based on partial query input.</p> <p>Query Parameters: - <code>query</code> (string, required): Partial search query - <code>limit</code> (integer, optional, default: 10): Maximum suggestions to return</p> <p>Success Response (200 OK): <pre><code>{\n  \"suggestions\": [\n    \"machine learning\",\n    \"artificial intelligence\",\n    \"data processing\",\n    \"neural networks\"\n  ]\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#4-cleanup-management","title":"4. Cleanup Management","text":""},{"location":"userGuide/performanceapi/#run-cleanup","title":"Run Cleanup","text":"<p>Endpoint: <code>POST /cleanup/run</code></p> <p>Description: Manually triggers cleanup operations or forces scheduled cleanup.</p> <p>Query Parameters: - <code>force</code> (boolean, optional): Force immediate cleanup - <code>schedule_name</code> (string, optional): Specific schedule to run</p> <p>Success Response (200 OK): <pre><code>{\n  \"cleanup_id\": \"cleanup_123\",\n  \"files_removed\": 25,\n  \"space_freed_bytes\": 10485760,\n  \"duration_seconds\": 2.5,\n  \"status\": \"completed\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#get-cleanup-statistics","title":"Get Cleanup Statistics","text":"<p>Endpoint: <code>GET /cleanup/stats</code></p> <p>Description: Retrieves comprehensive cleanup operation statistics.</p> <p>Success Response (200 OK): <pre><code>{\n  \"total_cleanups\": 45,\n  \"files_removed\": 1250,\n  \"space_freed_bytes\": 524288000,\n  \"avg_cleanup_time_seconds\": 3.2,\n  \"last_cleanup\": \"2024-01-15T14:30:00Z\",\n  \"cleanup_percentage\": 85.5\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#add-cleanup-schedule","title":"Add Cleanup Schedule","text":"<p>Endpoint: <code>POST /cleanup/schedules</code></p> <p>Description: Creates a new automated cleanup schedule.</p> <p>Request Body: <pre><code>{\n  \"name\": \"daily_cleanup\",\n  \"interval_seconds\": 86400,\n  \"retention_days\": 30,\n  \"priority\": \"normal\"\n}\n</code></pre></p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"Cleanup schedule 'daily_cleanup' added successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#remove-cleanup-schedule","title":"Remove Cleanup Schedule","text":"<p>Endpoint: <code>DELETE /cleanup/schedules/{schedule_name}</code></p> <p>Description: Removes an existing cleanup schedule.</p> <p>Path Parameters: - <code>schedule_name</code> (string, required): Name of schedule to remove</p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"Schedule 'daily_cleanup' removed successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#optimize-cleanup-schedule","title":"Optimize Cleanup Schedule","text":"<p>Endpoint: <code>GET /cleanup/optimize</code></p> <p>Description: Provides optimization recommendations for cleanup schedules.</p> <p>Success Response (200 OK): <pre><code>{\n  \"recommendations\": [\n    \"Increase retention period for high-value data\",\n    \"Reduce cleanup frequency for low-activity periods\",\n    \"Consolidate overlapping schedules\"\n  ],\n  \"estimated_savings\": {\n    \"storage_mb\": 500,\n    \"processing_time_seconds\": 120\n  }\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#5-performance-monitoring","title":"5. Performance Monitoring","text":""},{"location":"userGuide/performanceapi/#get-performance-metrics","title":"Get Performance Metrics","text":"<p>Endpoint: <code>GET /performance/metrics</code></p> <p>Description: Retrieves comprehensive system performance metrics.</p> <p>Success Response (200 OK): <pre><code>{\n  \"cache\": {\n    \"result_cache\": {\n      \"total_entries\": 150,\n      \"hit_rate\": 0.85,\n      \"avg_access_time_ms\": 2.3\n    }\n  },\n  \"jobs\": {\n    \"pending_jobs\": 5,\n    \"running_jobs\": 3,\n    \"avg_processing_time_seconds\": 45.2\n  },\n  \"cleanup\": {\n    \"files_removed\": 1250,\n    \"space_freed_bytes\": 524288000,\n    \"cleanup_percentage\": 85.5\n  },\n  \"system\": {\n    \"total_cache_size_bytes\": 3072000,\n    \"active_jobs\": 3,\n    \"pending_jobs\": 5,\n    \"cleanup_percentage\": 85.5\n  }\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#reset-performance-metrics","title":"Reset Performance Metrics","text":"<p>Endpoint: <code>POST /performance/reset</code></p> <p>Description: Resets all performance metrics and clears caches for fresh monitoring.</p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"Performance metrics reset successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/performanceapi/#performance-optimization-features","title":"Performance Optimization Features","text":""},{"location":"userGuide/performanceapi/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>Multi-level Caching: Result cache, search cache, metadata cache</li> <li>Intelligent Invalidation: Automatic cache cleanup based on data changes</li> <li>Performance Monitoring: Hit rates, access times, and cache utilization</li> </ul>"},{"location":"userGuide/performanceapi/#job-queue-optimization","title":"Job Queue Optimization","text":"<ul> <li>Priority Queues: Support for urgent, high, normal, and low priority jobs</li> <li>Load Balancing: Automatic distribution across available workers</li> <li>Queue Monitoring: Real-time statistics and performance metrics</li> </ul>"},{"location":"userGuide/performanceapi/#lazy-loading","title":"Lazy Loading","text":"<ul> <li>On-demand Loading: Load data only when needed</li> <li>Memory Efficient: Reduce memory usage for large datasets</li> <li>Fast Counting: Get result counts without loading full datasets</li> </ul>"},{"location":"userGuide/performanceapi/#automated-cleanup","title":"Automated Cleanup","text":"<ul> <li>Scheduled Cleanup: Configurable retention policies</li> <li>Space Optimization: Automatic removal of old/unused data</li> <li>Performance Maintenance: Prevent storage bloat and performance degradation</li> </ul>"},{"location":"userGuide/performanceapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/performanceapi/#cache-management","title":"Cache Management","text":"<ul> <li>Monitor cache hit rates and adjust cache sizes accordingly</li> <li>Clear caches during maintenance windows</li> <li>Use appropriate cache TTL settings for different data types</li> </ul>"},{"location":"userGuide/performanceapi/#job-queue-optimization_1","title":"Job Queue Optimization","text":"<ul> <li>Use appropriate job priorities for different workloads</li> <li>Monitor queue depth and processing times</li> <li>Scale worker processes based on load</li> </ul>"},{"location":"userGuide/performanceapi/#search-performance","title":"Search Performance","text":"<ul> <li>Use lazy counting for large result sets</li> <li>Implement appropriate pagination limits</li> <li>Cache frequent search queries</li> </ul>"},{"location":"userGuide/performanceapi/#cleanup-operations","title":"Cleanup Operations","text":"<ul> <li>Schedule cleanups during low-usage periods</li> <li>Set appropriate retention policies based on data value</li> <li>Monitor cleanup effectiveness and adjust schedules</li> </ul>"},{"location":"userGuide/performanceapi/#performance-monitoring","title":"Performance Monitoring","text":"<ul> <li>Regularly review performance metrics</li> <li>Set up alerts for performance degradation</li> <li>Use metrics to guide optimization efforts</li> </ul> <p>This performance API provides comprehensive optimization capabilities to ensure Compileo operates efficiently at scale.</p>"},{"location":"userGuide/plugins/","title":"Plugin Management User Guide","text":"<p>Compileo's Plugin System allows you to easily extend the platform's capabilities by installing add-ons. This guide explains how to manage plugins via the Graphical User Interface (GUI).</p>"},{"location":"userGuide/plugins/#accessing-the-plugin-manager","title":"Accessing the Plugin Manager","text":"<ol> <li>Navigate to the Settings page from the sidebar.</li> <li>Click on the Plugins tab.</li> </ol>"},{"location":"userGuide/plugins/#installing-a-plugin","title":"Installing a Plugin","text":"<p>To install a new plugin:</p> <ol> <li>Locate the Install Plugin section in the Plugins tab.</li> <li>Click Browse files or drag and drop your plugin <code>.zip</code> file into the upload area.</li> <li>Click the Install button.</li> <li>The system will upload, verify, and install the plugin. A success message will appear upon completion.</li> </ol> <p>Note: Some plugins may perform automated setup tasks during installation (e.g., downloading necessary tools or drivers). This may take a few moments. Ensure you only install plugins from trusted sources.</p>"},{"location":"userGuide/plugins/#managing-installed-plugins","title":"Managing Installed Plugins","text":"<p>The Installed Plugins section lists all currently active plugins on your system. For each plugin, you can see:</p> <ul> <li>Name &amp; Version: The display name and version number.</li> <li>Description: A brief summary of what the plugin does.</li> <li>Author: The creator of the plugin.</li> <li>Details: Technical details like the plugin ID and entry point.</li> </ul>"},{"location":"userGuide/plugins/#uninstalling-a-plugin","title":"Uninstalling a Plugin","text":"<p>To remove a plugin:</p> <ol> <li>Find the plugin you wish to remove in the Installed Plugins list.</li> <li>Expand the plugin details if necessary.</li> <li>Click the \ud83d\uddd1\ufe0f Uninstall button.</li> <li>Confirm the action if prompted. The plugin will be immediately removed from the system.</li> </ol>"},{"location":"userGuide/plugins/#example-anki-dataset-generator","title":"Example: Anki Dataset Generator","text":"<p>A reference plugin, \"Anki Dataset Exporter\", is available to add Anki flashcard export capabilities to the Dataset Generation module.</p> <ol> <li>Install the <code>compileo-anki-plugin.zip</code>.</li> <li>Go to the Dataset Generation page.</li> <li>In the \"Output Format\" dropdown, you will now see an option for anki.</li> <li>Select your desired Generation Mode (Instruction Following, Question and Answer, Summarization, etc.).</li> <li>Generating a dataset with this format will produce a semicolon-separated text file (<code>.txt</code> extension) compatible with Anki import.</li> </ol> <p>Generation Mode Support: - Instruction Following: Creates flashcards with instructions on the front and responses on the back - Question and Answer: Traditional Q&amp;A format with questions on the front and answers on the back - Summarization: Content summaries on the front with key points on the back - Other Modes: Automatically adapts to any generation mode supported by Compileo</p> <p>File Format: - Output: <code>dataset_[job_id]_extract.txt</code> - Format: <code>question;answer</code> (semicolon-separated) - Compatible with Anki's built-in import feature - HTML formatting supported for rich text display</p>"},{"location":"userGuide/plugins/#example-scrapy-playwright-website-scraper","title":"Example: Scrapy-Playwright Website Scraper","text":"<p>The \"Scrapy-Playwright Website Scraper\" plugin enables extracting content from dynamic websites that require JavaScript rendering.</p> <ol> <li>Install: Upload and install the <code>compileo-scrapy-playwright-scraper.zip</code>. The plugin will automatically install browser dependencies (<code>playwright install</code>) during setup.</li> <li>Usage (CLI): Use the new command provided by the plugin:     <pre><code>compileo scrape-website --url \"https://example.com\" --project-id 1 --depth 2\n</code></pre></li> <li>Usage (API): Send a POST request to <code>/api/v1/plugins/scrapy-playwright/scrape</code> with the URL and configuration.</li> <li>Usage (GUI): In the Document Processing tab, you can use the \"Scrape Website\" feature to ingest content directly from URLs.</li> </ol>"},{"location":"userGuide/pluginsapi/","title":"Plugin Management API","text":"<p>The Plugin API allows programmatic management of Compileo extensions.</p>"},{"location":"userGuide/pluginsapi/#endpoints","title":"Endpoints","text":""},{"location":"userGuide/pluginsapi/#list-plugins","title":"List Plugins","text":"<p>Retrieves a list of all installed plugins.</p> <ul> <li>URL: <code>/api/v1/plugins/</code></li> <li>Method: <code>GET</code></li> <li>Response: <code>200 OK</code> <pre><code>[\n  {\n    \"id\": \"compileo-anki-plugin\",\n    \"name\": \"Anki Dataset Exporter\",\n    \"version\": \"1.0.0\",\n    \"author\": \"Compileo Team\",\n    \"description\": \"Exports datasets in Anki-compatible text format.\",\n    \"entry_point\": \"src.anki_formatter\",\n    \"extensions\": {\n      \"compileo.datasetgen.formatter\": {\n        \"anki\": \"AnkiOutputFormatter\"\n      }\n    }\n  }\n]\n</code></pre></li> </ul>"},{"location":"userGuide/pluginsapi/#upload-plugin","title":"Upload Plugin","text":"<p>Uploads and installs a plugin from a <code>.zip</code> file.</p> <ul> <li>URL: <code>/api/v1/plugins/upload</code></li> <li>Method: <code>POST</code></li> <li>Content-Type: <code>multipart/form-data</code></li> <li>Parameters:<ul> <li><code>file</code>: The plugin <code>.zip</code> file.</li> </ul> </li> <li>Response: <code>200 OK</code> <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Plugin compileo-anki-plugin installed successfully\",\n  \"plugin_id\": \"compileo-anki-plugin\"\n}\n</code></pre></li> </ul>"},{"location":"userGuide/pluginsapi/#get-dataset-formats","title":"Get Dataset Formats","text":"<p>Retrieves all available dataset output formats, including built-in formats and plugin-provided formats.</p> <ul> <li>URL: <code>/api/v1/plugins/dataset-formats</code></li> <li>Method: <code>GET</code></li> <li>Response: <code>200 OK</code> <pre><code>{\n  \"formats\": [\n    {\n      \"id\": \"jsonl\",\n      \"name\": \"JSON Lines\",\n      \"description\": \"JSON Lines format for dataset entries\",\n      \"file_extension\": \"jsonl\",\n      \"category\": \"built-in\"\n    },\n    {\n      \"id\": \"parquet\",\n      \"name\": \"Apache Parquet\",\n      \"description\": \"Columnar storage format for datasets\",\n      \"file_extension\": \"parquet\",\n      \"category\": \"built-in\"\n    },\n    {\n      \"id\": \"anki\",\n      \"name\": \"Anki Flashcards\",\n      \"description\": \"Anki-compatible semicolon-separated text format\",\n      \"file_extension\": \"txt\",\n      \"category\": \"plugin\",\n      \"plugin_id\": \"compileo-anki-plugin\"\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"userGuide/pluginsapi/#uninstall-plugin","title":"Uninstall Plugin","text":"<p>Uninstalls a plugin by ID.</p> <ul> <li>URL: <code>/api/v1/plugins/{plugin_id}</code></li> <li>Method: <code>DELETE</code></li> <li>Parameters:<ul> <li><code>plugin_id</code>: The ID of the plugin to uninstall.</li> </ul> </li> <li>Response: <code>200 OK</code>     ```json     {       \"status\": \"success\",       \"message\": \"Plugin compileo-anki-plugin uninstalled\"     }</li> </ul>"},{"location":"userGuide/pluginsapi/#plugin-specific-apis","title":"Plugin-Specific APIs","text":"<p>Plugins can also register their own API routers, which become available under <code>/api/v1/plugins/{plugin-id}/</code>.</p>"},{"location":"userGuide/pluginsapi/#example-scrapy-playwright-scraper","title":"Example: Scrapy-Playwright Scraper","text":"<ul> <li>Scrape Website: <code>POST /api/v1/plugins/scrapy-playwright/scrape</code><ul> <li>Body:     <pre><code>{\n  \"url\": \"https://example.com\",\n  \"depth\": 1,\n  \"project_id\": \"1\",\n  \"document_id\": 123,  // Optional: Attach to existing document\n  \"wait_for\": \"networkidle\", // Optional: load, domcontentloaded, networkidle\n  \"scroll_to_bottom\": true   // Optional: Scroll to load dynamic content\n}\n</code></pre></li> </ul> </li> <li>Get Configuration: <code>GET /api/v1/plugins/scrapy-playwright/config</code></li> </ul>"},{"location":"userGuide/pluginscli/","title":"Plugin Management CLI","text":"<p>The Compileo CLI provides a comprehensive set of commands for managing plugins from the terminal.</p>"},{"location":"userGuide/pluginscli/#command-group-plugins","title":"Command Group: <code>plugins</code>","text":"<p>All plugin-related commands are grouped under <code>compileo plugins</code>.</p> <p>Note: Plugins may also extend the CLI with their own top-level commands. For example, the <code>compileo-scrapy-playwright-scraper</code> plugin adds a <code>compileo scrape-website</code> command. These commands become available automatically after installing the plugin.</p>"},{"location":"userGuide/pluginscli/#list-plugins","title":"List Plugins","text":"<p>List all currently installed plugins.</p> <pre><code>compileo plugins list\n</code></pre> <p>Options: *   <code>--format [table|json]</code>: Output format (default: table).</p> <p>Example Output: <pre><code>Installed Plugins (1):\n--------------------------------------------------------------------------------\nID                        Name                      Version    Author\n--------------------------------------------------------------------------------\ncompileo-anki-plugin      Anki Dataset Exporter     1.0.0      Compileo Team\n</code></pre></p>"},{"location":"userGuide/pluginscli/#install-plugin","title":"Install Plugin","text":"<p>Install a new plugin from a zip file.</p> <pre><code>compileo plugins install &lt;path_to_plugin_zip&gt;\n</code></pre> <p>Arguments: *   <code>plugin_file</code>: Path to the plugin <code>.zip</code> file.</p> <p>Example: <pre><code>compileo plugins install ./compileo-anki-plugin.zip\n</code></pre></p>"},{"location":"userGuide/pluginscli/#uninstall-plugin","title":"Uninstall Plugin","text":"<p>Remove an installed plugin.</p> <pre><code>compileo plugins uninstall &lt;plugin_id&gt;\n</code></pre> <p>Arguments: *   <code>plugin_id</code>: The unique ID of the plugin to uninstall.</p> <p>Options: *   <code>--confirm</code>: Skip confirmation prompt.</p> <p>Example: ```bash compileo plugins uninstall compileo-anki-plugin</p>"},{"location":"userGuide/projectsapi/","title":"Projects Module API Usage Guide","text":"<p>The Compileo Projects API provides comprehensive REST endpoints for project management, including creation, retrieval, updating, and deletion of projects with associated document and dataset tracking.</p>"},{"location":"userGuide/projectsapi/#base-url-apiv1projects","title":"Base URL: <code>/api/v1/projects</code>","text":""},{"location":"userGuide/projectsapi/#project-listing","title":"Project Listing","text":""},{"location":"userGuide/projectsapi/#get","title":"GET <code>/</code>","text":"<p>List all projects with pagination support.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/projects?page=1&amp;per_page=20\"\n</code></pre></p> <p>Query Parameters: - <code>page</code>: Page number (default: 1, minimum: 1) - <code>per_page</code>: Items per page (default: 20, range: 1-1000)</p> <p>Response: <pre><code>{\n  \"projects\": [\n    {\n      \"id\": 1,\n      \"name\": \"Medical Research Project\",\n      \"description\": \"Analysis of medical documents\",\n      \"created_at\": \"2024-01-15T10:30:00Z\",\n      \"updated_at\": null,\n      \"document_count\": 15,\n      \"dataset_count\": 3,\n      \"status\": \"active\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"Legal Document Analysis\",\n      \"description\": \"Contract analysis and classification\",\n      \"created_at\": \"2024-01-20T14:45:00Z\",\n      \"updated_at\": null,\n      \"document_count\": 8,\n      \"dataset_count\": 1,\n      \"status\": \"active\"\n    }\n  ],\n  \"total\": 2,\n  \"page\": 1,\n  \"per_page\": 20\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#project-creation","title":"Project Creation","text":""},{"location":"userGuide/projectsapi/#post","title":"POST <code>/</code>","text":"<p>Create a new project.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/projects/\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"New Research Project\",\n    \"description\": \"A project for analyzing research documents\"\n  }'\n</code></pre></p> <p>Request Body: - <code>name</code>: Project name (required, unique) - <code>description</code>: Project description (optional)</p> <p>Response: <pre><code>{\n  \"id\": 3,\n  \"name\": \"New Research Project\",\n  \"description\": \"A project for analyzing research documents\",\n  \"created_at\": \"2024-01-25T09:15:00Z\",\n  \"updated_at\": null,\n  \"document_count\": 0,\n  \"dataset_count\": 0,\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#project-retrieval","title":"Project Retrieval","text":""},{"location":"userGuide/projectsapi/#get-project_id","title":"GET <code>/{project_id}</code>","text":"<p>Get detailed information about a specific project.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/projects/1\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": 1,\n  \"name\": \"Medical Research Project\",\n  \"description\": \"Analysis of medical documents\",\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"updated_at\": null,\n  \"document_count\": 15,\n  \"dataset_count\": 3,\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#get-project_iddocuments","title":"GET <code>/{project_id}/documents</code>","text":"<p>Get all documents associated with a project.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/projects/1/documents\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"documents\": [\n    {\n      \"id\": 101,\n      \"project_id\": 1,\n      \"file_name\": \"medical_report.pdf\",\n      \"source_file_path\": \"storage/uploads/1/medical_report.pdf\",\n      \"parsed_file_path\": \"storage/parsed/1/medical_report.md\",\n      \"parsed_files_manifest\": null,\n      \"created_at\": \"2024-01-15T11:00:00Z\",\n      \"status\": \"parsed\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#get-project_iddatasets","title":"GET <code>/{project_id}/datasets</code>","text":"<p>Get all datasets associated with a project.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/projects/1/datasets\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"datasets\": [\n    {\n      \"id\": 201,\n      \"project_id\": 1,\n      \"output_type\": \"dataset\",\n      \"output_file_path\": \"storage/datasets/1/dataset_v1.0.0.json\",\n      \"created_at\": \"2024-01-16T14:30:00Z\"\n    }\n  ]\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#project-updates","title":"Project Updates","text":""},{"location":"userGuide/projectsapi/#put-project_id","title":"PUT <code>/{project_id}</code>","text":"<p>Update project information.</p> <p>Request: <pre><code>curl -X PUT \"http://localhost:8000/api/v1/projects/1\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Updated Medical Research Project\",\n    \"description\": \"Updated analysis of medical documents with new focus\"\n  }'\n</code></pre></p> <p>Request Body: - <code>name</code>: New project name (optional, must be unique if changed) - <code>description</code>: New project description (optional)</p> <p>Response: <pre><code>{\n  \"id\": 1,\n  \"name\": \"Updated Medical Research Project\",\n  \"description\": \"Updated analysis of medical documents with new focus\",\n  \"created_at\": \"2024-01-15T10:30:00Z\",\n  \"updated_at\": \"2024-01-25T10:15:00Z\",\n  \"document_count\": 15,\n  \"dataset_count\": 3,\n  \"status\": \"active\"\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#project-deletion","title":"Project Deletion","text":""},{"location":"userGuide/projectsapi/#delete-project_id","title":"DELETE <code>/{project_id}</code>","text":"<p>Delete a project and all associated data with comprehensive cascading cleanup.</p> <p>Warning: This operation permanently deletes the project and performs complete cascading deletion across the entire data pipeline. All associated records and files are removed in the correct order to maintain data integrity.</p> <p>What Gets Deleted: - Database Records (in reverse pipeline order):   - Dataset records (versions, parameters, jobs, lineage, changes)   - Extraction records (results, jobs)   - Taxonomy records and associated files   - Document chunks   - Parsed document records and files   - Document records   - Project record - Filesystem Files:   - All uploaded document files   - Parsed markdown files and manifests   - Chunk files and split PDFs   - Taxonomy JSON files   - Extraction result files   - Dataset files - Project Directories:   - <code>storage/uploads/{project_id}/</code>   - <code>storage/parsed/{project_id}/</code>   - <code>storage/chunks/{project_id}/</code>   - <code>storage/taxonomy/{project_id}/</code>   - <code>storage/extracted/{project_id}/</code>   - <code>storage/datasets/{project_id}/</code></p> <p>Request: <pre><code>curl -X DELETE \"http://localhost:8000/api/v1/projects/1\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Project 1 and all associated data deleted successfully\"\n}\n</code></pre></p> <p>Error Handling: - If any deletion step fails, the operation continues with remaining items - Warning logs are generated for individual failures - The operation completes successfully as long as the project record is deleted - Database transactions ensure consistency where possible</p>"},{"location":"userGuide/projectsapi/#delete","title":"DELETE <code>/</code>","text":"<p>Bulk delete multiple projects.</p> <p>Request: <pre><code>curl -X DELETE \"http://localhost:8000/api/v1/projects/\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_ids\": [1, 2, 3]\n  }'\n</code></pre></p> <p>Request Body: - <code>project_ids</code>: Array of project IDs to delete (required)</p> <p>Response: <pre><code>{\n  \"message\": \"Successfully deleted 3 projects and all associated data\",\n  \"deleted\": [1, 2, 3]\n}\n</code></pre></p>"},{"location":"userGuide/projectsapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/projectsapi/#1-project-organization","title":"1. Project Organization","text":"<p>Naming Conventions: <pre><code>import requests\n\n# Use descriptive, unique project names\nprojects_to_create = [\n    {\"name\": \"Q4_2024_Medical_Research\", \"description\": \"Medical document analysis for Q4\"},\n    {\"name\": \"Legal_Contract_Analysis\", \"description\": \"Automated contract review system\"},\n    {\"name\": \"Technical_Documentation_AI\", \"description\": \"AI-powered technical documentation processing\"}\n]\n\nfor project in projects_to_create:\n    response = requests.post('http://localhost:8000/api/v1/projects/', json=project)\n    if response.status_code == 200:\n        print(f\"Created project: {response.json()['name']}\")\n</code></pre></p> <p>Project Structure Planning: <pre><code># Plan project structure based on use case\ndef create_project_structure(domain, year, quarter):\n    base_name = f\"{domain}_{year}_Q{quarter}\"\n    projects = [\n        {\"name\": f\"{base_name}_Raw_Data\", \"description\": \"Initial data ingestion and parsing\"},\n        {\"name\": f\"{base_name}_Processed\", \"description\": \"Cleaned and processed datasets\"},\n        {\"name\": f\"{base_name}_Analysis\", \"description\": \"Final analysis and reporting\"}\n    ]\n    return projects\n</code></pre></p>"},{"location":"userGuide/projectsapi/#2-project-lifecycle-management","title":"2. Project Lifecycle Management","text":"<p>Regular Cleanup: <pre><code># Archive old projects instead of deleting\ndef archive_old_projects(days_old=365):\n    # Get all projects\n    response = requests.get('http://localhost:8000/api/v1/projects/')\n    projects = response.json()['projects']\n\n    # Find projects older than threshold\n    import datetime\n    cutoff_date = datetime.datetime.now() - datetime.timedelta(days=days_old)\n\n    old_projects = []\n    for project in projects:\n        created_date = datetime.datetime.fromisoformat(project['created_at'].replace('Z', '+00:00'))\n        if created_date &lt; cutoff_date:\n            old_projects.append(project)\n\n    print(f\"Found {len(old_projects)} projects older than {days_old} days\")\n    return old_projects\n\n# Use with caution - deletion is permanent\n# archive_projects = archive_old_projects()\n</code></pre></p> <p>Project Metrics Tracking: <pre><code>def get_project_metrics(project_id):\n    # Get project details\n    project_response = requests.get(f'http://localhost:8000/api/v1/projects/{project_id}')\n    project = project_response.json()\n\n    # Get associated data\n    docs_response = requests.get(f'http://localhost:8000/api/v1/projects/{project_id}/documents')\n    datasets_response = requests.get(f'http://localhost:8000/api/v1/projects/{project_id}/datasets')\n\n    metrics = {\n        \"project_name\": project[\"name\"],\n        \"document_count\": project[\"document_count\"],\n        \"dataset_count\": project[\"dataset_count\"],\n        \"documents\": docs_response.json()[\"documents\"],\n        \"datasets\": datasets_response.json()[\"datasets\"]\n    }\n\n    return metrics\n</code></pre></p>"},{"location":"userGuide/projectsapi/#3-bulk-operations","title":"3. Bulk Operations","text":"<p>Batch Project Creation: <pre><code>def create_projects_batch(project_list):\n    created_projects = []\n    failed_projects = []\n\n    for project_data in project_list:\n        try:\n            response = requests.post('http://localhost:8000/api/v1/projects/', json=project_data)\n            response.raise_for_status()\n            created_projects.append(response.json())\n        except requests.exceptions.HTTPError as e:\n            failed_projects.append({\n                \"project\": project_data,\n                \"error\": e.response.json().get('detail', str(e))\n            })\n        except Exception as e:\n            failed_projects.append({\n                \"project\": project_data,\n                \"error\": str(e)\n            })\n\n    return {\n        \"created\": created_projects,\n        \"failed\": failed_projects,\n        \"success_rate\": len(created_projects) / len(project_list) if project_list else 0\n    }\n</code></pre></p> <p>Bulk Project Deletion: <pre><code>def cleanup_test_projects(name_pattern=\"test_\"):\n    # Get all projects\n    response = requests.get('http://localhost:8000/api/v1/projects/')\n    all_projects = response.json()['projects']\n\n    # Filter test projects\n    test_projects = [p for p in all_projects if name_pattern in p['name'].lower()]\n    test_project_ids = [p['id'] for p in test_projects]\n\n    if not test_project_ids:\n        print(\"No test projects found\")\n        return\n\n    print(f\"Found {len(test_project_ids)} test projects to delete\")\n\n    # Bulk delete\n    delete_response = requests.delete('http://localhost:8000/api/v1/projects/', json={\n        \"project_ids\": test_project_ids\n    })\n\n    result = delete_response.json()\n    print(f\"Deleted: {len(result.get('deleted', []))}\")\n    if result.get('failed'):\n        print(f\"Failed: {len(result['failed'])}\")\n</code></pre></p>"},{"location":"userGuide/projectsapi/#4-error-handling","title":"4. Error Handling","text":"<p>Robust API Usage: <pre><code>def safe_project_operations():\n    try:\n        # Create project\n        create_response = requests.post('http://localhost:8000/api/v1/projects/', json={\n            \"name\": \"Test Project\",\n            \"description\": \"Testing project operations\"\n        })\n\n        if create_response.status_code == 400:\n            error_detail = create_response.json().get('detail', '')\n            if 'already exists' in error_detail:\n                print(\"Project name already taken, choosing alternative...\")\n                # Handle duplicate name\n            else:\n                print(f\"Validation error: {error_detail}\")\n        elif create_response.status_code == 200:\n            project = create_response.json()\n            project_id = project['id']\n            print(f\"Created project: {project['name']}\")\n\n            # Get project details\n            get_response = requests.get(f'http://localhost:8000/api/v1/projects/{project_id}')\n            if get_response.status_code == 200:\n                details = get_response.json()\n                print(f\"Project has {details['document_count']} documents\")\n\n            # Clean up - delete project\n            delete_response = requests.delete(f'http://localhost:8000/api/v1/projects/{project_id}')\n            if delete_response.status_code == 200:\n                print(\"Project deleted successfully\")\n        else:\n            print(f\"Unexpected status code: {create_response.status_code}\")\n\n    except requests.exceptions.ConnectionError:\n        print(\"Cannot connect to API server\")\n    except requests.exceptions.Timeout:\n        print(\"Request timed out\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n</code></pre></p>"},{"location":"userGuide/projectsapi/#5-project-templates","title":"5. Project Templates","text":"<p>Project Template System: <pre><code>PROJECT_TEMPLATES = {\n    \"medical_research\": {\n        \"description\": \"Medical document analysis and research project\",\n        \"default_documents\": [\"medical_reports\", \"clinical_trials\", \"research_papers\"],\n        \"suggested_taxonomies\": [\"medical_conditions\", \"treatments\", \"diagnostics\"]\n    },\n    \"legal_analysis\": {\n        \"description\": \"Legal document review and contract analysis\",\n        \"default_documents\": [\"contracts\", \"agreements\", \"legal_opinions\"],\n        \"suggested_taxonomies\": [\"contract_types\", \"legal_entities\", \"obligations\"]\n    },\n    \"technical_docs\": {\n        \"description\": \"Technical documentation processing and analysis\",\n        \"default_documents\": [\"api_docs\", \"user_manuals\", \"specifications\"],\n        \"suggested_taxonomies\": [\"components\", \"features\", \"functionality\"]\n    }\n}\n\ndef create_project_from_template(template_name, project_name):\n    if template_name not in PROJECT_TEMPLATES:\n        raise ValueError(f\"Unknown template: {template_name}\")\n\n    template = PROJECT_TEMPLATES[template_name]\n\n    project_data = {\n        \"name\": project_name,\n        \"description\": template[\"description\"]\n    }\n\n    response = requests.post('http://localhost:8000/api/v1/projects/', json=project_data)\n\n    if response.status_code == 200:\n        project = response.json()\n        print(f\"Created project '{project_name}' from {template_name} template\")\n        print(f\"Suggested document types: {', '.join(template['default_documents'])}\")\n        print(f\"Suggested taxonomies: {', '.join(template['suggested_taxonomies'])}\")\n        return project\n    else:\n        raise Exception(f\"Failed to create project: {response.text}\")\n</code></pre></p> <p>This API provides comprehensive project management capabilities essential for organizing document processing workflows, tracking progress, and maintaining data organization within the Compileo system.</p>"},{"location":"userGuide/promptsapi/","title":"Prompts API in Compileo","text":""},{"location":"userGuide/promptsapi/#overview","title":"Overview","text":"<p>The Compileo Prompts API provides comprehensive management capabilities for AI prompts used throughout the system. It enables creation, retrieval, updating, and deletion of prompts that guide AI model behavior for various tasks.</p>"},{"location":"userGuide/promptsapi/#base-url-apiv1prompts","title":"Base URL: <code>/api/v1/prompts</code>","text":""},{"location":"userGuide/promptsapi/#1-list-prompts","title":"1. List Prompts","text":"<p>Endpoint: <code>GET /</code></p> <p>Description: Retrieves a list of all available prompts with optional filtering.</p> <p>Query Parameters: - <code>name_filter</code> (string, optional): Filter prompts by name substring - <code>limit</code> (integer, optional, default: 50): Maximum number of prompts to return</p> <p>Success Response (200 OK): <pre><code>{\n  \"prompts\": [\n    {\n      \"id\": 1,\n      \"name\": \"medical_qa_generation\",\n      \"content\": \"Generate medical Q&amp;A pairs based on the following context...\",\n      \"created_at\": \"2024-01-15T10:30:00Z\",\n      \"updated_at\": \"2024-01-20T14:45:00Z\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"taxonomy_extraction\",\n      \"content\": \"Extract taxonomy categories from the provided text...\",\n      \"created_at\": \"2024-01-16T09:15:00Z\",\n      \"updated_at\": null\n    }\n  ],\n  \"total\": 2\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#2-create-prompt","title":"2. Create Prompt","text":"<p>Endpoint: <code>POST /</code></p> <p>Description: Creates a new prompt in the system.</p> <p>Request Body: <pre><code>{\n  \"name\": \"custom_extraction_prompt\",\n  \"content\": \"You are an expert at extracting structured information from medical texts. Given the following text, identify and categorize all medical conditions, treatments, and symptoms mentioned...\"\n}\n</code></pre></p> <p>Parameters: - <code>name</code> (string, required): Unique identifier for the prompt - <code>content</code> (string, required): Full prompt text content</p> <p>Success Response (201 Created): <pre><code>{\n  \"id\": 3,\n  \"name\": \"custom_extraction_prompt\",\n  \"content\": \"You are an expert at extracting structured information from medical texts...\",\n  \"created_at\": \"2024-01-21T11:00:00Z\"\n}\n</code></pre></p> <p>Error Response (400 Bad Request): <pre><code>{\n  \"detail\": \"Prompt with name 'custom_extraction_prompt' already exists\"\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#3-get-prompt-by-id","title":"3. Get Prompt by ID","text":"<p>Endpoint: <code>GET /{prompt_id}</code></p> <p>Description: Retrieves a specific prompt by its unique identifier.</p> <p>Path Parameters: - <code>prompt_id</code> (integer, required): Prompt identifier</p> <p>Success Response (200 OK): <pre><code>{\n  \"id\": 3,\n  \"name\": \"custom_extraction_prompt\",\n  \"content\": \"You are an expert at extracting structured information from medical texts...\",\n  \"created_at\": \"2024-01-21T11:00:00Z\",\n  \"updated_at\": \"2024-01-21T15:30:00Z\"\n}\n</code></pre></p> <p>Error Response (404 Not Found): <pre><code>{\n  \"detail\": \"Prompt with ID 999 not found\"\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#4-get-prompt-by-name","title":"4. Get Prompt by Name","text":"<p>Endpoint: <code>GET /by-name/{name}</code></p> <p>Description: Retrieves a specific prompt by its name identifier.</p> <p>Path Parameters: - <code>name</code> (string, required): Prompt name identifier</p> <p>Success Response (200 OK): Same as Get Prompt by ID</p> <p>Error Response (404 Not Found): <pre><code>{\n  \"detail\": \"Prompt 'nonexistent_prompt' not found\"\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#5-update-prompt","title":"5. Update Prompt","text":"<p>Endpoint: <code>PUT /{prompt_id}</code></p> <p>Description: Updates an existing prompt's name and/or content.</p> <p>Path Parameters: - <code>prompt_id</code> (integer, required): Prompt identifier</p> <p>Request Body: <pre><code>{\n  \"name\": \"updated_extraction_prompt\",\n  \"content\": \"Updated prompt content with additional instructions...\"\n}\n</code></pre></p> <p>Parameters: - <code>name</code> (string, optional): New name for the prompt - <code>content</code> (string, optional): Updated prompt content</p> <p>Success Response (200 OK): <pre><code>{\n  \"id\": 3,\n  \"name\": \"updated_extraction_prompt\",\n  \"content\": \"Updated prompt content with additional instructions...\",\n  \"created_at\": \"2024-01-21T11:00:00Z\",\n  \"updated_at\": \"2024-01-21T16:45:00Z\"\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#6-delete-prompt","title":"6. Delete Prompt","text":"<p>Endpoint: <code>DELETE /{prompt_id}</code></p> <p>Description: Permanently removes a prompt from the system.</p> <p>Path Parameters: - <code>prompt_id</code> (integer, required): Prompt identifier</p> <p>Success Response (200 OK): <pre><code>{\n  \"message\": \"Prompt 3 deleted successfully\"\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#prompt-types-and-usage","title":"Prompt Types and Usage","text":""},{"location":"userGuide/promptsapi/#dataset-generation-prompts","title":"Dataset Generation Prompts","text":"<p>Used for creating training data from processed documents: - Question-answer pair generation - Multiple choice question creation - Fill-in-the-blank exercises - Explanation generation</p>"},{"location":"userGuide/promptsapi/#taxonomy-extraction-prompts","title":"Taxonomy Extraction Prompts","text":"<p>Guide AI models in categorizing content: - Category identification and classification - Hierarchical taxonomy construction - Content tagging and labeling - Semantic categorization</p>"},{"location":"userGuide/promptsapi/#quality-assessment-prompts","title":"Quality Assessment Prompts","text":"<p>Support quality analysis workflows: - Content evaluation criteria - Bias detection guidelines - Difficulty assessment frameworks - Consistency validation rules</p>"},{"location":"userGuide/promptsapi/#custom-analysis-prompts","title":"Custom Analysis Prompts","text":"<p>Domain-specific or specialized prompts: - Medical content analysis - Legal document processing - Technical documentation parsing - Research paper summarization</p>"},{"location":"userGuide/promptsapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/promptsapi/#prompt-design","title":"Prompt Design","text":"<ul> <li>Clear Instructions: Provide explicit, unambiguous guidance</li> <li>Context Setting: Include relevant background and constraints</li> <li>Output Formatting: Specify desired response structure</li> <li>Error Handling: Include guidance for edge cases</li> </ul>"},{"location":"userGuide/promptsapi/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Descriptive Names: Use clear, descriptive identifiers</li> <li>Consistent Prefixes: Group related prompts by functionality</li> <li>Version Indicators: Include version numbers for iterations</li> <li>Domain Specificity: Reflect domain or use case in naming</li> </ul>"},{"location":"userGuide/promptsapi/#content-management","title":"Content Management","text":"<ul> <li>Version Control: Track prompt evolution and improvements</li> <li>Testing: Validate prompts across different content types</li> <li>Performance Monitoring: Track prompt effectiveness over time</li> <li>Regular Updates: Refine prompts based on results and feedback</li> </ul>"},{"location":"userGuide/promptsapi/#security-considerations","title":"Security Considerations","text":"<ul> <li>Input Validation: Sanitize prompt content and metadata</li> <li>Access Control: Implement appropriate permission levels</li> <li>Audit Logging: Track prompt creation, modification, and usage</li> <li>Content Review: Review prompts for sensitive or inappropriate content</li> </ul>"},{"location":"userGuide/promptsapi/#integration-examples","title":"Integration Examples","text":""},{"location":"userGuide/promptsapi/#using-prompts-in-dataset-generation","title":"Using Prompts in Dataset Generation","text":"<pre><code># Retrieve prompt for dataset generation\nprompt = get_prompt_by_name(\"medical_qa_generation\")\n\n# Use in AI model interaction\nresponse = ai_model.generate(\n    prompt=prompt.content,\n    context=document_content,\n    parameters={\"max_questions\": 10}\n)\n</code></pre>"},{"location":"userGuide/promptsapi/#dynamic-prompt-selection","title":"Dynamic Prompt Selection","text":"<pre><code># Select appropriate prompt based on content type\nif content_type == \"medical\":\n    prompt = get_prompt_by_name(\"medical_extraction\")\nelif content_type == \"legal\":\n    prompt = get_prompt_by_name(\"legal_analysis\")\nelse:\n    prompt = get_prompt_by_name(\"general_extraction\")\n</code></pre>"},{"location":"userGuide/promptsapi/#prompt-versioning","title":"Prompt Versioning","text":"<pre><code># Use versioned prompts for consistency\nprompt_v2 = get_prompt_by_name(\"extraction_prompt_v2.1\")\nresults = process_content(content, prompt_v2.content)\n</code></pre>"},{"location":"userGuide/promptsapi/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/promptsapi/#common-error-responses","title":"Common Error Responses","text":"<p>400 Bad Request: <pre><code>{\n  \"detail\": \"Prompt with name 'duplicate_name' already exists\"\n}\n</code></pre></p> <p>404 Not Found: <pre><code>{\n  \"detail\": \"Prompt with ID 999 not found\"\n}\n</code></pre></p> <p>500 Internal Server Error: <pre><code>{\n  \"detail\": \"Database connection failed during prompt creation\"\n}\n</code></pre></p>"},{"location":"userGuide/promptsapi/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Read Operations: 100 requests per minute</li> <li>Write Operations: 30 requests per minute</li> <li>Bulk Operations: 10 requests per minute</li> </ul> <p>This prompts API provides a centralized system for managing AI model guidance, ensuring consistent and effective prompt usage across all Compileo operations.</p>"},{"location":"userGuide/settingsgui/","title":"Settings GUI User Guide","text":""},{"location":"userGuide/settingsgui/#overview","title":"Overview","text":"<p>The Settings page provides comprehensive configuration options for Compileo's AI models, system security, and processing parameters. This includes API key management for the Compileo system itself, as well as multiple AI providers (Ollama, Gemini, Grok, OpenAI). This guide covers security configuration and Ollama parameter fine-tuning.</p>"},{"location":"userGuide/settingsgui/#api-authentication-security","title":"\ud83d\udd12 API Authentication &amp; Security","text":"<p>Compileo implements an \"Auto-Lock\" security model. This allows for a simplified setup while ensuring production-grade security once configured.</p>"},{"location":"userGuide/settingsgui/#configuring-the-system-api-key","title":"Configuring the System API Key","text":"<p>To secure your Compileo instance: 1.  Navigate to the Settings page in the Compileo GUI. 2.  Locate the \ud83d\udd17 API Configuration section. 3.  Enter a strong, secret value in the API Key field. 4.  Click Save Settings.</p> <p>Security Behavior: *   Unsecured Mode: If the API Key field is empty (and no environment variables are set), Compileo allows all requests. *   Secured Mode: Once a key is saved, the API immediately \"locks\" and requires an <code>X-API-Key</code> header for all further access from both the GUI and external tools.</p>"},{"location":"userGuide/settingsgui/#provider-api-keys","title":"Provider API Keys","text":"<p>The settings page also allows you to manage keys for external LLM providers (Gemini, Grok, OpenAI). These are stored in the database and used by Compileo's backend to perform AI tasks.</p>"},{"location":"userGuide/settingsgui/#ollama-parameter-configuration","title":"Ollama Parameter Configuration","text":"<p>Compileo supports advanced configuration of Ollama AI models through role-specific parameters. Each AI processing role (parsing, taxonomy, classification, generation) can have customized Ollama parameters.</p>"},{"location":"userGuide/settingsgui/#accessing-parameter-settings","title":"Accessing Parameter Settings","text":"<ol> <li>Navigate to the Settings page in the Compileo GUI</li> <li>Scroll to the AI Model Configuration section</li> <li>Select an Ollama model for any processing role (parsing, taxonomy, classification, generation)</li> <li>Parameter input fields will appear below the model dropdown</li> </ol>"},{"location":"userGuide/settingsgui/#parameter-layout","title":"Parameter Layout","text":"<p>Parameters are arranged in a compact 4-column by 2-row grid for each Ollama role:</p> <p>Row 1: temperature | repeat_penalty | top_p | top_k Row 2: num_predict | seed | num_ctx | (empty)</p>"},{"location":"userGuide/settingsgui/#available-parameters","title":"Available Parameters","text":""},{"location":"userGuide/settingsgui/#temperature-00-20","title":"Temperature (0.0-2.0)","text":"<p>Controls randomness in AI responses: - Lower values (0.0-0.5): More focused, deterministic responses - Higher values (1.0-2.0): More creative, varied responses - Default: 0.1 (parsing), 0.8 (other roles)</p>"},{"location":"userGuide/settingsgui/#repeat-penalty-00-20","title":"Repeat Penalty (0.0-2.0)","text":"<p>Reduces repetition in generated text: - Lower values (&lt; 1.0): Allow more repetition - Higher values (&gt; 1.0): Strongly penalize repetition - Default: 1.1</p>"},{"location":"userGuide/settingsgui/#top-p-00-10","title":"Top P (0.0-1.0)","text":"<p>Nucleus sampling parameter: - Lower values (0.1-0.5): More focused responses - Higher values (0.9-1.0): More diverse responses - Default: 0.9</p>"},{"location":"userGuide/settingsgui/#top-k-0-100","title":"Top K (0-100)","text":"<p>Limits candidate tokens for sampling: - Lower values (1-20): More focused responses - Higher values (50-100): More diverse responses - Default: 40</p>"},{"location":"userGuide/settingsgui/#num-predict-1-4096","title":"Num Predict (1-4096)","text":"<p>Maximum number of tokens to generate: - Lower values (100-500): Shorter responses - Higher values (1000-4000): Longer responses - Default: 1024 (parsing), 8192 (parsing with higher limit)</p>"},{"location":"userGuide/settingsgui/#seed-0-4294967295","title":"Seed (0-4294967295)","text":"<p>Random seed for reproducible results: - Same seed: Identical responses for same input - Different/random seed: Varied responses - Default: None (random)</p>"},{"location":"userGuide/settingsgui/#num-ctx-1-131072","title":"Num Ctx (1-131072)","text":"<p>Context window size for Ollama models: - Lower values (4096-8192): Faster processing, less memory usage - Higher values (32768-131072): Better context understanding, more memory usage - Default: 60000 (balanced performance and capability)</p>"},{"location":"userGuide/settingsgui/#role-specific-recommendations","title":"Role-Specific Recommendations","text":""},{"location":"userGuide/settingsgui/#document-parsing","title":"Document Parsing","text":"<ul> <li>Temperature: 0.1 (deterministic OCR output)</li> <li>Num Predict: 8192 (handle long documents)</li> <li>Num Ctx: 32768 (sufficient context for document understanding)</li> <li>Other parameters: Conservative defaults for accuracy</li> </ul>"},{"location":"userGuide/settingsgui/#taxonomy-generation","title":"Taxonomy Generation","text":"<ul> <li>Temperature: 0.8 (balanced creativity)</li> <li>Num Predict: 2048 (comprehensive taxonomies)</li> <li>Num Ctx: 65536 (large context for analyzing multiple chunks)</li> <li>Top P: 0.95 (diverse category suggestions)</li> </ul>"},{"location":"userGuide/settingsgui/#classification","title":"Classification","text":"<ul> <li>Temperature: 0.3 (consistent categorization)</li> <li>Num Predict: 512 (concise classifications)</li> <li>Num Ctx: 16384 (focused context for individual classifications)</li> <li>Repeat Penalty: 1.2 (avoid repetitive classifications)</li> </ul>"},{"location":"userGuide/settingsgui/#dataset-generation","title":"Dataset Generation","text":"<ul> <li>Temperature: 0.8 (creative variations)</li> <li>Num Predict: 1024 (balanced response length)</li> <li>Num Ctx: 32768 (adequate context for chunk-based generation)</li> <li>Top P: 0.9 (diverse question/answer pairs)</li> </ul>"},{"location":"userGuide/settingsgui/#saving-configuration","title":"Saving Configuration","text":"<ol> <li>Adjust parameter values as needed</li> <li>Click Save Settings to persist changes</li> <li>Parameters are immediately applied to subsequent processing jobs</li> </ol>"},{"location":"userGuide/settingsgui/#parameter-validation","title":"Parameter Validation","text":"<p>The interface includes built-in validation: - Range checking: Values must be within specified ranges - Type validation: Numeric inputs only - Real-time feedback: Invalid values show error indicators</p>"},{"location":"userGuide/settingsgui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/settingsgui/#parameters-not-applied","title":"Parameters Not Applied","text":"<ul> <li>Ensure settings are saved after changes</li> <li>Restart any running jobs to pick up new parameters</li> <li>Check that the correct Ollama model is selected</li> </ul>"},{"location":"userGuide/settingsgui/#unexpected-ai-behavior","title":"Unexpected AI Behavior","text":"<ul> <li>Try conservative parameter values first (temperature 0.1-0.3)</li> <li>Adjust one parameter at a time to isolate effects</li> <li>Reset to defaults if issues persist</li> </ul>"},{"location":"userGuide/settingsgui/#performance-issues","title":"Performance Issues","text":"<ul> <li>Lower num_predict for faster processing</li> <li>Reduce temperature for more consistent outputs</li> <li>Monitor Ollama resource usage</li> </ul>"},{"location":"userGuide/settingsgui/#global-settings","title":"Global Settings","text":""},{"location":"userGuide/settingsgui/#log-level-configuration","title":"Log Level Configuration","text":"<p>Compileo provides a project-wide logging system that can be controlled through the GUI, API, or CLI. This allows you to adjust the verbosity of logs based on your needs.</p>"},{"location":"userGuide/settingsgui/#available-log-levels","title":"Available Log Levels","text":"<ul> <li>none: Disables all logging output. Useful for production environments where minimal noise is desired.</li> <li>error: Only logs critical errors and exceptions. Recommended for standard usage.</li> <li>debug: Enables extensive log reporting, including internal process details and JSON-structured debug information. Intended for developers and troubleshooting.</li> </ul>"},{"location":"userGuide/settingsgui/#configuring-log-level-in-gui","title":"Configuring Log Level in GUI","text":"<ol> <li>Navigate to the General tab in the Settings page.</li> <li>Locate the Log Level dropdown.</li> <li>Select your desired level (none, error, or debug).</li> <li>Click Save Settings. The new log level is applied immediately to all system components, including background workers.</li> </ol>"},{"location":"userGuide/settingsgui/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"userGuide/settingsgui/#custom-model-selection","title":"Custom Model Selection","text":"<p>Parameters are configured per Ollama model. Different models can have different parameter sets:</p> <ol> <li>Select different Ollama models for different roles</li> <li>Configure parameters independently for each model</li> <li>Compare results across different model+parameter combinations</li> </ol>"},{"location":"userGuide/settingsgui/#batch-processing","title":"Batch Processing","text":"<p>Configured parameters apply to all batch processing operations: - Document parsing jobs - Taxonomy generation tasks - Dataset creation workflows - Classification operations</p>"},{"location":"userGuide/settingsgui/#integration-with-cliapi","title":"Integration with CLI/API","text":"<p>Parameters configured in the GUI are automatically used by: - CLI commands using Ollama models - API endpoints processing with Ollama - Background job execution</p>"},{"location":"userGuide/settingsgui/#best-practices","title":"Best Practices","text":"<ol> <li>Start Conservative: Begin with default or low parameter values</li> <li>Test Incrementally: Change one parameter at a time</li> <li>Document Settings: Note parameter combinations that work well</li> <li>Role-Specific Tuning: Use different parameters for different AI tasks</li> <li>Monitor Performance: Adjust based on output quality and processing speed</li> </ol>"},{"location":"userGuide/settingsgui/#support","title":"Support","text":"<p>For issues with parameter configuration: - Check Ollama server logs for API errors - Verify parameter ranges are valid - Ensure settings are properly saved</p>"},{"location":"userGuide/settingsgui/#plugin-management","title":"Plugin Management","text":"<p>The Settings page includes a dedicated Plugins tab for extending Compileo's functionality.</p>"},{"location":"userGuide/settingsgui/#accessing-plugins","title":"Accessing Plugins","text":"<ol> <li>Click the Plugins tab at the top of the Settings page.</li> <li>View the list of installed plugins or upload new ones.</li> </ol>"},{"location":"userGuide/settingsgui/#features","title":"Features","text":"<ul> <li>Upload Plugin: Install new plugins by uploading <code>.zip</code> files.</li> <li>List Plugins: View details of installed plugins including version and author.</li> <li>Uninstall: Remove plugins that are no longer needed.</li> </ul> <p>For detailed instructions, refer to the Plugin Management User Guide. - Test with default parameters first</p>"},{"location":"userGuide/taxonomyapi/","title":"Taxonomy Module API Usage Guide","text":"<p>The Compileo Taxonomy API provides comprehensive REST endpoints for taxonomy management, including creation, generation, extension, and retrieval. This guide covers all available API endpoints with examples and best practices.</p>"},{"location":"userGuide/taxonomyapi/#base-url-apiv1taxonomy","title":"Base URL: <code>/api/v1/taxonomy</code>","text":""},{"location":"userGuide/taxonomyapi/#taxonomy-creation","title":"Taxonomy Creation","text":""},{"location":"userGuide/taxonomyapi/#post","title":"POST <code>/</code>","text":"<p>Create a new manual taxonomy from JSON structure.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/taxonomy/\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Medical Conditions\",\n    \"description\": \"Classification of medical conditions\",\n    \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n    \"taxonomy\": {\n      \"name\": \"Medical Conditions\",\n      \"description\": \"Hierarchical classification of medical conditions\",\n      \"children\": [\n        {\n          \"name\": \"Cardiovascular\",\n          \"description\": \"Heart and blood vessel conditions\",\n          \"confidence_threshold\": 0.8,\n          \"children\": []\n        },\n        {\n          \"name\": \"Respiratory\",\n          \"description\": \"Lung and breathing conditions\",\n          \"confidence_threshold\": 0.8,\n          \"children\": []\n        }\n      ]\n    }\n  }'\n</code></pre></p> <p>Request Body: - <code>name</code>: Taxonomy name (required) - <code>description</code>: Taxonomy description (optional) - <code>project_id</code>: Associated project ID (required) - <code>taxonomy</code>: JSON taxonomy structure (required)</p> <p>Response: <pre><code>{\n  \"id\": \"069671d5-6c2c-4327-88dc-6abd12b5671c\",\n  \"name\": \"Medical Conditions\",\n  \"description\": \"Classification of medical conditions\",\n  \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n  \"categories_count\": 3,\n  \"confidence_score\": 0.8,\n  \"created_at\": \"2024-01-21T12:00:00Z\",\n  \"file_path\": \"storage/taxonomy/b357e573-89a5-4b40-8e1b-4c075a1835a6/manual_taxonomy_uuid.json\"\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#ai-taxonomy-generation","title":"AI Taxonomy Generation","text":""},{"location":"userGuide/taxonomyapi/#post-generate","title":"POST <code>/generate</code>","text":"<p>Generate a new taxonomy using AI from document chunks.</p> <p>Important: Documents must be parsed and chunked before taxonomy generation. The system follows a multi-step workflow: Upload \u2192 Parse \u2192 Chunk \u2192 Generate Taxonomy.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/taxonomy/generate\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n    \"name\": \"AI Generated Taxonomy\",\n    \"documents\": [\"b0b69234-8f99-41d2-a0e7-b7cd5cfcb593\", \"a1c2d3e4-f5g6-h7i8-j9k0-l1m2n3o4p5q6\"],\n    \"depth\": 3,\n    \"generator\": \"gemini\",\n    \"domain\": \"medical\",\n    \"batch_size\": 10,\n    \"category_limits\": [5, 10, 15],\n    \"specificity_level\": 1,\n    \"processing_mode\": \"complete\"\n  }'\n</code></pre></p> <p>Request Body: - <code>project_id</code>: Project containing documents (required) - <code>name</code>: Taxonomy name (optional) - <code>documents</code>: Array of document IDs (required) - documents must be parsed and chunked - <code>depth</code>: Taxonomy hierarchy depth (default: 3) - <code>generator</code>: AI model (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>openai</code>) (default: <code>gemini</code>) - <code>domain</code>: Content domain (default: <code>general</code>) - <code>batch_size</code>: Number of complete chunks to process per batch (default: 10) - <code>category_limits</code>: Max categories per level (optional) - <code>specificity_level</code>: Specificity level 1-5 (default: 1) - <code>processing_mode</code>: Generation mode (<code>\"fast\"</code> or <code>\"complete\"</code>) (default: <code>\"fast\"</code>)   - <code>\"fast\"</code>: Samples a single batch of chunks (quickest, but may miss content).   - <code>\"complete\"</code>: Iteratively processes 100% of chunks in batches (comprehensive, but slower).</p> <p>Prerequisites: - Documents must be uploaded and parsed - Documents must be chunked using the chunking API - Chunks are stored as individual <code>.md</code> files</p> <p>Response: <pre><code>{\n  \"id\": \"069671d5-6c2c-4327-88dc-6abd12b5671c\",\n  \"name\": \"AI Generated Taxonomy\",\n  \"description\": \"AI-generated taxonomy using gemini\",\n  \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n  \"categories_count\": 45,\n  \"confidence_score\": 0.85,\n  \"created_at\": \"2024-01-21T12:05:00Z\",\n  \"file_path\": \"storage/taxonomy/b357e573-89a5-4b40-8e1b-4c075a1835a6/ai_taxonomy_uuid.json\"\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#taxonomy-extension","title":"Taxonomy Extension","text":""},{"location":"userGuide/taxonomyapi/#post-extend","title":"POST <code>/extend</code>","text":"<p>Extend an existing taxonomy with additional hierarchy levels.</p> <p>Request: <pre><code>curl -X POST \"http://localhost:8000/api/v1/taxonomy/extend\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"taxonomy_id\": 102,\n    \"additional_depth\": 2,\n    \"generator\": \"gemini\",\n    \"domain\": \"medical\",\n    \"batch_size\": 10,\n    \"category_limits\": [8, 12],\n    \"specificity_level\": 2,\n    \"processing_mode\": \"complete\"\n  }'\n</code></pre></p> <p>Request Body: - <code>taxonomy_id</code>: ID of taxonomy to extend (required) - <code>additional_depth</code>: Number of levels to add (default: 2) - <code>generator</code>: AI model for extension (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>openai</code>) (default: <code>gemini</code>) - <code>domain</code>: Content domain (default: <code>general</code>) - <code>batch_size</code>: Number of complete chunks to process per batch (optional) - <code>category_limits</code>: Max categories per new level (optional) - <code>specificity_level</code>: Specificity level 1-5 (default: 1) - <code>documents</code>: Array of document IDs to analyze for extension context (optional) - <code>processing_mode</code>: Generation mode (<code>\"fast\"</code> or <code>\"complete\"</code>) (default: <code>\"fast\"</code>)</p> <p>Alternative: Extend from taxonomy data <pre><code>{\n  \"taxonomy_data\": {\n    \"name\": \"Cardiovascular\",\n    \"description\": \"Heart conditions\",\n    \"children\": []\n  },\n  \"project_id\": 1,\n  \"additional_depth\": 2,\n  \"generator\": \"gemini\"\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"id\": 102,\n  \"name\": \"AI Generated Taxonomy\",\n  \"description\": \"AI-extended taxonomy with 2 additional levels using gemini\",\n  \"project_id\": 1,\n  \"categories_count\": 78,\n  \"confidence_score\": 0.82,\n  \"created_at\": \"2024-01-21T12:10:00Z\",\n  \"file_path\": \"storage/taxonomy/1/ai_taxonomy_uuid.json\"\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#taxonomy-retrieval","title":"Taxonomy Retrieval","text":""},{"location":"userGuide/taxonomyapi/#get","title":"GET <code>/</code>","text":"<p>List all available taxonomies with optional project filtering.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/taxonomy/?project_id=b357e573-89a5-4b40-8e1b-4c075a1835a6\"\n</code></pre></p> <p>Query Parameters: - <code>project_id</code>: Filter by project ID (optional)</p> <p>Response: <pre><code>{\n  \"taxonomies\": [\n    {\n      \"id\": \"069671d5-6c2c-4327-88dc-6abd12b5671c\",\n      \"name\": \"Medical Conditions\",\n      \"description\": \"Classification of medical conditions\",\n      \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n      \"categories_count\": 45,\n      \"confidence_score\": 0.85,\n      \"created_at\": \"2024-01-21T12:00:00Z\",\n      \"file_path\": \"storage/taxonomy/b357e573-89a5-4b40-8e1b-4c075a1835a6/manual_taxonomy_uuid.json\"\n    },\n    {\n      \"id\": \"1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p\",\n      \"name\": \"AI Generated Taxonomy\",\n      \"description\": \"AI-generated taxonomy using gemini\",\n      \"project_id\": \"b357e573-89a5-4b40-8e1b-4c075a1835a6\",\n      \"categories_count\": 78,\n      \"confidence_score\": 0.82,\n      \"created_at\": \"2024-01-21T12:05:00Z\",\n      \"file_path\": \"storage/taxonomy/b357e573-89a5-4b40-8e1b-4c075a1835a6/ai_taxonomy_uuid.json\"\n    }\n  ],\n  \"total\": 2\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#get-taxonomy_id","title":"GET <code>/{taxonomy_id}</code>","text":"<p>Get detailed taxonomy information including structure and analytics.</p> <p>Request: <pre><code>curl \"http://localhost:8000/api/v1/taxonomy/069671d5-6c2c-4327-88dc-6abd12b5671c\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"taxonomy\": {\n    \"name\": \"Medical Conditions\",\n    \"description\": \"Classification of medical conditions\",\n    \"children\": [\n      {\n        \"name\": \"Cardiovascular\",\n        \"description\": \"Heart and blood vessel conditions\",\n        \"confidence_threshold\": 0.8,\n        \"children\": [\n          {\n            \"name\": \"Coronary Artery Disease\",\n            \"description\": \"Blockage of coronary arteries\",\n            \"confidence_threshold\": 0.85,\n            \"children\": []\n          }\n        ]\n      }\n    ]\n  },\n  \"metadata\": {\n    \"type\": \"manual\",\n    \"confidence_score\": 0.8,\n    \"created_manually\": true\n  },\n  \"analytics\": {\n    \"depth_analysis\": {\n      \"total_categories\": 3,\n      \"max_depth\": 2\n    }\n  }\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#taxonomy-management","title":"Taxonomy Management","text":""},{"location":"userGuide/taxonomyapi/#put-taxonomy_id","title":"PUT <code>/{taxonomy_id}</code>","text":"<p>Update taxonomy information.</p> <p>Request: <pre><code>curl -X PUT \"http://localhost:8000/api/v1/taxonomy/069671d5-6c2c-4327-88dc-6abd12b5671c\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Updated Medical Conditions\"\n  }'\n</code></pre></p> <p>Request Body: - <code>name</code>: New taxonomy name (required)</p> <p>Response: <pre><code>{\n  \"id\": 101,\n  \"name\": \"Updated Medical Conditions\",\n  \"description\": \"Classification of medical conditions\",\n  \"project_id\": 1,\n  \"categories_count\": 45,\n  \"confidence_score\": 0.85,\n  \"created_at\": \"2024-01-21T12:00:00Z\",\n  \"file_path\": \"storage/taxonomy/1/manual_taxonomy_uuid.json\"\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#delete-taxonomy_id","title":"DELETE <code>/{taxonomy_id}</code>","text":"<p>Delete a taxonomy. This operation performs a complete cleanup, removing the taxonomy's file from the filesystem and deleting all associated extraction jobs and their results (both database entries and filesystem files).</p> <p>Request: <pre><code>curl -X DELETE \"http://localhost:8000/api/v1/taxonomy/069671d5-6c2c-4327-88dc-6abd12b5671c\"\n</code></pre></p> <p>Response: <pre><code>{\n  \"message\": \"Taxonomy 101 deleted successfully. Associated files and extraction results have been cleaned up.\"\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#delete","title":"DELETE <code>/</code>","text":"<p>Bulk delete multiple taxonomies. This operation performs a complete cleanup for all specified taxonomies, removing their files from the filesystem and deleting all associated extraction jobs and their results (both database entries and filesystem files).</p> <p>Request: <pre><code>curl -X DELETE \"http://localhost:8000/api/v1/taxonomy/\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"taxonomy_ids\": [\"069671d5-6c2c-4327-88dc-6abd12b5671c\", \"1a2b3c4d-5e6f-7g8h-9i0j-1k2l3m4n5o6p\", \"2b3c4d5-6e7f-8g9h-0i1j-2k3l4m5n6o7p\"]\n  }'\n</code></pre></p> <p>Request Body: - <code>taxonomy_ids</code>: Array of taxonomy IDs to delete (required)</p> <p>Response: <pre><code>{\n  \"message\": \"Successfully deleted 3 taxonomies\",\n  \"deleted\": [101, 102, 103]\n}\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/taxonomyapi/#1-taxonomy-generation","title":"1. Taxonomy Generation","text":"<p>Document Selection: <pre><code>import requests\n\n# Get available documents\ndocs_response = requests.get('http://localhost:8000/api/v1/documents/?project_id=1')\ndocuments = docs_response.json()['documents']\n\n# Select documents with chunks\ndoc_ids = [doc['id'] for doc in documents if doc.get('status') == 'parsed']\n\n# Generate taxonomy\ntaxonomy_response = requests.post('http://localhost:8000/api/v1/taxonomy/generate', json={\n    'project_id': 1,\n    'name': 'Medical Taxonomy',\n    'documents': doc_ids,\n    'depth': 3,\n    'generator': 'gemini',\n    'domain': 'medical',\n    'batch_size': 10\n})\n</code></pre></p> <p>Category Limits: <pre><code># Balanced taxonomy structure\ncategory_limits = [5, 10, 15]  # Level 1: 5, Level 2: 10, Level 3: 15\n\nrequests.post('http://localhost:8000/api/v1/taxonomy/generate', json={\n    'project_id': 1,\n    'documents': doc_ids,\n    'category_limits': category_limits,\n    'specificity_level': 2  # More specific categories\n})\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#2-taxonomy-extension","title":"2. Taxonomy Extension","text":"<p>Incremental Growth: <pre><code># Extend existing taxonomy with specific documents\nrequests.post('http://localhost:8000/api/v1/taxonomy/extend', json={\n    'taxonomy_id': 101,\n    'additional_depth': 1,\n    'generator': 'gemini',\n    'domain': 'medical',\n    'documents': [101, 102]\n})\n</code></pre></p> <p>Category-Specific Extension: <pre><code># Get taxonomy details\ntax_response = requests.get('http://localhost:8000/api/v1/taxonomy/101')\ntaxonomy = tax_response.json()['taxonomy']\n\n# Find specific category\ncardiovascular = None\nfor category in taxonomy.get('children', []):\n    if category['name'] == 'Cardiovascular':\n        cardiovascular = category\n        break\n\n# Extend specific category\nif cardiovascular:\n    requests.post('http://localhost:8000/api/v1/taxonomy/extend', json={\n        'taxonomy_data': cardiovascular,\n        'project_id': 1,\n        'additional_depth': 2,\n        'generator': 'gemini'\n    })\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#3-taxonomy-management","title":"3. Taxonomy Management","text":"<p>Bulk Operations: <pre><code># List all taxonomies\ntaxonomies = requests.get('http://localhost:8000/api/v1/taxonomy/').json()\n\n# Filter by confidence score\nhigh_confidence = [t for t in taxonomies['taxonomies'] if t['confidence_score'] &gt; 0.8]\n\n# Bulk delete low-confidence taxonomies\nlow_confidence_ids = [t['id'] for t in taxonomies['taxonomies'] if t['confidence_score'] &lt; 0.6]\nif low_confidence_ids:\n    requests.delete('http://localhost:8000/api/v1/taxonomy/', json={\n        'taxonomy_ids': low_confidence_ids\n    })\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#4-error-handling","title":"4. Error Handling","text":"<p>API Error Responses: <pre><code>try:\n    response = requests.post('http://localhost:8000/api/v1/taxonomy/generate', json={...})\n    response.raise_for_status()\n\n    taxonomy = response.json()\n    print(f\"Created taxonomy: {taxonomy['name']}\")\n\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 404:\n        print(\"Project not found\")\n    elif e.response.status_code == 400:\n        error_detail = e.response.json().get('detail', 'Bad request')\n        print(f\"Validation error: {error_detail}\")\n    else:\n        print(f\"API error: {e}\")\n\nexcept requests.exceptions.RequestException as e:\n    print(f\"Network error: {e}\")\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#5-performance-optimization","title":"5. Performance Optimization","text":"<p>Batch Processing: <pre><code># Process multiple taxonomy generations\ntaxonomies_to_create = [\n    {'name': 'Medical', 'domain': 'medical'},\n    {'name': 'Legal', 'domain': 'legal'},\n    {'name': 'Technical', 'domain': 'technical'}\n]\n\nfor tax_config in taxonomies_to_create:\n    response = requests.post('http://localhost:8000/api/v1/taxonomy/generate', json={\n        'project_id': 1,\n        'documents': doc_ids,\n        'depth': 3,\n        'generator': 'gemini',\n        **tax_config\n    })\n\n    if response.status_code == 200:\n        print(f\"Created taxonomy: {response.json()['name']}\")\n    else:\n        print(f\"Failed to create {tax_config['name']}: {response.text}\")\n</code></pre></p>"},{"location":"userGuide/taxonomyapi/#6-taxonomy-analytics","title":"6. Taxonomy Analytics","text":"<p>Structure Analysis: <pre><code># Get detailed taxonomy information\ntax_response = requests.get('http://localhost:8000/api/v1/taxonomy/101')\ntax_data = tax_response.json()\n\n# Analyze structure\ntaxonomy = tax_data['taxonomy']\nanalytics = tax_data['analytics']\n\nprint(f\"Total categories: {analytics['depth_analysis']['total_categories']}\")\nprint(f\"Maximum depth: {analytics['depth_analysis']['max_depth']}\")\nprint(f\"Confidence score: {tax_data['metadata']['confidence_score']}\")\n\n# Traverse taxonomy tree\ndef analyze_category(category, level=0):\n    indent = \"  \" * level\n    print(f\"{indent}{category['name']} (confidence: {category.get('confidence_threshold', 0)})\")\n\n    for child in category.get('children', []):\n        analyze_category(child, level + 1)\n\nanalyze_category(taxonomy)\n</code></pre></p> <p>This API provides comprehensive taxonomy management capabilities with support for manual creation, AI generation, extension, and full CRUD operations suitable for both development and production workflows.</p>"},{"location":"userGuide/taxonomycli/","title":"Taxonomy Module CLI Usage Guide","text":"<p>The Compileo Taxonomy CLI provides comprehensive command-line tools for taxonomy management, including creation, generation, extension, and retrieval. This guide covers all available CLI commands with examples and best practices.</p>"},{"location":"userGuide/taxonomycli/#command-overview","title":"Command Overview","text":"<pre><code>graph TD\n    A[Taxonomy CLI] --&gt; B[List]\n    A --&gt; C[Create]\n    A --&gt; D[Generate]\n    A --&gt; E[Extend]\n    A --&gt; F[Load]\n    A --&gt; G[Update]\n    A --&gt; H[Delete]\n    A --&gt; I[Bulk Delete]\n\n    B --&gt; B1[All taxonomies]\n    C --&gt; C1[Manual taxonomy]\n    D --&gt; D1[AI generation]\n    E --&gt; E1[Extend existing]\n    F --&gt; F1[View details]\n    G --&gt; G1[Update name]\n    H --&gt; H1[Single delete]\n    I --&gt; I1[Multiple delete]\n</code></pre>"},{"location":"userGuide/taxonomycli/#taxonomy-listing","title":"Taxonomy Listing","text":"<p>List all available taxonomies with optional project filtering:</p> <pre><code>compileo taxonomy list --project-id 1 --format table\n</code></pre> <p>Parameters: - <code>--project-id</code>: Filter by project ID (optional) - <code>--format</code>: Output format (<code>table</code>, <code>json</code>) (default: <code>table</code>)</p> <p>Example Output: <pre><code>\ud83d\udccb Taxonomies in project 1:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ID \u2502 Name                \u2502 Description                     \u2502 Categories  \u2502 Confidence      \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 101\u2502 Medical Conditions  \u2502 Classification of conditions    \u2502 45          \u2502 0.85            \u2502\n\u2502 102\u2502 AI Generated        \u2502 AI-generated taxonomy           \u2502 78          \u2502 0.82            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#manual-taxonomy-creation","title":"Manual Taxonomy Creation","text":"<p>Create a new taxonomy from a JSON file:</p> <pre><code>compileo taxonomy create --project-id 1 --name \"Medical Conditions\" --description \"Classification system\" --file taxonomy.json\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project ID for the taxonomy (required) - <code>--name</code>: Taxonomy name (required) - <code>--description</code>: Taxonomy description (optional) - <code>--file</code>: JSON file containing taxonomy structure (optional)</p> <p>Taxonomy JSON Structure: <pre><code>{\n  \"name\": \"Medical Conditions\",\n  \"description\": \"Hierarchical classification\",\n  \"children\": [\n    {\n      \"name\": \"Cardiovascular\",\n      \"description\": \"Heart conditions\",\n      \"confidence_threshold\": 0.8,\n      \"children\": [\n        {\n          \"name\": \"Coronary Artery Disease\",\n          \"description\": \"Artery blockage\",\n          \"confidence_threshold\": 0.85,\n          \"children\": []\n        }\n      ]\n    }\n  ]\n}\n</code></pre></p> <p>Example Output: <pre><code>\u2705 Taxonomy created successfully!\n\ud83d\udcca Taxonomy ID: 101\n\ud83d\udcc2 File: storage/taxonomy/1/manual_taxonomy_uuid.json\n\ud83c\udff7\ufe0f Categories: 3\n\ud83c\udfaf Confidence: 0.8\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#ai-taxonomy-generation","title":"AI Taxonomy Generation","text":"<p>Generate a new taxonomy using AI from document chunks:</p> <pre><code>compileo taxonomy generate --project-id 1 --name \"AI Medical Taxonomy\" --documents 101,102,103 --depth 3 --generator gemini --domain medical --batch-size 10 --category-limits 5,10,15 --specificity-level 2\n</code></pre> <p>Parameters: - <code>--project-id</code>: Project containing documents (required) - <code>--name</code>: Taxonomy name (required) - <code>--documents</code>: Comma-separated document IDs (required) - <code>--depth</code>: Taxonomy hierarchy depth (default: 3) - <code>--generator</code>: AI model (<code>gemini</code>, <code>grok</code>, <code>ollama</code>, <code>openai</code>) (default: <code>gemini</code>) - <code>--domain</code>: Content domain (default: <code>general</code>) - <code>--batch-size</code>: Number of complete chunks to process (default: 10) - <code>--category-limits</code>: Max categories per level (comma-separated) - <code>--specificity-level</code>: Specificity level 1-5 (default: 1)</p> <p>Example Output: <pre><code>\ud83e\udd16 Generating taxonomy with gemini...\n\ud83d\udcc4 Analyzing 3 documents (100 chunks)\n\ud83c\udfaf Domain: medical, Depth: 3\n\u23f3 Generation in progress...\n\u2705 Taxonomy generated successfully!\n\ud83d\udcca Taxonomy ID: 102\n\ud83c\udff7\ufe0f Categories: 45\n\ud83c\udfaf Confidence: 0.85\n\ud83d\udcc2 File: storage/taxonomy/1/ai_taxonomy_uuid.json\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#taxonomy-extension","title":"Taxonomy Extension","text":"<p>Extend an existing taxonomy with additional hierarchy levels:</p> <pre><code>compileo taxonomy extend --taxonomy-data taxonomy.json --project-id 1 --additional-depth 2 --generator gemini --domain medical --batch-size 10\n</code></pre> <p>Parameters: - <code>--taxonomy-data</code>: JSON file with taxonomy/category data (optional) - <code>--project-id</code>: Project ID (required if using taxonomy-data) - <code>--additional-depth</code>: Levels to add (default: 2) - <code>--generator</code>: AI model (default: <code>gemini</code>) - <code>--domain</code>: Content domain (default: <code>general</code>) - <code>--batch-size</code>: Number of complete chunks to process (optional) - <code>--documents</code>: Comma-separated list of document IDs to analyze (optional) - <code>--processing-mode</code>: Processing mode (<code>fast</code> or <code>complete</code>) (default: <code>fast</code>)   - <code>fast</code>: Quick processing with sampling (default)   - <code>complete</code>: Comprehensive processing of all content</p> <p>Alternative: Extend by taxonomy ID <pre><code>compileo taxonomy extend --taxonomy-id 102 --additional-depth 1 --generator gemini\n</code></pre></p> <p>Example Output: <pre><code>\ud83d\ude80 Extending taxonomy 102...\n\ud83d\udcc8 Adding 1 additional level\n\u23f3 Extension in progress...\n\u2705 Taxonomy extended successfully!\n\ud83d\udcca New categories: 78 (was 45)\n\ud83c\udfaf Updated confidence: 0.82\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#taxonomy-viewing","title":"Taxonomy Viewing","text":"<p>Load detailed information about a specific taxonomy:</p> <pre><code>compileo taxonomy load 101 --format json --output taxonomy_backup.json\n</code></pre> <p>Parameters: - <code>taxonomy_id</code>: Taxonomy ID to load (required) - <code>--format</code>: Output format (<code>json</code>, <code>text</code>) (default: <code>json</code>) - <code>--output</code>: Save to file instead of displaying (optional)</p> <p>Example Output: <pre><code>{\n  \"taxonomy\": {\n    \"name\": \"Medical Conditions\",\n    \"description\": \"Classification system\",\n    \"children\": [...]\n  },\n  \"metadata\": {\n    \"type\": \"manual\",\n    \"confidence_score\": 0.8,\n    \"created_manually\": true\n  },\n  \"analytics\": {\n    \"depth_analysis\": {\n      \"total_categories\": 45,\n      \"max_depth\": 3\n    }\n  }\n}\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#taxonomy-management","title":"Taxonomy Management","text":""},{"location":"userGuide/taxonomycli/#update-taxonomy","title":"Update Taxonomy","text":"<p>Update taxonomy information:</p> <pre><code>compileo taxonomy update 101 --name \"Updated Medical Conditions\"\n</code></pre> <p>Parameters: - <code>taxonomy_id</code>: Taxonomy ID to update (required) - <code>--name</code>: New taxonomy name (required)</p>"},{"location":"userGuide/taxonomycli/#delete-taxonomy","title":"Delete Taxonomy","text":"<p>Remove a taxonomy. This operation performs a complete cleanup, removing the taxonomy's file from the filesystem and deleting all associated extraction jobs and their results (both database entries and filesystem files).</p> <pre><code>compileo taxonomy delete 101 --confirm\n</code></pre> <p>Parameters: - <code>taxonomy_id</code>: Taxonomy ID to delete (required) - <code>--confirm</code>: Skip confirmation prompt (flag)</p>"},{"location":"userGuide/taxonomycli/#bulk-delete-taxonomies","title":"Bulk Delete Taxonomies","text":"<p>Delete multiple taxonomies at once. This operation performs a complete cleanup for all specified taxonomies, removing their files from the filesystem and deleting all associated extraction jobs and their results (both database entries and filesystem files).</p> <pre><code>compileo taxonomy bulk-delete --taxonomy-ids 101,102,103 --confirm\n</code></pre> <p>Parameters: - <code>--taxonomy-ids</code>: Comma-separated taxonomy IDs (required) - <code>--confirm</code>: Skip confirmation prompt (flag)</p>"},{"location":"userGuide/taxonomycli/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"userGuide/taxonomycli/#complete-taxonomy-workflow","title":"Complete Taxonomy Workflow","text":"<pre><code>#!/bin/bash\n# Complete taxonomy creation and management workflow\n\nPROJECT_ID=1\nDOC_IDS=\"101,102,103\"\n\necho \"\ud83d\ude80 Starting taxonomy workflow...\"\n\n# 1. Generate AI taxonomy\necho \"\ud83e\udd16 Generating AI taxonomy...\"\ncompileo taxonomy generate \\\n    --project-id $PROJECT_ID \\\n    --name \"Medical Knowledge Base\" \\\n    --documents $DOC_IDS \\\n    --depth 3 \\\n    --generator gemini \\\n    --domain medical \\\n    --batch-size 10 \\\n    --category-limits 5,10,15\n\n# 2. Extend with additional depth\necho \"\ud83d\udcc8 Extending taxonomy...\"\ncompileo taxonomy extend \\\n    --taxonomy-id $(compileo taxonomy list --project-id $PROJECT_ID --format json | jq -r '.taxonomies[0].id') \\\n    --additional-depth 1 \\\n    --generator gemini\n\n# 3. Backup taxonomy\necho \"\ud83d\udcbe Creating backup...\"\ncompileo taxonomy load $(compileo taxonomy list --project-id $PROJECT_ID --format json | jq -r '.taxonomies[0].id') \\\n    --output medical_taxonomy_backup.json\n\necho \"\u2705 Taxonomy workflow completed!\"\n</code></pre>"},{"location":"userGuide/taxonomycli/#batch-taxonomy-generation","title":"Batch Taxonomy Generation","text":"<pre><code>#!/bin/bash\n# Generate taxonomies for multiple domains\n\nPROJECT_ID=1\nDOMAINS=(\"medical\" \"legal\" \"technical\")\nDOC_IDS=\"101,102,103,104,105\"\n\nfor domain in \"${DOMAINS[@]}\"; do\n    echo \"\ud83c\udfd7\ufe0f Generating $domain taxonomy...\"\n    compileo taxonomy generate \\\n        --project-id $PROJECT_ID \\\n        --name \"${domain^} Classification\" \\\n        --documents $DOC_IDS \\\n        --depth 3 \\\n        --generator gemini \\\n        --domain $domain \\\n        --batch-size 10 \\\n        --category-limits 5,8,12\ndone\n\necho \"\ud83d\udcca Generated taxonomies for: ${DOMAINS[*]}\"\n</code></pre>"},{"location":"userGuide/taxonomycli/#taxonomy-quality-assessment","title":"Taxonomy Quality Assessment","text":"<pre><code>#!/bin/bash\n# Assess taxonomy quality and cleanup\n\nPROJECT_ID=1\n\necho \"\ud83d\udd0d Assessing taxonomy quality...\"\n\n# List all taxonomies with confidence scores\ncompileo taxonomy list --project-id $PROJECT_ID --format json | jq -r '.taxonomies[] | \"\\(.id): \\(.name) - Confidence: \\(.confidence_score)\"'\n\n# Remove low-confidence taxonomies\nLOW_CONFIDENCE=$(compileo taxonomy list --project-id $PROJECT_ID --format json | jq -r '.taxonomies[] | select(.confidence_score &lt; 0.7) | .id' | tr '\\n' ',' | sed 's/,$//')\n\nif [ ! -z \"$LOW_CONFIDENCE\" ]; then\n    echo \"\ud83d\uddd1\ufe0f Removing low-confidence taxonomies: $LOW_CONFIDENCE\"\n    compileo taxonomy bulk-delete --taxonomy-ids $LOW_CONFIDENCE --confirm\nfi\n\necho \"\u2705 Quality assessment completed!\"\n</code></pre>"},{"location":"userGuide/taxonomycli/#integration-with-scripts","title":"Integration with Scripts","text":""},{"location":"userGuide/taxonomycli/#python-automation","title":"Python Automation","text":"<pre><code>import subprocess\nimport json\nimport time\n\ndef create_taxonomy_workflow(project_id, document_ids, name, domain=\"general\"):\n    \"\"\"Complete taxonomy creation workflow.\"\"\"\n\n    # Generate AI taxonomy\n    cmd = [\n        \"compileo\", \"taxonomy\", \"generate\",\n        \"--project-id\", str(project_id),\n        \"--name\", name,\n        \"--documents\", \",\".join(map(str, document_ids)),\n        \"--depth\", \"3\",\n        \"--generator\", \"gemini\",\n        \"--domain\", domain,\n        \"--sample-size\", \"100\"\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        raise Exception(f\"Taxonomy generation failed: {result.stderr}\")\n\n    # Extract taxonomy ID from output\n    # (Implementation would parse the output to get taxonomy ID)\n\n    # Extend taxonomy\n    extend_cmd = [\n        \"compileo\", \"taxonomy\", \"extend\",\n        \"--taxonomy-id\", \"102\",  # Would be extracted from generation output\n        \"--additional-depth\", \"1\",\n        \"--generator\", \"gemini\"\n    ]\n\n    extend_result = subprocess.run(extend_cmd, capture_output=True, text=True)\n    if extend_result.returncode != 0:\n        raise Exception(f\"Taxonomy extension failed: {extend_result.stderr}\")\n\n    return True\n\n# Usage\ntry:\n    success = create_taxonomy_workflow(\n        project_id=1,\n        document_ids=[101, 102, 103],\n        name=\"Medical Taxonomy\",\n        domain=\"medical\"\n    )\n    if success:\n        print(\"Taxonomy workflow completed successfully!\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"userGuide/taxonomycli/#taxonomy-comparison-script","title":"Taxonomy Comparison Script","text":"<pre><code>import subprocess\nimport json\n\ndef compare_taxonomies(project_id, taxonomy_ids):\n    \"\"\"Compare multiple taxonomies.\"\"\"\n\n    taxonomies = {}\n\n    for tax_id in taxonomy_ids:\n        # Load taxonomy details\n        cmd = [\"compileo\", \"taxonomy\", \"load\", str(tax_id), \"--format\", \"json\"]\n        result = subprocess.run(cmd, capture_output=True, text=True)\n\n        if result.returncode == 0:\n            tax_data = json.loads(result.stdout)\n            taxonomies[tax_id] = {\n                'name': tax_data['taxonomy']['name'],\n                'categories': tax_data['analytics']['depth_analysis']['total_categories'],\n                'depth': tax_data['analytics']['depth_analysis']['max_depth'],\n                'confidence': tax_data['metadata']['confidence_score']\n            }\n\n    # Print comparison\n    print(\"\ud83d\udcca Taxonomy Comparison:\")\n    print(\"-\" * 60)\n    for tax_id, data in taxonomies.items():\n        print(f\"ID {tax_id}: {data['name']}\")\n        print(f\"  Categories: {data['categories']}, Depth: {data['depth']}, Confidence: {data['confidence']}\")\n        print()\n\n# Usage\ncompare_taxonomies(1, [101, 102, 103])\n</code></pre>"},{"location":"userGuide/taxonomycli/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/taxonomycli/#taxonomy-generation","title":"Taxonomy Generation","text":"<p>Document Selection: - Choose documents with diverse content for better taxonomy coverage - Ensure documents are parsed before taxonomy generation - Use representative samples (100-200 chunks) for optimal results</p> <p>Parameter Optimization: <pre><code># Medical domain taxonomy\ncompileo taxonomy generate \\\n    --project-id 1 \\\n    --documents 101,102,103 \\\n    --depth 3 \\\n    --domain medical \\\n    --category-limits 5,10,15 \\\n    --specificity-level 2\n\n# Technical documentation\ncompileo taxonomy generate \\\n    --project-id 1 \\\n    --documents 201,202 \\\n    --depth 4 \\\n    --domain technical \\\n    --category-limits 3,8,12,20 \\\n    --specificity-level 1\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#taxonomy-extension_1","title":"Taxonomy Extension","text":"<p>Incremental Growth: <pre><code># Add one level at a time for better control\ncompileo taxonomy extend --taxonomy-id 101 --additional-depth 1 --generator gemini\n\n# Use domain-specific extension with selected documents\ncompileo taxonomy extend --taxonomy-id 101 --additional-depth 2 --domain medical --generator gemini --documents 101,102\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#quality-management","title":"Quality Management","text":"<p>Regular Assessment: <pre><code># Check taxonomy health\ncompileo taxonomy list --project-id 1 --format json | jq '.taxonomies[] | select(.confidence_score &lt; 0.8)'\n\n# Archive old taxonomies\ncompileo taxonomy bulk-delete --taxonomy-ids $(compileo taxonomy list --project-id 1 --format json | jq -r '.taxonomies[] | select(.created_at &lt; \"2024-01-01\") | .id' | tr '\\n' ',')\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#backup-and-recovery","title":"Backup and Recovery","text":"<p>Regular Backups: <pre><code># Backup all taxonomies\nfor tax_id in $(compileo taxonomy list --project-id 1 --format json | jq -r '.taxonomies[].id'); do\n    compileo taxonomy load $tax_id --output \"backup_taxonomy_${tax_id}.json\"\ndone\n\n# Restore from backup\ncompileo taxonomy create --project-id 1 --file backup_taxonomy_101.json\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#error-handling","title":"Error Handling","text":""},{"location":"userGuide/taxonomycli/#common-issues","title":"Common Issues","text":"<p>Missing Documents: <pre><code># Error: No chunks found for selected documents\ncompileo taxonomy generate --project-id 1 --documents 999\n# Solution: Check document IDs and ensure they are parsed\ncompileo documents list --project-id 1\n</code></pre></p> <p>API Key Issues: <pre><code># Error: API key not configured\ncompileo taxonomy generate --project-id 1 --documents 101 --generator gemini\n# Solution: Ensure API keys are set in configuration\n</code></pre></p> <p>Invalid JSON: <pre><code># Error: Invalid taxonomy file format\ncompileo taxonomy create --project-id 1 --file invalid.json\n# Solution: Validate JSON structure before use\n</code></pre></p> <p>Permission Issues: <pre><code># Error: Cannot write to taxonomy directory\ncompileo taxonomy generate --project-id 1 --documents 101\n# Solution: Check file system permissions\n</code></pre></p>"},{"location":"userGuide/taxonomycli/#performance-optimization","title":"Performance Optimization","text":""},{"location":"userGuide/taxonomycli/#large-taxonomy-operations","title":"Large Taxonomy Operations","text":"<pre><code># Use smaller sample sizes for faster generation\ncompileo taxonomy generate \\\n    --project-id 1 \\\n    --documents 101,102,103 \\\n    --batch-size 10 \\\n    --generator gemini\n\n# Process taxonomies in batches\ncompileo taxonomy extend --taxonomy-id 101 --additional-depth 1 --sample-size 25\n</code></pre>"},{"location":"userGuide/taxonomycli/#memory-management","title":"Memory Management","text":"<pre><code># Limit concurrent operations\n# Use appropriate chunk sizes for your system\ncompileo taxonomy generate \\\n    --project-id 1 \\\n    --documents 101 \\\n    --sample-size 50 \\\n    --category-limits 3,5,8\n</code></pre> <p>This CLI provides comprehensive taxonomy management capabilities with support for manual creation, AI generation, extension, and full lifecycle management suitable for both interactive use and automated workflows.</p>"},{"location":"userGuide/taxonomygui/","title":"Taxonomy Module GUI Usage Guide","text":"<p>The Compileo Taxonomy GUI provides an intuitive web interface for taxonomy management, including creation, generation, extension, and content extraction. This guide covers all GUI features with step-by-step instructions.</p>"},{"location":"userGuide/taxonomygui/#accessing-taxonomy-builder","title":"Accessing Taxonomy Builder","text":"<p>Navigate to the \"\ud83c\udff7\ufe0f Taxonomy Builder\" page from the main menu. The interface is organized into three main tabs:</p> <ol> <li>\ud83c\udfd7\ufe0f Build Taxonomy - Create and edit taxonomies</li> <li>\ud83d\udce4 Extraction - Extract content using taxonomies</li> <li>\ud83d\udccb Browse &amp; Manage Taxonomies - View and manage existing taxonomies</li> </ol>"},{"location":"userGuide/taxonomygui/#tab-1-build-taxonomy","title":"Tab 1: Build Taxonomy","text":""},{"location":"userGuide/taxonomygui/#unified-taxonomy-builder","title":"Unified Taxonomy Builder","text":"<p>The main taxonomy building interface combines manual editing with AI assistance:</p>"},{"location":"userGuide/taxonomygui/#manual-taxonomy-creation","title":"Manual Taxonomy Creation","text":"<ol> <li>Start New Taxonomy:</li> <li>Click \"Create New Taxonomy\"</li> <li>Enter taxonomy name and description</li> <li> <p>Select project association</p> </li> <li> <p>Add Root Categories:</p> </li> <li>Click \"Add Category\" to create top-level categories</li> <li>Enter category name and description</li> <li> <p>Set confidence threshold (0.0-1.0)</p> </li> <li> <p>Build Hierarchy:</p> </li> <li>Click on any category to expand</li> <li>Add subcategories with \"Add Subcategory\"</li> <li>Drag and drop to reorganize structure</li> <li> <p>Delete categories with confirmation</p> </li> <li> <p>Import/Export:</p> </li> <li>Import taxonomy from JSON file</li> <li>Export current taxonomy structure</li> <li>Validate taxonomy structure before saving</li> </ol>"},{"location":"userGuide/taxonomygui/#ai-assisted-generation","title":"AI-Assisted Generation","text":"<ol> <li>AI Generation Setup:<ul> <li>Select \"Generate with AI\" mode</li> <li>Choose AI model (Gemini, Grok, Ollama)</li> <li>Select source documents (must be parsed)</li> </ul> </li> </ol> <p>Ollama Generator Configuration: When using Ollama for taxonomy generation, you can fine-tune AI behavior by configuring parameters in Settings \u2192 AI Model Configuration. Available parameters include temperature, repeat penalty, top-p, top-k, and num_predict for optimal taxonomy generation results.</p> <ol> <li>Generation Parameters:</li> <li>Domain: Content domain (medical, legal, technical, general)</li> <li>Processing Mode:<ul> <li>Fast (Sampled): Quickly generates taxonomy from a sample of up to 10 chunks.</li> <li>Complete (All Content): Iteratively processes every chunk in the document for comprehensive coverage.</li> </ul> </li> <li>Depth: Hierarchy levels (1-5)</li> <li>Chunk Batch Size: Number of complete chunks to process per batch (1-50)</li> <li>Category Limits: Max categories per level</li> <li> <p>Specificity Level: Detail level (1-5)</p> </li> <li> <p>Generation Process:</p> </li> <li>Click \"Generate Taxonomy\"</li> <li>Monitor progress in real-time</li> <li>Review generated structure</li> <li>Edit manually if needed</li> <li>Save final taxonomy</li> </ol>"},{"location":"userGuide/taxonomygui/#taxonomy-extension","title":"Taxonomy Extension","text":"<ol> <li>Extend Existing Taxonomy:</li> <li>Select taxonomy to extend</li> <li> <p>Choose extension method:</p> <ul> <li>Add Levels: Add depth to entire taxonomy</li> <li>Expand Category: Extend specific category</li> <li>Refine Existing: Improve existing categories</li> </ul> </li> <li> <p>Extension Parameters:</p> </li> <li>Additional depth levels</li> <li>AI model selection</li> <li>Domain specification</li> <li> <p>Sample size adjustment</p> </li> <li> <p>Review and Apply:</p> </li> <li>Preview extension results</li> <li>Accept or modify changes</li> <li>Save extended taxonomy</li> </ol>"},{"location":"userGuide/taxonomygui/#tab-2-extraction","title":"Tab 2: Extraction","text":""},{"location":"userGuide/taxonomygui/#content-classification-setup","title":"Content Classification Setup","text":"<ol> <li>Select Taxonomy:</li> <li>Choose taxonomy for extraction</li> <li>View taxonomy structure preview</li> <li> <p>Select specific categories or use entire taxonomy</p> </li> <li> <p>Document Selection:</p> </li> <li>Choose project containing documents</li> <li>Select individual documents or all documents</li> <li> <p>Filter by document status (parsed, chunked)</p> </li> <li> <p>Extraction Parameters:</p> </li> <li>Confidence Threshold: Minimum classification confidence (0.0-1.0)</li> <li>Max Chunks: Limit processing volume</li> <li>Validation Stage: Enable two-stage classification</li> <li>Primary Classifier: Main AI model</li> <li>Validation Classifier: Secondary model for validation</li> </ol>"},{"location":"userGuide/taxonomygui/#extraction-process","title":"Extraction Process","text":"<ol> <li>Start Extraction:</li> <li>Click \"Start Extraction Job\"</li> <li>Monitor progress with real-time updates</li> <li> <p>View processing statistics</p> </li> <li> <p>Results Review:</p> </li> <li>Browse extracted content by category</li> <li>Filter results by confidence score</li> <li>Export results to various formats</li> <li> <p>Generate summary reports</p> </li> <li> <p>Quality Assessment:</p> </li> <li>View classification accuracy metrics</li> <li>Identify low-confidence classifications</li> <li>Re-run extraction with adjusted parameters</li> </ol>"},{"location":"userGuide/taxonomygui/#advanced-features","title":"Advanced Features","text":"<p>Batch Extraction: - Process multiple documents simultaneously - Queue extraction jobs for background processing - Monitor multiple jobs in job dashboard</p> <p>Incremental Extraction: - Extract from new documents only - Update existing extractions - Merge results across multiple runs</p>"},{"location":"userGuide/taxonomygui/#tab-3-browse-manage-taxonomies","title":"Tab 3: Browse &amp; Manage Taxonomies","text":""},{"location":"userGuide/taxonomygui/#taxonomy-browser","title":"Taxonomy Browser","text":"<ol> <li>Taxonomy List:</li> <li>View all taxonomies in selected project</li> <li>Sort by name, creation date, confidence score</li> <li> <p>Filter by taxonomy type (manual, AI-generated)</p> </li> <li> <p>Taxonomy Details:</p> </li> <li>Click taxonomy name to view full structure</li> <li>Expand/collapse hierarchy levels</li> <li>View category statistics and confidence scores</li> <li> <p>Export taxonomy structure</p> </li> <li> <p>Analytics Dashboard:</p> </li> <li>Depth Analysis: Hierarchy depth and distribution</li> <li>Category Count: Total categories and distribution</li> <li>Confidence Metrics: Average and distribution of confidence scores</li> <li>Usage Statistics: Extraction jobs and results</li> </ol>"},{"location":"userGuide/taxonomygui/#taxonomy-management","title":"Taxonomy Management","text":""},{"location":"userGuide/taxonomygui/#edit-taxonomy","title":"Edit Taxonomy","text":"<ol> <li>Modify Structure:</li> <li>Add, remove, or rename categories</li> <li>Reorganize hierarchy with drag-and-drop</li> <li> <p>Update category descriptions and confidence thresholds</p> </li> <li> <p>Bulk Operations:</p> </li> <li>Import categories from CSV/JSON</li> <li>Export selected branches</li> <li>Clone taxonomy structure</li> </ol>"},{"location":"userGuide/taxonomygui/#delete-taxonomy","title":"Delete Taxonomy","text":"<ol> <li> <p>Safe Deletion:</p> <ul> <li>Confirmation prompts prevent accidents</li> <li>Complete Cleanup: Deleting a taxonomy automatically removes its file from the filesystem and cleans up all associated extraction jobs and their results (both database entries and filesystem files).</li> <li>Check for dependent extraction jobs</li> <li>Option to archive instead of delete</li> </ul> </li> <li> <p>Bulk Management:</p> </li> <li>Select multiple taxonomies for deletion</li> <li>Filter by criteria (old, low-confidence, unused)</li> <li>Batch operation confirmation</li> </ol>"},{"location":"userGuide/taxonomygui/#taxonomy-comparison","title":"Taxonomy Comparison","text":"<ol> <li>Side-by-Side View:</li> <li>Compare two taxonomies visually</li> <li>Highlight differences in structure</li> <li> <p>Merge compatible branches</p> </li> <li> <p>Metrics Comparison:</p> </li> <li>Compare depth, category count, confidence scores</li> <li>View overlap analysis</li> <li>Generate comparison reports</li> </ol>"},{"location":"userGuide/taxonomygui/#advanced-gui-features","title":"Advanced GUI Features","text":""},{"location":"userGuide/taxonomygui/#state-management","title":"State Management","text":"<p>Session Persistence: - Current selections remembered across page refreshes - Unsaved changes protected with confirmation prompts - Progress tracking for long-running operations</p> <p>Navigation States: - Seamless transitions between views - Breadcrumb navigation for deep taxonomy editing - Back/forward navigation support</p>"},{"location":"userGuide/taxonomygui/#real-time-updates","title":"Real-time Updates","text":"<p>Live Progress: - Real-time progress bars for generation and extraction - Live statistics updates during processing - Instant feedback on parameter changes</p> <p>Collaborative Features: - Lock mechanism for concurrent editing - Change notifications for shared taxonomies - Version history tracking</p>"},{"location":"userGuide/taxonomygui/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<ul> <li>Ctrl+S: Save current taxonomy</li> <li>Ctrl+Z: Undo last change</li> <li>Ctrl+Y: Redo last change</li> <li>Delete: Remove selected category</li> <li>Enter: Add subcategory to selected item</li> </ul>"},{"location":"userGuide/taxonomygui/#best-practices","title":"Best Practices","text":""},{"location":"userGuide/taxonomygui/#taxonomy-design","title":"Taxonomy Design","text":"<p>Structure Guidelines: - Start with 3-4 levels maximum for usability - Use clear, descriptive category names - Maintain consistent naming conventions - Set appropriate confidence thresholds</p> <p>Quality Assurance: - Regularly review and update taxonomy structure - Test extraction accuracy on sample documents - Maintain version history for important taxonomies - Document taxonomy purpose and scope</p>"},{"location":"userGuide/taxonomygui/#performance-optimization","title":"Performance Optimization","text":"<p>Large Taxonomies: - Use category limits to control growth - Implement pagination for deep hierarchies - Consider splitting very large taxonomies</p> <p>Processing Efficiency: - Select appropriate sample sizes for generation - Use incremental extraction for updates - Monitor and optimize confidence thresholds</p>"},{"location":"userGuide/taxonomygui/#maintenance-workflows","title":"Maintenance Workflows","text":"<p>Regular Maintenance: <pre><code># Monthly taxonomy review checklist\n- [ ] Review extraction accuracy metrics\n- [ ] Update category descriptions\n- [ ] Remove unused categories\n- [ ] Test on new document types\n- [ ] Archive outdated taxonomies\n</code></pre></p> <p>Version Control: - Create backups before major changes - Use descriptive names for taxonomy versions - Document changes and rationale - Maintain changelog for important taxonomies</p>"},{"location":"userGuide/taxonomygui/#troubleshooting","title":"Troubleshooting","text":""},{"location":"userGuide/taxonomygui/#common-issues","title":"Common Issues","text":"<p>Generation Failures: - Check document parsing status - Verify API key configuration - Ensure sufficient document content - Try different AI models</p> <p>Extraction Problems: - Validate taxonomy structure - Check confidence threshold settings - Review document chunking quality - Monitor API rate limits</p> <p>Performance Issues: - Reduce sample sizes for large document sets - Use category limits to control taxonomy size - Implement pagination for large result sets</p> <p>UI Responsiveness: - Clear browser cache and cookies - Check internet connection stability - Close unused browser tabs - Update browser to latest version</p>"},{"location":"userGuide/taxonomygui/#integration-examples","title":"Integration Examples","text":""},{"location":"userGuide/taxonomygui/#workflow-automation","title":"Workflow Automation","text":"<p>Document Processing Pipeline: 1. Upload and parse documents 2. Generate or select taxonomy 3. Configure extraction parameters 4. Run batch extraction jobs 5. Review and export results</p> <p>Quality Assurance Process: 1. Create test taxonomy with known categories 2. Run extraction on labeled test documents 3. Compare results with expected classifications 4. Adjust parameters and re-run tests 5. Validate on production documents</p>"},{"location":"userGuide/taxonomygui/#api-integration","title":"API Integration","text":"<p>Programmatic Taxonomy Management: <pre><code>// Create taxonomy via API\nconst taxonomy = await fetch('/api/v1/taxonomy/', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    name: 'Medical Conditions',\n    project_id: 1,\n    taxonomy: taxonomyStructure\n  })\n});\n\n// Generate taxonomy\nconst generation = await fetch('/api/v1/taxonomy/generate', {\n  method: 'POST',\n  headers: { 'Content-Type': 'application/json' },\n  body: JSON.stringify({\n    project_id: 1,\n    documents: [101, 102, 103],\n    generator: 'gemini',\n    domain: 'medical'\n  })\n});\n</code></pre></p> <p>Real-time Updates: <pre><code>// WebSocket connection for live updates\nconst ws = new WebSocket('ws://localhost:8000/ws/taxonomy');\n\nws.onmessage = (event) =&gt; {\n  const update = JSON.parse(event.data);\n  if (update.type === 'extraction_progress') {\n    updateProgressBar(update.progress);\n  }\n};\n</code></pre></p> <p>This GUI provides a comprehensive taxonomy management system with intuitive interfaces for creation, editing, generation, and extraction, suitable for both novice users and advanced taxonomy designers.</p>"}]}